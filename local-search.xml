<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>深度学习-15-pytorch-8-具体网络分析</title>
    <link href="/2021/08/15/pytorchtrain8/"/>
    <url>/2021/08/15/pytorchtrain8/</url>
    
    <content type="html"><![CDATA[<blockquote><p>创作声明：主要内容参考于张贤同学<a href="https://zhuanlan.zhihu.com/p/265394674">https://zhuanlan.zhihu.com/p/265394674</a></p></blockquote><p>这篇文章主要介绍了 图像分类的 inference，其中会着重介绍 <code>ResNet</code>。</p><h2 id="模型概览"><a href="#模型概览" class="headerlink" title="模型概览"></a>模型概览</h2><p>在<code>torchvision.model</code>中，有很多封装好的模型。</p><p><img src="/img/pytorchtrain8/1.png"><br> 可以分类 3 类：</p><ul><li>经典网络<ul><li>alexnet</li><li>vgg</li><li>resnet</li><li>inception</li><li>densenet</li><li>googlenet</li></ul></li><li>轻量化网络<ul><li>squeezenet</li><li>mobilenet</li><li>shufflenetv2</li></ul></li><li>自动神经结构搜索方法的网络<ul><li>mnasnet</li></ul></li></ul><h2 id="ResNet18-的使用"><a href="#ResNet18-的使用" class="headerlink" title="ResNet18 的使用"></a>ResNet18 的使用</h2><p>以 <code>ResNet 18</code> 为例。</p><p>首先加载训练好的模型参数：</p><figure class="highlight pgsql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs pgsql">resnet18 = models.resnet18()<br><br># 修改全连接层的输出<br>num_ftrs = resnet18.fc.in_features<br>resnet18.fc = nn.Linear(num_ftrs, <span class="hljs-number">2</span>)<br><br># 加载模型参数<br><span class="hljs-keyword">checkpoint</span> = torch.<span class="hljs-keyword">load</span>(m_path)<br>resnet18.load_state_dict(<span class="hljs-keyword">checkpoint</span>[<span class="hljs-string">&#x27;model_state_dict&#x27;</span>])<br></code></pre></td></tr></table></figure><p>然后比较重要的是把模型放到 GPU 上，并且转换到<code>eval</code>模式：</p><figure class="highlight autoit"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs autoit">resnet18.<span class="hljs-keyword">to</span>(device)<br>resnet18.<span class="hljs-built_in">eval</span>()<br></code></pre></td></tr></table></figure><p>在inference 时，主要流程如下：</p><ul><li><p>代码要放在<code>with torch.no_grad():</code>下。<code>torch.no_grad()</code>会关闭反向传播，可以减少内存、加快速度。</p></li><li><p>根据路径读取图片，把图片转换为 tensor，然后使用<code>unsqueeze_(0)</code>方法把形状扩大为$B \times C \times H \times W$，再把 tensor 放到 GPU 上 。</p></li><li><p>模型的输出数据<code>outputs</code>的形状是$1 \times 2$，表示 <code>batch_size</code> 为 1，分类数量为 2。<code>torch.max(outputs,0)</code>是返回<code>outputs</code>中<strong>每一列</strong>最大的元素和索引，<code>torch.max(outputs,1)</code>是返回<code>outputs</code>中<strong>每一行</strong>最大的元素和索引。</p><p>这里使用<code>_, pred_int = torch.max(outputs.data, 1)</code>返回最大元素的索引，然后根据索引获得 label：<code>pred_str = classes[int(pred_int)]</code>。</p></li></ul><p>关键代码如下：</p><figure class="highlight pgsql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs pgsql"><span class="hljs-keyword">with</span> torch.no_grad():<br>    <span class="hljs-keyword">for</span> idx, img_name <span class="hljs-keyword">in</span> enumerate(img_names):<br><br>        path_img = os.path.<span class="hljs-keyword">join</span>(img_dir, img_name)<br><br>        # step <span class="hljs-number">1</span>/<span class="hljs-number">4</span> : <span class="hljs-type">path</span> <span class="hljs-comment">--&gt; img</span><br>        img_rgb = Image.<span class="hljs-keyword">open</span>(path_img).convert(<span class="hljs-string">&#x27;RGB&#x27;</span>)<br><br>        # step <span class="hljs-number">2</span>/<span class="hljs-number">4</span> : img <span class="hljs-comment">--&gt; tensor</span><br>        img_tensor = img_transform(img_rgb, inference_transform)<br>        img_tensor.unsqueeze_(<span class="hljs-number">0</span>)<br>        img_tensor = img_tensor.<span class="hljs-keyword">to</span>(device)<br><br>        # step <span class="hljs-number">3</span>/<span class="hljs-number">4</span> : tensor <span class="hljs-comment">--&gt; vector</span><br>        outputs = resnet18(img_tensor)<br><br>        # step <span class="hljs-number">4</span>/<span class="hljs-number">4</span> : <span class="hljs-keyword">get</span> label<br>        _, pred_int = torch.max(outputs.data, <span class="hljs-number">1</span>)<br>        pred_str = classes[<span class="hljs-type">int</span>(pred_int)]<br></code></pre></td></tr></table></figure><p>全部代码如下所示：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> os<br><span class="hljs-keyword">import</span> time<br><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<br><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torchvision.transforms <span class="hljs-keyword">as</span> transforms<br><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image<br><span class="hljs-keyword">from</span> matplotlib <span class="hljs-keyword">import</span> pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-keyword">import</span> torchvision.models <span class="hljs-keyword">as</span> models<br><span class="hljs-keyword">import</span> enviroments<br>BASE_DIR = os.path.dirname(os.path.abspath(__file__))<br><span class="hljs-comment"># device = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;)</span><br>device = torch.device(<span class="hljs-string">&quot;cpu&quot;</span>)<br><br><span class="hljs-comment"># config</span><br>vis = <span class="hljs-literal">True</span><br><span class="hljs-comment"># vis = False</span><br>vis_row = <span class="hljs-number">4</span><br><br>norm_mean = [<span class="hljs-number">0.485</span>, <span class="hljs-number">0.456</span>, <span class="hljs-number">0.406</span>]<br>norm_std = [<span class="hljs-number">0.229</span>, <span class="hljs-number">0.224</span>, <span class="hljs-number">0.225</span>]<br><br>inference_transform = transforms.Compose([<br>    transforms.Resize(<span class="hljs-number">256</span>),<br>    transforms.CenterCrop(<span class="hljs-number">224</span>),<br>    transforms.ToTensor(),<br>    transforms.Normalize(norm_mean, norm_std),<br>])<br><br>classes = [<span class="hljs-string">&quot;ants&quot;</span>, <span class="hljs-string">&quot;bees&quot;</span>]<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">img_transform</span>(<span class="hljs-params">img_rgb, transform=<span class="hljs-literal">None</span></span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    将数据转换为模型读取的形式</span><br><span class="hljs-string">    :param img_rgb: PIL Image</span><br><span class="hljs-string">    :param transform: torchvision.transform</span><br><span class="hljs-string">    :return: tensor</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br><br>    <span class="hljs-keyword">if</span> transform <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>        <span class="hljs-keyword">raise</span> ValueError(<span class="hljs-string">&quot;找不到transform！必须有transform对img进行处理&quot;</span>)<br><br>    img_t = transform(img_rgb)<br>    <span class="hljs-keyword">return</span> img_t<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_img_name</span>(<span class="hljs-params">img_dir, <span class="hljs-built_in">format</span>=<span class="hljs-string">&quot;jpg&quot;</span></span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    获取文件夹下format格式的文件名</span><br><span class="hljs-string">    :param img_dir: str</span><br><span class="hljs-string">    :param format: str</span><br><span class="hljs-string">    :return: list</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    file_names = os.listdir(img_dir)<br>    <span class="hljs-comment"># 使用 list(filter(lambda())) 筛选出 jpg 后缀的文件</span><br>    img_names = <span class="hljs-built_in">list</span>(<span class="hljs-built_in">filter</span>(<span class="hljs-keyword">lambda</span> x: x.endswith(<span class="hljs-built_in">format</span>), file_names))<br><br>    <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(img_names) &lt; <span class="hljs-number">1</span>:<br>        <span class="hljs-keyword">raise</span> ValueError(<span class="hljs-string">&quot;&#123;&#125;下找不到&#123;&#125;格式数据&quot;</span>.<span class="hljs-built_in">format</span>(img_dir, <span class="hljs-built_in">format</span>))<br>    <span class="hljs-keyword">return</span> img_names<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_model</span>(<span class="hljs-params">m_path, vis_model=<span class="hljs-literal">False</span></span>):<br><br>    resnet18 = models.resnet18()<br><br>    <span class="hljs-comment"># 修改全连接层的输出</span><br>    num_ftrs = resnet18.fc.in_features<br>    resnet18.fc = nn.Linear(num_ftrs, <span class="hljs-number">2</span>)<br><br>    <span class="hljs-comment"># 加载模型参数</span><br>    checkpoint = torch.load(m_path)<br>    resnet18.load_state_dict(checkpoint[<span class="hljs-string">&#x27;model_state_dict&#x27;</span>])<br><br><br>    <span class="hljs-keyword">if</span> vis_model:<br>        <span class="hljs-keyword">from</span> torchsummary <span class="hljs-keyword">import</span> summary<br>        summary(resnet18, input_size=(<span class="hljs-number">3</span>, <span class="hljs-number">224</span>, <span class="hljs-number">224</span>), device=<span class="hljs-string">&quot;cpu&quot;</span>)<br><br>    <span class="hljs-keyword">return</span> resnet18<br><br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&quot;__main__&quot;</span>:<br><br>    img_dir = os.path.join(enviroments.hymenoptera_data_dir,<span class="hljs-string">&quot;val/bees&quot;</span>)<br>    model_path = <span class="hljs-string">&quot;./checkpoint_14_epoch.pkl&quot;</span><br>    time_total = <span class="hljs-number">0</span><br>    img_list, img_pred = <span class="hljs-built_in">list</span>(), <span class="hljs-built_in">list</span>()<br><br>    <span class="hljs-comment"># 1. data</span><br>    img_names = get_img_name(img_dir)<br>    num_img = <span class="hljs-built_in">len</span>(img_names)<br><br>    <span class="hljs-comment"># 2. model</span><br>    resnet18 = get_model(model_path, <span class="hljs-literal">True</span>)<br>    resnet18.to(device)<br>    resnet18.<span class="hljs-built_in">eval</span>()<br><br>    <span class="hljs-keyword">with</span> torch.no_grad():<br>        <span class="hljs-keyword">for</span> idx, img_name <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(img_names):<br><br>            path_img = os.path.join(img_dir, img_name)<br><br>            <span class="hljs-comment"># step 1/4 : path --&gt; img</span><br>            img_rgb = Image.<span class="hljs-built_in">open</span>(path_img).convert(<span class="hljs-string">&#x27;RGB&#x27;</span>)<br><br>            <span class="hljs-comment"># step 2/4 : img --&gt; tensor</span><br>            img_tensor = img_transform(img_rgb, inference_transform)<br>            img_tensor.unsqueeze_(<span class="hljs-number">0</span>)<br>            img_tensor = img_tensor.to(device)<br><br>            <span class="hljs-comment"># step 3/4 : tensor --&gt; vector</span><br>            time_tic = time.time()<br>            outputs = resnet18(img_tensor)<br>            time_toc = time.time()<br><br>            <span class="hljs-comment"># step 4/4 : visualization</span><br>            _, pred_int = torch.<span class="hljs-built_in">max</span>(outputs.data, <span class="hljs-number">1</span>)<br>            pred_str = classes[<span class="hljs-built_in">int</span>(pred_int)]<br><br>            <span class="hljs-keyword">if</span> vis:<br>                img_list.append(img_rgb)<br>                img_pred.append(pred_str)<br><br>                <span class="hljs-keyword">if</span> (idx+<span class="hljs-number">1</span>) % (vis_row*vis_row) == <span class="hljs-number">0</span> <span class="hljs-keyword">or</span> num_img == idx+<span class="hljs-number">1</span>:<br>                    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(img_list)):<br>                        plt.subplot(vis_row, vis_row, i+<span class="hljs-number">1</span>).imshow(img_list[i])<br>                        plt.title(<span class="hljs-string">&quot;predict:&#123;&#125;&quot;</span>.<span class="hljs-built_in">format</span>(img_pred[i]))<br>                    plt.show()<br>                    plt.close()<br>                    img_list, img_pred = <span class="hljs-built_in">list</span>(), <span class="hljs-built_in">list</span>()<br><br>            time_s = time_toc-time_tic<br>            time_total += time_s<br><br>            <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;&#123;:d&#125;/&#123;:d&#125;: &#123;&#125; &#123;:.3f&#125;s &#x27;</span>.<span class="hljs-built_in">format</span>(idx + <span class="hljs-number">1</span>, num_img, img_name, time_s))<br><br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;\ndevice:&#123;&#125; total time:&#123;:.1f&#125;s mean:&#123;:.3f&#125;s&quot;</span>.<br>          <span class="hljs-built_in">format</span>(device, time_total, time_total/num_img))<br>    <span class="hljs-keyword">if</span> torch.cuda.is_available():<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;GPU name:&#123;&#125;&quot;</span>.<span class="hljs-built_in">format</span>(torch.cuda.get_device_name()))<br></code></pre></td></tr></table></figure><p>总结一下 inference 阶段需要注意的事项：</p><ul><li>确保 model 处于 eval 状态，而非 trainning 状态</li><li>设置 torch.no_grad()，减少内存消耗，加快运算速度</li><li>数据预处理需要保持一致，比如 RGB 或者 rBGR</li></ul><h2 id="残差连接"><a href="#残差连接" class="headerlink" title="残差连接"></a>残差连接</h2><p>以 ResNet 为例：</p><p><img src="/img/pytorchtrain8/2.png"><br> 一个残差块有2条路径$F(x)$和$x$，$F(x)$路径拟合残差，不妨称之为残差路径；$x$路径为<code>identity mapping</code>恒等映射，称之为<code>shortcut</code>。图中的⊕为<code>element-wise addition</code>，要求参与运算的$F(x)$和$x$的尺寸要相同。</p><p><code>shortcut</code> 路径大致可以分成2种，取决于残差路径是否改变了<code>feature map</code>数量和尺寸。</p><ul><li>一种是将输入<code>x</code>原封不动地输出。</li><li>另一种则需要经过$1×1$卷积来升维或者降采样，主要作用是将输出与$F(x)$路径的输出保持<code>shape</code>一致，对网络性能的提升并不明显。</li></ul><p>两种结构如下图所示：</p><p><img src="/img/pytorchtrain8/3.png"><br> <code>ResNet</code> 中，使用了上面 2 种 <code>shortcut</code>。</p><h2 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h2><p>ResNet 有很多变种，包括 <code>ResNet 18</code>、<code>ResNet 34</code>、<code>ResNet 50</code>、<code>ResNet 101</code>、<code>ResNet 152</code>，网络结构对比如下：</p><p><img src="/img/pytorchtrain8/4.png"><br> <code>ResNet</code> 的各个变种，数据处理大致流程如下：</p><ul><li>输入的图片形状是$3 \times 224 \times 224$。</li><li>图片经过 <code>conv1</code> 层，输出图片大小为 $ 64 \times 112 \times 112$。</li><li>图片经过 <code>max pool</code> 层，输出图片大小为 $ 64 \times 56 \times 56 $。</li><li>图片经过 <code>conv2</code> 层，输出图片大小为 $ 64 \times 56 \times 56$。<strong>（注意，图片经过这个 <code>layer</code>, 大小是不变的）</strong></li><li>图片经过 <code>conv3</code> 层，输出图片大小为 $ 128 \times 28 \times 28$。</li><li>图片经过 <code>conv4</code> 层，输出图片大小为 $ 256 \times 14 \times 14$。</li><li>图片经过 <code>conv5</code> 层，输出图片大小为 $ 512 \times 7 \times 7$。</li><li>图片经过 <code>avg pool</code> 层，输出大小为 $ 512 \times 1 \times 1$。</li><li>图片经过 <code>fc</code> 层，输出维度为 $ num_classes$，表示每个分类的 <code>logits</code>。</li></ul><p>下面，我们称每个 <code>conv</code> 层为一个 <code>layer</code>（第一个 <code>conv</code> 层就是一个卷积层，因此第一个 <code>conv</code> 层除外）。</p><p>其中 <code>ResNet 18</code>、<code>ResNet 34</code> 的每个 <code>layer</code> 由多个 <code>BasicBlock</code> 组成，只是每个 <code>layer</code> 里堆叠的 <code>BasicBlock</code> 数量不一样。</p><p>而 <code>ResNet 50</code>、<code>ResNet 101</code>、<code>ResNet 152</code> 的每个 <code>layer</code> 由多个 <code>Bottleneck</code> 组成，只是每个 <code>layer</code> 里堆叠的 <code>Bottleneck</code> 数量不一样。</p><h2 id="源码分析"><a href="#源码分析" class="headerlink" title="源码分析"></a>源码分析</h2><p>我们来看看各个 <code>ResNet</code> 的源码，首先从构造函数开始。</p><h3 id="构造函数"><a href="#构造函数" class="headerlink" title="构造函数"></a>构造函数</h3><h4 id="ResNet-18"><a href="#ResNet-18" class="headerlink" title="ResNet 18"></a>ResNet 18</h4><p><code>resnet18</code> 的构造函数如下。</p><p><code>[2, 2, 2, 2]</code> 表示有 4 个 <code>layer</code>，每个 layer 中有 2 个 <code>BasicBlock</code>。</p><p><code>conv1</code>为 1 层，<code>conv2</code>、<code>conv3</code>、<code>conv4</code>、<code>conv5</code>均为 4 层（每个 <code>layer</code> 有 2 个 <code>BasicBlock</code>，每个 <code>BasicBlock</code> 有 2 个卷积层），总共为 16 层，最后一层全连接层，$总层数 &#x3D; 1+ 4 \times 4 + 1 &#x3D; 18$，依此类推。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">resnet18</span>(<span class="hljs-params">pretrained=<span class="hljs-literal">False</span>, progress=<span class="hljs-literal">True</span>, **kwargs</span>):<br>    <span class="hljs-string">r&quot;&quot;&quot;ResNet-18 model from</span><br><span class="hljs-string">    `&quot;Deep Residual Learning for Image Recognition&quot; &lt;https://arxiv.org/pdf/1512.03385.pdf&gt;`_</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Args:</span><br><span class="hljs-string">        pretrained (bool): If True, returns a model pre-trained on ImageNet</span><br><span class="hljs-string">        progress (bool): If True, displays a progress bar of the download to stderr</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-keyword">return</span> _resnet(<span class="hljs-string">&#x27;resnet18&#x27;</span>, BasicBlock, [<span class="hljs-number">2</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>], pretrained, progress,<br>                   **kwargs)<br></code></pre></td></tr></table></figure><h4 id="ResNet-34"><a href="#ResNet-34" class="headerlink" title="ResNet 34"></a>ResNet 34</h4><p><code>resnet 34</code> 的构造函数如下。</p><p><code>[3, 4, 6, 3]</code> 表示有 4 个 <code>layer</code>，每个 <code>layer</code> 的 <code>BasicBlock</code> 数量分别为 3, 4, 6, 3。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">resnet34</span>(<span class="hljs-params">pretrained=<span class="hljs-literal">False</span>, progress=<span class="hljs-literal">True</span>, **kwargs</span>):<br>    <span class="hljs-string">r&quot;&quot;&quot;ResNet-34 model from</span><br><span class="hljs-string">    `&quot;Deep Residual Learning for Image Recognition&quot; &lt;https://arxiv.org/pdf/1512.03385.pdf&gt;`_</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Args:</span><br><span class="hljs-string">        pretrained (bool): If True, returns a model pre-trained on ImageNet</span><br><span class="hljs-string">        progress (bool): If True, displays a progress bar of the download to stderr</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-keyword">return</span> _resnet(<span class="hljs-string">&#x27;resnet34&#x27;</span>, BasicBlock, [<span class="hljs-number">3</span>, <span class="hljs-number">4</span>, <span class="hljs-number">6</span>, <span class="hljs-number">3</span>], pretrained, progress,<br>                   **kwargs)<br></code></pre></td></tr></table></figure><h4 id="ResNet-50"><a href="#ResNet-50" class="headerlink" title="ResNet 50"></a>ResNet 50</h4><p><code>resnet 50</code> 的构造函数如下。</p><p><code>[3, 4, 6, 3]</code> 表示有 4 个 <code>layer</code>，每个 <code>layer</code> 的 <code>Bottleneck</code> 数量分别为 3, 4, 6, 3。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">resnet50</span>(<span class="hljs-params">pretrained=<span class="hljs-literal">False</span>, progress=<span class="hljs-literal">True</span>, **kwargs</span>):<br>    <span class="hljs-string">r&quot;&quot;&quot;ResNet-50 model from</span><br><span class="hljs-string">    `&quot;Deep Residual Learning for Image Recognition&quot; &lt;https://arxiv.org/pdf/1512.03385.pdf&gt;`_</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Args:</span><br><span class="hljs-string">        pretrained (bool): If True, returns a model pre-trained on ImageNet</span><br><span class="hljs-string">        progress (bool): If True, displays a progress bar of the download to stderr</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-keyword">return</span> _resnet(<span class="hljs-string">&#x27;resnet50&#x27;</span>, Bottleneck, [<span class="hljs-number">3</span>, <span class="hljs-number">4</span>, <span class="hljs-number">6</span>, <span class="hljs-number">3</span>], pretrained, progress,<br>                   **kwargs)<br></code></pre></td></tr></table></figure><p>依此类推，<code>ResNet 101</code> 和 <code>ResNet 152</code> 也是由多个 <code>layer</code> 组成的。</p><h3 id="resnet"><a href="#resnet" class="headerlink" title="_resnet()"></a>_resnet()</h3><p>上面所有的构造函数中，都调用了 <code>_resnet()</code> 方法来创建网络，下面来看看 <code>_resnet()</code> 方法。</p><figure class="highlight stan"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs stan">def _resnet(arch, <span class="hljs-built_in">block</span>, layers, pretrained, progress, **kwargs):<br>    <span class="hljs-title">model</span> = ResNet(<span class="hljs-built_in">block</span>, layers, **kwargs)<br>    <span class="hljs-comment"># 加载预训练好的模型参数</span><br>    <span class="hljs-keyword">if</span> pretrained:<br>        state_dict = load_state_dict_from_url(model_urls[arch],<br>                                              progress=progress)<br>        <span class="hljs-title">model</span>.load_state_dict(state_dict)<br>    <span class="hljs-keyword">return</span> <span class="hljs-title">model</span><br></code></pre></td></tr></table></figure><p>可以看到，在 <code>_resnet()</code> 方法中，又调用了 <code>ResNet()</code> 方法创建模型，然后加载训练好的模型参数。</p><h3 id="ResNet-的构造函数"><a href="#ResNet-的构造函数" class="headerlink" title="ResNet()的构造函数"></a>ResNet()的构造函数</h3><p>构造函数的重要参数如下：</p><ul><li>block：每个 <code>layer</code> 里面使用的 <code>block</code>，可以是 <code>BasicBlock</code> <code>Bottleneck</code>。</li><li>num_classes：分类数量，用于构建最后的全连接层。</li><li>layers：一个 list，表示每个 <code>layer</code> 中 <code>block</code> 的数量。</li></ul><p>构造函数的主要流程如下：</p><ul><li><p>判断是否传入 <code>norm_layer</code>，没有传入，则使用 <code>BatchNorm2d</code>。</p></li><li><p>判断是否传入空洞卷积参数 <code>replace_stride_with_dilation</code>，如果不指定，则赋值为 <code>[False, False, False]</code>，表示不使用空洞卷积。</p></li><li><p>读取分组卷积的参数 <code>groups</code>，<code>width_per_group</code>。</p></li><li><p>然后真正开始构造网络。</p></li><li><p><code>conv1</code> 层的结构是 <code>Conv2d -&gt; norm_layer -&gt; ReLU</code>。</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs routeros">self.conv1 = nn.Conv2d(3, self.inplanes, <span class="hljs-attribute">kernel_size</span>=7, <span class="hljs-attribute">stride</span>=2, <span class="hljs-attribute">padding</span>=3, <span class="hljs-attribute">bias</span>=<span class="hljs-literal">False</span>)<br>self.bn1 = norm_layer(self.inplanes)<br>self.relu = nn.ReLU(<span class="hljs-attribute">inplace</span>=<span class="hljs-literal">True</span>)<br></code></pre></td></tr></table></figure></li><li><p><code>conv2</code> 层的代码如下，对应于 <code>layer1</code>，这个 <code>layer</code> 的参数没有指定 <code>stride</code>，默认 <code>stride=1</code>，因此这个 <code>layer</code> 不会改变图片大小：</p><figure class="highlight oxygene"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs oxygene"><span class="hljs-keyword">self</span>.layer1 = <span class="hljs-keyword">self</span>._make_layer(<span class="hljs-keyword">block</span>, <span class="hljs-number">64</span>, layers[<span class="hljs-number">0</span>])<br></code></pre></td></tr></table></figure></li><li><p><code>conv3</code> 层的代码如下，对应于 <code>layer2</code>（注意这个 <code>layer</code> 指定 <code>stride=2</code>，会降采样，详情看下面 <code>_make_layer</code> 的讲解）：</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">self</span>.layer2 = self._make_layer(block, <span class="hljs-number">128</span>, layers[<span class="hljs-number">1</span>], stride=<span class="hljs-number">2</span>, dilate=replace_stride_with_dilation[<span class="hljs-number">0</span>])<br></code></pre></td></tr></table></figure></li><li><p><code>conv4</code> 层的代码如下，对应于 <code>layer3</code>（注意这个 <code>layer</code> 指定 <code>stride=2</code>，会降采样，详情看下面 <code>_make_layer</code> 的讲解）：</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">self</span>.layer3 = self._make_layer(block, <span class="hljs-number">256</span>, layers[<span class="hljs-number">2</span>], stride=<span class="hljs-number">2</span>,<br><span class="hljs-attribute">dilate</span>=replace_stride_with_dilation[<span class="hljs-number">1</span>])<br></code></pre></td></tr></table></figure></li><li><p><code>conv5</code> 层的代码如下，对应于 <code>layer4</code>（注意这个 <code>layer</code> 指定 <code>stride=2</code>，会降采样，详情看下面 <code>_make_layer</code> 的讲解）：</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">self</span>.layer4 = self._make_layer(block, <span class="hljs-number">512</span>, layers[<span class="hljs-number">3</span>], stride=<span class="hljs-number">2</span>,<br><span class="hljs-attribute">dilate</span>=replace_stride_with_dilation[<span class="hljs-number">2</span>])<br></code></pre></td></tr></table></figure></li><li><p>接着是 <code>AdaptiveAvgPool2d</code> 层和 <code>fc</code> 层。</p></li><li><p>最后是网络参数的初始：</p><ul><li>卷积层采用 <code>kaiming_normal_()</code> 初始化方法。</li><li><code>bn</code> 层和 <code>GroupNorm</code> 层初始化为 <code>weight=1</code>，<code>bias=0</code>。</li><li>其中每个 <code>BasicBlock</code> 和 <code>Bottleneck</code> 的最后一层 <code>bn</code> 的 <code>weight=0</code>，可以提升准确率 0.2~0.3%。</li></ul></li></ul><p>完整的构造函数代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, block, layers, num_classes=<span class="hljs-number">1000</span>, zero_init_residual=<span class="hljs-literal">False</span>,</span><br><span class="hljs-params">             groups=<span class="hljs-number">1</span>, width_per_group=<span class="hljs-number">64</span>, replace_stride_with_dilation=<span class="hljs-literal">None</span>,</span><br><span class="hljs-params">             norm_layer=<span class="hljs-literal">None</span></span>):<br>    <span class="hljs-built_in">super</span>(ResNet, <span class="hljs-variable language_">self</span>).__init__()<br>    <span class="hljs-comment"># 使用 bn 层</span><br>    <span class="hljs-keyword">if</span> norm_layer <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>        norm_layer = nn.BatchNorm2d<br>    <span class="hljs-variable language_">self</span>._norm_layer = norm_layer<br><br>    <span class="hljs-variable language_">self</span>.inplanes = <span class="hljs-number">64</span><br>    <span class="hljs-variable language_">self</span>.dilation = <span class="hljs-number">1</span><br>    <span class="hljs-keyword">if</span> replace_stride_with_dilation <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>        <span class="hljs-comment"># each element in the tuple indicates if we should replace</span><br>        <span class="hljs-comment"># the 2x2 stride with a dilated convolution instead</span><br>        replace_stride_with_dilation = [<span class="hljs-literal">False</span>, <span class="hljs-literal">False</span>, <span class="hljs-literal">False</span>]<br>    <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(replace_stride_with_dilation) != <span class="hljs-number">3</span>:<br>        <span class="hljs-keyword">raise</span> ValueError(<span class="hljs-string">&quot;replace_stride_with_dilation should be None &quot;</span><br>                         <span class="hljs-string">&quot;or a 3-element tuple, got &#123;&#125;&quot;</span>.<span class="hljs-built_in">format</span>(replace_stride_with_dilation))<br>    <span class="hljs-variable language_">self</span>.groups = groups<br>    <span class="hljs-variable language_">self</span>.base_width = width_per_group<br>    <span class="hljs-comment"># 对应于 conv1</span><br>    <span class="hljs-variable language_">self</span>.conv1 = nn.Conv2d(<span class="hljs-number">3</span>, <span class="hljs-variable language_">self</span>.inplanes, kernel_size=<span class="hljs-number">7</span>, stride=<span class="hljs-number">2</span>, padding=<span class="hljs-number">3</span>,<br>                           bias=<span class="hljs-literal">False</span>)<br>    <span class="hljs-variable language_">self</span>.bn1 = norm_layer(<span class="hljs-variable language_">self</span>.inplanes)<br>    <span class="hljs-variable language_">self</span>.relu = nn.ReLU(inplace=<span class="hljs-literal">True</span>)<br>    <span class="hljs-comment"># 对应于 conv2</span><br>    <span class="hljs-variable language_">self</span>.maxpool = nn.MaxPool2d(kernel_size=<span class="hljs-number">3</span>, stride=<span class="hljs-number">2</span>, padding=<span class="hljs-number">1</span>)<br>    <span class="hljs-variable language_">self</span>.layer1 = <span class="hljs-variable language_">self</span>._make_layer(block, <span class="hljs-number">64</span>, layers[<span class="hljs-number">0</span>])<br>    <span class="hljs-comment"># 对应于 conv3</span><br>    <span class="hljs-variable language_">self</span>.layer2 = <span class="hljs-variable language_">self</span>._make_layer(block, <span class="hljs-number">128</span>, layers[<span class="hljs-number">1</span>], stride=<span class="hljs-number">2</span>,<br>                                   dilate=replace_stride_with_dilation[<span class="hljs-number">0</span>])<br>    对应于 conv4<br>    <span class="hljs-variable language_">self</span>.layer3 = <span class="hljs-variable language_">self</span>._make_layer(block, <span class="hljs-number">256</span>, layers[<span class="hljs-number">2</span>], stride=<span class="hljs-number">2</span>,<br>                                   dilate=replace_stride_with_dilation[<span class="hljs-number">1</span>])<br>    对应于 conv5<br>    <span class="hljs-variable language_">self</span>.layer4 = <span class="hljs-variable language_">self</span>._make_layer(block, <span class="hljs-number">512</span>, layers[<span class="hljs-number">3</span>], stride=<span class="hljs-number">2</span>,<br>                                   dilate=replace_stride_with_dilation[<span class="hljs-number">2</span>])<br>    <span class="hljs-variable language_">self</span>.avgpool = nn.AdaptiveAvgPool2d((<span class="hljs-number">1</span>, <span class="hljs-number">1</span>))<br>    <span class="hljs-variable language_">self</span>.fc = nn.Linear(<span class="hljs-number">512</span> * block.expansion, num_classes)<br><br>    <span class="hljs-keyword">for</span> m <span class="hljs-keyword">in</span> <span class="hljs-variable language_">self</span>.modules():<br>        <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(m, nn.Conv2d):<br>            nn.init.kaiming_normal_(m.weight, mode=<span class="hljs-string">&#x27;fan_out&#x27;</span>, nonlinearity=<span class="hljs-string">&#x27;relu&#x27;</span>)<br>        <span class="hljs-keyword">elif</span> <span class="hljs-built_in">isinstance</span>(m, (nn.BatchNorm2d, nn.GroupNorm)):<br>            nn.init.constant_(m.weight, <span class="hljs-number">1</span>)<br>            nn.init.constant_(m.bias, <span class="hljs-number">0</span>)<br><br>    <span class="hljs-comment"># Zero-initialize the last BN in each residual branch,</span><br>    <span class="hljs-comment"># so that the residual branch starts with zeros, and each residual block behaves like an identity.</span><br>    <span class="hljs-comment"># This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677</span><br>    <span class="hljs-keyword">if</span> zero_init_residual:<br>        <span class="hljs-keyword">for</span> m <span class="hljs-keyword">in</span> <span class="hljs-variable language_">self</span>.modules():<br>            <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(m, Bottleneck):<br>                nn.init.constant_(m.bn3.weight, <span class="hljs-number">0</span>)<br>            <span class="hljs-keyword">elif</span> <span class="hljs-built_in">isinstance</span>(m, BasicBlock):<br>                nn.init.constant_(m.bn2.weight, <span class="hljs-number">0</span>)<br></code></pre></td></tr></table></figure><h3 id="forward"><a href="#forward" class="headerlink" title="forward()"></a>forward()</h3><p>在 <code>ResNet</code> 中，网络经过层层封装，因此<code>forward()</code> 方法非常简洁。</p><p>数据变换大致流程如下：</p><ul><li>输入的图片形状是$3 \times 224 \times 224$。</li><li>图片经过 <code>conv1</code> 层，输出图片大小为 $ 64 \times 112 \times 112$。</li><li>图片经过 <code>max pool</code> 层，输出图片大小为 $ 64 \times 56 \times 56 $。</li><li>对于 <code>ResNet 18</code>、<code>ResNet 34</code> （使用 <code>BasicBlock</code>）：<ul><li>图片经过 <code>conv2</code> 层，对应于 <code>layer1</code>，输出图片大小为 $ 64 \times 56 \times 56$。<strong>（注意，图片经过这个 <code>layer</code>, 大小是不变的）</strong></li><li>图片经过 <code>conv3</code> 层，对应于 <code>layer2</code>，输出图片大小为 $ 128 \times 28 \times 28$。</li><li>图片经过 <code>conv4</code> 层，对应于 <code>layer3</code>，输出图片大小为 $ 256 \times 14 \times 14$。</li><li>图片经过 <code>conv5</code> 层，对应于 <code>layer4</code>，输出图片大小为 $ 512 \times 7 \times 7$。</li><li>图片经过 <code>avg pool</code> 层，输出大小为 $ 512 \times 1 \times 1$。</li></ul></li><li>对于 <code>ResNet 50</code>、<code>ResNet 101</code>、<code>ResNet 152</code>（使用 <code>Bottleneck</code>）：<ul><li>图片经过 <code>conv2</code> 层，对应于 <code>layer1</code>，输出图片大小为 $ 256 \times 56 \times 56$。<strong>（注意，图片经过这个 <code>layer</code>, 大小是不变的）</strong></li><li>图片经过 <code>conv3</code> 层，对应于 <code>layer2</code>，输出图片大小为 $ 512 \times 28 \times 28$。</li><li>图片经过 <code>conv4</code> 层，对应于 <code>layer3</code>，输出图片大小为 $ 1024 \times 14 \times 14$。</li><li>图片经过 <code>conv5</code> 层，对应于 <code>layer4</code>，输出图片大小为 $ 2048 \times 7 \times 7$。</li><li>图片经过 <code>avg pool</code> 层，输出大小为 $ 2048 \times 1 \times 1$。</li></ul></li><li>图片经过 <code>fc</code> 层，输出维度为 $ num_classes$，表示每个分类的 <code>logits</code>。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">_forward_impl</span>(<span class="hljs-params">self, x</span>):<br>    <span class="hljs-comment"># See note [TorchScript super()]</span><br><br>    <span class="hljs-comment"># conv1</span><br>    <span class="hljs-comment"># x: [3, 224, 224] -&gt; [64, 112, 112]</span><br>    x = <span class="hljs-variable language_">self</span>.conv1(x)<br>    x = <span class="hljs-variable language_">self</span>.bn1(x)<br>    x = <span class="hljs-variable language_">self</span>.relu(x)<br><br>    <span class="hljs-comment"># conv2</span><br>    <span class="hljs-comment"># x: [64, 112, 112] -&gt; [64, 56, 56]</span><br>    x = <span class="hljs-variable language_">self</span>.maxpool(x)<br><br>    <span class="hljs-comment"># x: [64, 56, 56] -&gt; [64, 56, 56]</span><br>    <span class="hljs-comment"># x 经过第一个 layer, 大小是不变的</span><br>    x = <span class="hljs-variable language_">self</span>.layer1(x)<br><br>    <span class="hljs-comment"># conv3</span><br>    <span class="hljs-comment"># x: [64, 56, 56] -&gt; [128, 28, 28]</span><br>    x = <span class="hljs-variable language_">self</span>.layer2(x)<br><br>    <span class="hljs-comment"># conv4</span><br>    <span class="hljs-comment"># x: [128, 28, 28] -&gt; [256, 14, 14]</span><br>    x = <span class="hljs-variable language_">self</span>.layer3(x)<br><br>    <span class="hljs-comment"># conv5</span><br>    <span class="hljs-comment"># x: [256, 14, 14] -&gt; [512, 7, 7]</span><br>    x = <span class="hljs-variable language_">self</span>.layer4(x)<br><br>    <span class="hljs-comment"># x: [512, 7, 7] -&gt; [512, 1, 1]</span><br>    x = <span class="hljs-variable language_">self</span>.avgpool(x)<br>    x = torch.flatten(x, <span class="hljs-number">1</span>)<br>    x = <span class="hljs-variable language_">self</span>.fc(x)<br><br>    <span class="hljs-keyword">return</span> x<br></code></pre></td></tr></table></figure><p>在构造函数中可以看到，上面每个 <code>layer</code> 都是使用 <code>_make_layer()</code> 方法来创建层的，下面来看下 <code>_make_layer()</code> 方法。</p><h3 id="make-layer"><a href="#make-layer" class="headerlink" title="_make_layer()"></a>_make_layer()</h3><p><code>_make_layer()</code>方法的参数如下：</p><ul><li>block：每个 <code>layer</code> 里面使用的 <code>block</code>，可以是 <code>BasicBlock</code>，<code>Bottleneck</code>。</li><li>planes：输出的通道数</li><li>blocks：一个整数，表示该层 <code>layer</code> 有多少个 <code>block</code>。</li><li>stride：第一个 <code>block</code> 的卷积层的 <code>stride</code>，默认为 1。注意，只有在每个 <code>layer</code> 的第一个 <code>block</code> 的第一个卷积层使用该参数。</li><li>dilate：是否使用空洞卷积。</li></ul><p>主要流程如下：</p><ul><li>判断空洞卷积，计算 <code>previous_dilation</code> 参数。</li><li>判断 <code>stride</code> 是否为 1，输入通道和输出通道是否相等。如果这两个条件都不成立，那么表明需要建立一个 1 X 1 的卷积层，来<strong>改变通道数和改变图片大小</strong>。具体是建立 <code>downsample</code> 层，包括 <code>conv1x1 -&gt; norm_layer</code>。</li><li>建立第一个 <code>block</code>，把 <code>downsample</code> 传给 <code>block</code> 作为降采样的层，并且 <code>stride</code> 也使用传入的 <code>stride</code>（stride&#x3D;2）。<strong>后面我们会分析 <code>downsample</code> 层在 <code>BasicBlock</code> 和 <code>Bottleneck</code> 中，具体是怎么用的</strong>。</li><li>改变通道数<code>self.inplanes = planes * block.expansion</code>。<ul><li>在 <code>BasicBlock</code> 里，<code>expansion=1</code>，因此这一步<strong>不会改变通道数</strong>。</li><li>在 <code>Bottleneck</code> 里，<code>expansion=4</code>，因此这一步<strong>会改变通道数</strong>。</li></ul></li><li>图片经过第一个 <code>block</code>后，就会改变通道数和图片大小。接下来 for 循环添加剩下的 <code>block</code>。从第 2 个 <code>block</code> 起，输入和输出通道数是相等的，因此就不用传入 <code>downsample</code> 和 <code>stride</code>（那么 <code>block</code> 的 <code>stride</code> 默认使用 1，下面我们会分析 <code>BasicBlock</code> 和 <code>Bottleneck</code> 的源码）。</li></ul><figure class="highlight mipsasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><code class="hljs mipsasm">def _make_layer(self, <span class="hljs-keyword">block, </span>planes, <span class="hljs-keyword">blocks, </span>stride=<span class="hljs-number">1</span>, <span class="hljs-keyword">dilate=False):</span><br><span class="hljs-keyword"></span>    <span class="hljs-keyword">norm_layer </span>= self._norm_layer<br>    downsample = None<br>    previous_dilation = self.<span class="hljs-keyword">dilation</span><br><span class="hljs-keyword"></span>    if <span class="hljs-keyword">dilate:</span><br><span class="hljs-keyword"></span>        self.<span class="hljs-keyword">dilation </span>*= stride<br>        stride = <span class="hljs-number">1</span><br>    <span class="hljs-comment"># 首先判断 stride 是否为1，输入通道和输出通道是否相等。不相等则使用 1 X 1 的卷积改变大小和通道</span><br>    <span class="hljs-comment">#作为 downsample</span><br>    <span class="hljs-comment"># 在 Resnet 中，每层 layer 传入的 stride =2</span><br>    if stride != <span class="hljs-number">1</span> <span class="hljs-keyword">or </span>self.inplanes != planes * <span class="hljs-keyword">block.expansion:</span><br><span class="hljs-keyword"></span>        downsample = nn.Sequential(<br>            conv1x1(self.inplanes, planes * <span class="hljs-keyword">block.expansion, </span>stride),<br>            <span class="hljs-keyword">norm_layer(planes </span>* <span class="hljs-keyword">block.expansion),</span><br><span class="hljs-keyword"></span>        )<br><br>    layers = []<br>    <span class="hljs-comment"># 然后添加第一个 basic block，把 downsample 传给 BasicBlock 作为降采样的层。</span><br>    layers.append(<span class="hljs-keyword">block(self.inplanes, </span>planes, stride, downsample, self.groups,<br>                        self.<span class="hljs-keyword">base_width, </span>previous_dilation, <span class="hljs-keyword">norm_layer))</span><br><span class="hljs-keyword"></span>    <span class="hljs-comment"># 修改输出的通道数            </span><br>    self.inplanes = planes * <span class="hljs-keyword">block.expansion</span><br><span class="hljs-keyword"></span>    <span class="hljs-comment"># 继续添加这个 layer 里接下来的 BasicBlock</span><br>    for _ in range(<span class="hljs-number">1</span>, <span class="hljs-keyword">blocks):</span><br><span class="hljs-keyword"></span>        layers.append(<span class="hljs-keyword">block(self.inplanes, </span>planes, groups=self.groups,<br>                            <span class="hljs-keyword">base_width=self.base_width, </span><span class="hljs-keyword">dilation=self.dilation,</span><br><span class="hljs-keyword"></span>                            <span class="hljs-keyword">norm_layer=norm_layer))</span><br><span class="hljs-keyword"></span><br>    return nn.Sequential(*layers)<br></code></pre></td></tr></table></figure><p>下面来看 <code>BasicBlock</code> 和 <code>Bottleneck</code> 的源码。</p><h3 id="BasicBlock"><a href="#BasicBlock" class="headerlink" title="BasicBlock"></a>BasicBlock</h3><h4 id="构造函数-1"><a href="#构造函数-1" class="headerlink" title="构造函数"></a>构造函数</h4><p><code>BasicBlock</code> 构造函数的主要参数如下：</p><ul><li>inplanes：输入通道数。</li><li>planes：输出通道数。</li><li>stride：第一个卷积层的 <code>stride</code>。</li><li>downsample：从 <code>layer</code> 中传入的 <code>downsample</code> 层。</li><li>groups：分组卷积的分组数，使用 1</li><li>base_width：每组卷积的通道数，使用 64</li><li>dilation：空洞卷积，为1，表示不使用 空洞卷积</li></ul><p>主要流程如下：</p><ul><li>首先判断是否传入了 <code>norm_layer</code> 层，如果没有，则使用 <code>BatchNorm2d</code>。</li><li>校验参数：<code>groups == 1</code>，<code>base_width == 64</code>，<code>dilation == 1</code>。也就是说，在 <code>BasicBlock</code> 中，不使用空洞卷积和分组卷积。 </li><li>定义第 1 组 <code>conv3x3 -&gt; norm_layer -&gt; relu</code>，这里使用传入的 <code>stride</code> 和 <code>inplanes</code>。（<strong>如果是 <code>layer2</code> ，<code>layer3</code> ，<code>layer4</code> 里的第一个 <code>BasicBlock</code>，那么 <code>stride=2</code>，这里会降采样和改变通道数</strong>）。</li><li>定义第 2 组 <code>conv3x3 -&gt; norm_layer -&gt; relu</code>，这里不使用传入的 <code>stride</code> （默认为 1），输入通道数和输出通道数使用<code>planes</code>，也就是<strong>不需要降采样和改变通道数</strong>。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">BasicBlock</span>(nn.Module):<br>    expansion = <span class="hljs-number">1</span><br>    __constants__ = [<span class="hljs-string">&#x27;downsample&#x27;</span>]<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, inplanes, planes, stride=<span class="hljs-number">1</span>, downsample=<span class="hljs-literal">None</span>, groups=<span class="hljs-number">1</span>,</span><br><span class="hljs-params">                 base_width=<span class="hljs-number">64</span>, dilation=<span class="hljs-number">1</span>, norm_layer=<span class="hljs-literal">None</span></span>):<br>        <span class="hljs-built_in">super</span>(BasicBlock, <span class="hljs-variable language_">self</span>).__init__()<br>        <span class="hljs-keyword">if</span> norm_layer <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>            norm_layer = nn.BatchNorm2d<br>        <span class="hljs-keyword">if</span> groups != <span class="hljs-number">1</span> <span class="hljs-keyword">or</span> base_width != <span class="hljs-number">64</span>:<br>            <span class="hljs-keyword">raise</span> ValueError(<span class="hljs-string">&#x27;BasicBlock only supports groups=1 and base_width=64&#x27;</span>)<br>        <span class="hljs-keyword">if</span> dilation &gt; <span class="hljs-number">1</span>:<br>            <span class="hljs-keyword">raise</span> NotImplementedError(<span class="hljs-string">&quot;Dilation &gt; 1 not supported in BasicBlock&quot;</span>)<br>        <span class="hljs-comment"># Both self.conv1 and self.downsample layers downsample the input when stride != 1</span><br>        <span class="hljs-variable language_">self</span>.conv1 = conv3x3(inplanes, planes, stride)<br>        <span class="hljs-variable language_">self</span>.bn1 = norm_layer(planes)<br>        <span class="hljs-variable language_">self</span>.relu = nn.ReLU(inplace=<span class="hljs-literal">True</span>)<br>        <span class="hljs-variable language_">self</span>.conv2 = conv3x3(planes, planes)<br>        <span class="hljs-variable language_">self</span>.bn2 = norm_layer(planes)<br>        <span class="hljs-variable language_">self</span>.downsample = downsample<br>        <span class="hljs-variable language_">self</span>.stride = stride<br></code></pre></td></tr></table></figure><h4 id="forward-1"><a href="#forward-1" class="headerlink" title="forward()"></a>forward()</h4><p><code>forward()</code> 方法的主要流程如下：</p><ul><li><code>x</code> 赋值给 <code>identity</code>，用于后面的 <code>shortcut</code> 连接。</li><li><code>x</code> 经过第 1 组 <code>conv3x3 -&gt; norm_layer -&gt; relu</code>，如果是 <code>layer2</code> ，<code>layer3</code> ，<code>layer4</code> 里的第一个 <code>BasicBlock</code>，那么 <code>stride=2</code>，第一个卷积层会降采样。</li><li><code>x</code> 经过第 1 组 <code>conv3x3 -&gt; norm_layer</code>，得到 <code>out</code>。</li><li>如果是 <code>layer2</code> ，<code>layer3</code> ，<code>layer4</code> 里的第一个 <code>BasicBlock</code>，那么 <code>downsample</code> 不为空，会经过 <code>downsample</code> 层，得到 <code>identity</code>。</li><li>最后将 <code>identity</code> 和 <code>out</code> 相加，经过 <code>relu</code> ，得到输出。</li></ul><blockquote><p>注意，2 个卷积层都需要经过 <code>relu</code> 层，但它们使用的是同一个 <code>relu</code> 层。</p></blockquote><figure class="highlight crystal"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs crystal"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span></span>(<span class="hljs-keyword">self</span>, x):<br>    identity = x<br>    <span class="hljs-comment"># 如果是 layer2，layer3，layer4 里的第一个 BasicBlock，第一个卷积层会降采样</span><br>    <span class="hljs-keyword">out</span> = <span class="hljs-keyword">self</span>.conv1(x)<br>    <span class="hljs-keyword">out</span> = <span class="hljs-keyword">self</span>.bn1(<span class="hljs-keyword">out</span>)<br>    <span class="hljs-keyword">out</span> = <span class="hljs-keyword">self</span>.relu(<span class="hljs-keyword">out</span>)<br><br>    <span class="hljs-keyword">out</span> = <span class="hljs-keyword">self</span>.conv2(<span class="hljs-keyword">out</span>)<br>    <span class="hljs-keyword">out</span> = <span class="hljs-keyword">self</span>.bn2(<span class="hljs-keyword">out</span>)<br><br>    <span class="hljs-keyword">if</span> <span class="hljs-keyword">self</span>.downsample is not <span class="hljs-symbol">None:</span><br>        identity = <span class="hljs-keyword">self</span>.downsample(x)<br><br>    <span class="hljs-keyword">out</span> += identity<br>    <span class="hljs-keyword">out</span> = <span class="hljs-keyword">self</span>.relu(<span class="hljs-keyword">out</span>)<br><br>    <span class="hljs-keyword">return</span> <span class="hljs-keyword">out</span><br></code></pre></td></tr></table></figure><h3 id="Bottleneck"><a href="#Bottleneck" class="headerlink" title="Bottleneck"></a>Bottleneck</h3><h4 id="构造函数-2"><a href="#构造函数-2" class="headerlink" title="构造函数"></a>构造函数</h4><p>参数如下：</p><ul><li>inplanes：输入通道数。</li><li>planes：输出通道数。</li><li>stride：第一个卷积层的 <code>stride</code>。</li><li>downsample：从 <code>layer</code> 中传入的 <code>downsample</code> 层。</li><li>groups：分组卷积的分组数，使用 1</li><li>base_width：每组卷积的通道数，使用 64</li><li>dilation：空洞卷积，为1，表示不使用 空洞卷积</li></ul><p>主要流程如下：</p><ul><li>首先判断是否传入了 <code>norm_layer</code> 层，如果没有，则使用 <code>BatchNorm2d</code>。</li><li>计算 <code>width</code>，等于传入的 <code>planes</code>，用于中间的 $ 3 \times 3 $ 卷积。 </li><li>定义第 1 组 <code>conv1x1 -&gt; norm_layer</code>，这里不使用传入的 <code>stride</code>，使用 <code>width</code>，作用是进行降维，减少通道数。</li><li>定义第 2 组 <code>conv3x3 -&gt; norm_layer</code>，这里使用传入的 <code>stride</code>，输入通道数和输出通道数使用<code>width</code>。（<strong>如果是 <code>layer2</code> ，<code>layer3</code> ，<code>layer4</code> 里的第一个 <code>Bottleneck</code>，那么 <code>stride=2</code>，这里会降采样</strong>）。</li><li>定义第 3 组 <code>conv1x1 -&gt; norm_layer</code>，这里不使用传入的 <code>stride</code>，使用 <code>planes * self.expansion</code>，作用是进行升维，增加通道数。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Bottleneck</span>(nn.Module):<br>    expansion = <span class="hljs-number">4</span><br>    __constants__ = [<span class="hljs-string">&#x27;downsample&#x27;</span>]<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, inplanes, planes, stride=<span class="hljs-number">1</span>, downsample=<span class="hljs-literal">None</span>, groups=<span class="hljs-number">1</span>,</span><br><span class="hljs-params">                 base_width=<span class="hljs-number">64</span>, dilation=<span class="hljs-number">1</span>, norm_layer=<span class="hljs-literal">None</span></span>):<br>        <span class="hljs-built_in">super</span>(Bottleneck, <span class="hljs-variable language_">self</span>).__init__()<br>        <span class="hljs-keyword">if</span> norm_layer <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>            norm_layer = nn.BatchNorm2d<br><br>        <span class="hljs-comment"># base_width = 64</span><br>        <span class="hljs-comment"># groups =1</span><br>        <span class="hljs-comment"># width = planes</span><br>        width = <span class="hljs-built_in">int</span>(planes * (base_width / <span class="hljs-number">64.</span>)) * groups<br>        <span class="hljs-comment"># Both self.conv2 and self.downsample layers downsample the input when stride != 1</span><br>        <span class="hljs-comment"># 1x1 的卷积是为了降维，减少通道数</span><br>        <span class="hljs-variable language_">self</span>.conv1 = conv1x1(inplanes, width)<br>        <span class="hljs-variable language_">self</span>.bn1 = norm_layer(width)<br>        <span class="hljs-comment"># 3x3 的卷积是为了改变图片大小，不改变通道数</span><br>        <span class="hljs-variable language_">self</span>.conv2 = conv3x3(width, width, stride, groups, dilation)<br>        <span class="hljs-variable language_">self</span>.bn2 = norm_layer(width)<br>        <span class="hljs-comment"># 1x1 的卷积是为了升维，增加通道数，增加到 planes * 4</span><br>        <span class="hljs-variable language_">self</span>.conv3 = conv1x1(width, planes * <span class="hljs-variable language_">self</span>.expansion)<br>        <span class="hljs-variable language_">self</span>.bn3 = norm_layer(planes * <span class="hljs-variable language_">self</span>.expansion)<br>        <span class="hljs-variable language_">self</span>.relu = nn.ReLU(inplace=<span class="hljs-literal">True</span>)<br>        <span class="hljs-variable language_">self</span>.downsample = downsample<br>        <span class="hljs-variable language_">self</span>.stride = stride<br></code></pre></td></tr></table></figure><h4 id="forward-2"><a href="#forward-2" class="headerlink" title="forward()"></a>forward()</h4><p><code>forward()</code> 方法的主要流程如下：</p><ul><li><code>x</code> 赋值给 <code>identity</code>，用于后面的 <code>shortcut</code> 连接。</li><li><code>x</code> 经过第 1 组 <code>conv1x1 -&gt; norm_layer -&gt; relu</code>，作用是进行降维，减少通道数。</li><li><code>x</code> 经过第 2 组 <code>conv3x3 -&gt; norm_layer -&gt; relu</code>。如果是 <code>layer2</code> ，<code>layer3</code> ，<code>layer4</code> 里的第一个 <code>Bottleneck</code>，那么 <code>stride=2</code>，第一个卷积层会降采样。</li><li><code>x</code> 经过第 1 组 <code>conv1x1 -&gt; norm_layer -&gt; relu</code>，作用是进行降维，减少通道数。</li><li>如果是 <code>layer2</code> ，<code>layer3</code> ，<code>layer4</code> 里的第一个 <code>Bottleneck</code>，那么 <code>downsample</code> 不为空，会经过 <code>downsample</code> 层，得到 <code>identity</code>。</li><li>最后将 <code>identity</code> 和 <code>out</code> 相加，经过 <code>relu</code> ，得到输出。</li></ul><blockquote><p>注意，3 个卷积层都需要经过 <code>relu</code> 层，但它们使用的是同一个 <code>relu</code> 层。</p></blockquote><figure class="highlight crystal"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs crystal"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span></span>(<span class="hljs-keyword">self</span>, x):<br>    identity = x<br><br>    <span class="hljs-keyword">out</span> = <span class="hljs-keyword">self</span>.conv1(x)<br>    <span class="hljs-keyword">out</span> = <span class="hljs-keyword">self</span>.bn1(<span class="hljs-keyword">out</span>)<br>    <span class="hljs-keyword">out</span> = <span class="hljs-keyword">self</span>.relu(<span class="hljs-keyword">out</span>)<br><br>    <span class="hljs-keyword">out</span> = <span class="hljs-keyword">self</span>.conv2(<span class="hljs-keyword">out</span>)<br>    <span class="hljs-keyword">out</span> = <span class="hljs-keyword">self</span>.bn2(<span class="hljs-keyword">out</span>)<br>    <span class="hljs-keyword">out</span> = <span class="hljs-keyword">self</span>.relu(<span class="hljs-keyword">out</span>)<br><br>    <span class="hljs-keyword">out</span> = <span class="hljs-keyword">self</span>.conv3(<span class="hljs-keyword">out</span>)<br>    <span class="hljs-keyword">out</span> = <span class="hljs-keyword">self</span>.bn3(<span class="hljs-keyword">out</span>)<br><br>    <span class="hljs-keyword">if</span> <span class="hljs-keyword">self</span>.downsample is not <span class="hljs-symbol">None:</span><br>        identity = <span class="hljs-keyword">self</span>.downsample(x)<br><br>    <span class="hljs-keyword">out</span> += identity<br>    <span class="hljs-keyword">out</span> = <span class="hljs-keyword">self</span>.relu(<span class="hljs-keyword">out</span>)<br><br>    <span class="hljs-keyword">return</span> <span class="hljs-keyword">out</span><br></code></pre></td></tr></table></figure><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>最后，总结一下。</p><ul><li><code>BasicBlock</code>  中有 1 个 $3 \times 3 $卷积层，如果是 <code>layer</code> 的第一个 <code>BasicBlock</code>，那么第一个卷积层的 <code>stride=2</code>，作用是进行降采样。</li><li><code>Bottleneck</code>  中有 2 个 $1 \times 1 $卷积层， 1 个 $3 \times 3 $ 卷积层。先经过第 1 个 $1 \times 1 $卷积层，进行降维，然后经过 $3 \times 3 $卷积层（如果是 <code>layer</code> 的第一个 <code>Bottleneck</code>，那么 $3 \times 3 $ 卷积层的 <code>stride=2</code>，作用是进行降采样），最后经过 $1 \times 1 $卷积层，进行升维 。</li></ul><h3 id="ResNet-18-图解"><a href="#ResNet-18-图解" class="headerlink" title="ResNet 18 图解"></a>ResNet 18 图解</h3><h4 id="layer1"><a href="#layer1" class="headerlink" title="layer1"></a>layer1</h4><p>下面是 <code>ResNet 18</code> ，使用的是 <code>BasicBlock</code> 的 <code>layer1</code>，特点是没有进行降采样，卷积层的 <code>stride = 1</code>，不会降采样。在进行 <code>shortcut</code> 连接时，也没有经过 <code>downsample</code> 层。</p><p><img src="/img/pytorchtrain8/5.png"></p><h4 id="layer2，layer3，layer4"><a href="#layer2，layer3，layer4" class="headerlink" title="layer2，layer3，layer4"></a>layer2，layer3，layer4</h4><p>而 <code>layer2</code>，<code>layer3</code>，<code>layer4</code> 的结构图如下，每个 <code>layer</code> 包含 2 个 <code>BasicBlock</code>，但是第 1 个 <code>BasicBlock</code> 的第 1 个卷积层的 <code>stride = 2</code>，会进行降采样。<strong>在进行 <code>shortcut</code> 连接时，会经过 <code>downsample</code> 层，进行降采样和降维</strong>。</p><p><img src="/img/pytorchtrain8/6.png"></p><h3 id="ResNet-50-图解"><a href="#ResNet-50-图解" class="headerlink" title="ResNet 50 图解"></a>ResNet 50 图解</h3><h4 id="layer1-1"><a href="#layer1-1" class="headerlink" title="layer1"></a>layer1</h4><p>在 <code>layer1</code> 中，首先第一个 <code>Bottleneck</code> 只会进行升维，不会降采样。<code>shortcut</code> 连接前，会经过 <code>downsample</code> 层升维处理。第二个 <code>Bottleneck</code> 的 <code>shortcut</code> 连接不会经过 <code>downsample</code> 层。</p><p><img src="/img/pytorchtrain8/7.png"></p><h4 id="layer2，layer3，layer4-1"><a href="#layer2，layer3，layer4-1" class="headerlink" title="layer2，layer3，layer4"></a>layer2，layer3，layer4</h4><p>而 <code>layer2</code>，<code>layer3</code>，<code>layer4</code> 的结构图如下，每个 <code>layer</code> 包含多个 <code>Bottleneck</code>，但是第 1 个 <code>Bottleneck</code> 的 $ 3 \times 3 $ 卷积层的 <code>stride = 2</code>，会进行降采样。<strong>在进行 <code>shortcut</code> 连接时，会经过 <code>downsample</code> 层，进行降采样和降维</strong>。</p><p><img src="/img/pytorchtrain8/8.png"></p>]]></content>
    
    
    <categories>
      
      <category>深度学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>pytorch</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>深度学习-14-pytorch-7-模型补充操作</title>
    <link href="/2021/08/14/pytorchtrain7/"/>
    <url>/2021/08/14/pytorchtrain7/</url>
    
    <content type="html"><![CDATA[<blockquote><p>创作声明：主要内容参考于张贤同学<a href="https://zhuanlan.zhihu.com/p/265394674">https://zhuanlan.zhihu.com/p/265394674</a></p></blockquote><h2 id="PyTorch-中的模型保存与加载"><a href="#PyTorch-中的模型保存与加载" class="headerlink" title="PyTorch 中的模型保存与加载"></a>PyTorch 中的模型保存与加载</h2><h3 id="序列化与反序列化"><a href="#序列化与反序列化" class="headerlink" title="序列化与反序列化"></a>序列化与反序列化</h3><p>模型在内存中是以对象的逻辑结构保存的，但是在硬盘中是以二进制流的方式保存的。</p><ul><li>序列化是指将内存中的数据以二进制序列的方式保存到硬盘中。PyTorch 的模型保存就是序列化。</li><li>反序列化是指将硬盘中的二进制序列加载到内存中，得到模型的对象。PyTorch 的模型加载就是反序列化。</li></ul><h3 id="torch-save"><a href="#torch-save" class="headerlink" title="torch.save"></a>torch.save</h3><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs routeros">torch.save(obj, f, pickle_module, <span class="hljs-attribute">pickle_protocol</span>=2, <span class="hljs-attribute">_use_new_zipfile_serialization</span>=<span class="hljs-literal">False</span>)<br></code></pre></td></tr></table></figure><p>主要参数：</p><ul><li>obj：保存的对象，可以是模型。也可以是 dict。因为一般在保存模型时，不仅要保存模型，还需要保存优化器、此时对应的 epoch 等参数。这时就可以用 dict 包装起来。 </li><li>f：输出路径</li></ul><p>其中模型保存还有两种方式：</p><h4 id="保存整个-Module"><a href="#保存整个-Module" class="headerlink" title="保存整个 Module"></a>保存整个 Module</h4><p>这种方法比较耗时，保存的文件大</p><figure class="highlight dos"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs dos">torch.savev(<span class="hljs-built_in">net</span>, <span class="hljs-built_in">path</span>)<br></code></pre></td></tr></table></figure><h4 id="只保存模型的参数"><a href="#只保存模型的参数" class="headerlink" title="只保存模型的参数"></a>只保存模型的参数</h4><p>推荐这种方法，运行比较快，保存的文件比较小</p><figure class="highlight dos"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs dos">state_sict = <span class="hljs-built_in">net</span>.state_dict()<br>torch.savev(state_sict, <span class="hljs-built_in">path</span>)<br></code></pre></td></tr></table></figure><p>下面是保存 LeNet 的例子。在网络初始化中，把权值都设置为 2020，然后保存模型。</p><figure class="highlight scss"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><code class="hljs scss">import torch<br>import numpy as np<br>import torch<span class="hljs-selector-class">.nn</span> as nn<br>from common_tools import set_seed<br><br><br>class <span class="hljs-built_in">LeNet2</span>(nn.Module):<br>    def <span class="hljs-built_in">__init__</span>(self, classes):<br>        <span class="hljs-built_in">super</span>(LeNet2, self).<span class="hljs-built_in">__init__</span>()<br>        self.features = nn.<span class="hljs-built_in">Sequential</span>(<br>            nn.<span class="hljs-built_in">Conv2d</span>(<span class="hljs-number">3</span>, <span class="hljs-number">6</span>, <span class="hljs-number">5</span>),<br>            nn.<span class="hljs-built_in">ReLU</span>(),<br>            nn.<span class="hljs-built_in">MaxPool2d</span>(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>),<br>            nn.<span class="hljs-built_in">Conv2d</span>(<span class="hljs-number">6</span>, <span class="hljs-number">16</span>, <span class="hljs-number">5</span>),<br>            nn.<span class="hljs-built_in">ReLU</span>(),<br>            nn.<span class="hljs-built_in">MaxPool2d</span>(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>)<br>        )<br>        self.classifier = nn.<span class="hljs-built_in">Sequential</span>(<br>            nn.<span class="hljs-built_in">Linear</span>(<span class="hljs-number">16</span>*<span class="hljs-number">5</span>*<span class="hljs-number">5</span>, <span class="hljs-number">120</span>),<br>            nn.<span class="hljs-built_in">ReLU</span>(),<br>            nn.<span class="hljs-built_in">Linear</span>(<span class="hljs-number">120</span>, <span class="hljs-number">84</span>),<br>            nn.<span class="hljs-built_in">ReLU</span>(),<br>            nn.<span class="hljs-built_in">Linear</span>(<span class="hljs-number">84</span>, classes)<br>        )<br><br>    def <span class="hljs-built_in">forward</span>(self, x):<br>        x = self.<span class="hljs-built_in">features</span>(x)<br>        x = x.<span class="hljs-built_in">view</span>(x.<span class="hljs-built_in">size</span>()[<span class="hljs-number">0</span>], -<span class="hljs-number">1</span>)<br>        x = self.<span class="hljs-built_in">classifier</span>(x)<br>        return x<br><br>    def <span class="hljs-built_in">initialize</span>(self):<br>        for p in self.<span class="hljs-built_in">parameters</span>():<br>            p.data.<span class="hljs-built_in">fill_</span>(<span class="hljs-number">2020</span>)<br><br><br>net = <span class="hljs-built_in">LeNet2</span>(classes=<span class="hljs-number">2019</span>)<br><br># <span class="hljs-string">&quot;训练&quot;</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;训练前: &quot;</span>, net.features[<span class="hljs-number">0</span>].weight[<span class="hljs-number">0</span>, ...])<br>net.<span class="hljs-built_in">initialize</span>()<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;训练后: &quot;</span>, net.features[<span class="hljs-number">0</span>].weight[<span class="hljs-number">0</span>, ...])<br><br>path_model = <span class="hljs-string">&quot;./model.pkl&quot;</span><br>path_state_dict = <span class="hljs-string">&quot;./model_state_dict.pkl&quot;</span><br><br># 保存整个模型<br>torch.<span class="hljs-built_in">save</span>(net, path_model)<br><br># 保存模型参数<br>net_state_dict = net.<span class="hljs-built_in">state_dict</span>()<br>torch.<span class="hljs-built_in">save</span>(net_state_dict, path_state_dict)<br></code></pre></td></tr></table></figure><p>运行完之后，文件夹中生成了&#96;&#96;&#96;model.pkl&#96;&#96;和<code>model_state_dict.pkl</code>，分别保存了整个网络和网络的参数</p><h3 id="torch-load"><a href="#torch-load" class="headerlink" title="torch.load"></a>torch.load</h3><figure class="highlight pgsql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs pgsql">torch.<span class="hljs-keyword">load</span>(f, map_location=<span class="hljs-keyword">None</span>, pickle_module, **pickle_load_args)<br></code></pre></td></tr></table></figure><p>主要参数：</p><ul><li>f：文件路径</li><li>map_location：指定存在 CPU 或者 GPU。</li></ul><p>加载模型也有两种方式</p><h4 id="加载整个-Module"><a href="#加载整个-Module" class="headerlink" title="加载整个 Module"></a>加载整个 Module</h4><p>如果保存的时候，保存的是整个模型，那么加载时就加载整个模型。这种方法不需要事先创建一个模型对象，也不用知道模型的结构，代码如下：</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs stylus">path_model = <span class="hljs-string">&quot;./model.pkl&quot;</span><br>net_load = torch<span class="hljs-selector-class">.load</span>(path_model)<br><br><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(net_load)</span></span><br></code></pre></td></tr></table></figure><p>输出如下：</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs routeros">LeNet2(<br>  (features): Sequential(<br>    (0): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))<br>    (1): ReLU()<br>    (2): MaxPool2d(<span class="hljs-attribute">kernel_size</span>=2, <span class="hljs-attribute">stride</span>=2, <span class="hljs-attribute">padding</span>=0, <span class="hljs-attribute">dilation</span>=1, <span class="hljs-attribute">ceil_mode</span>=<span class="hljs-literal">False</span>)<br>    (3): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))<br>    (4): ReLU()<br>    (5): MaxPool2d(<span class="hljs-attribute">kernel_size</span>=2, <span class="hljs-attribute">stride</span>=2, <span class="hljs-attribute">padding</span>=0, <span class="hljs-attribute">dilation</span>=1, <span class="hljs-attribute">ceil_mode</span>=<span class="hljs-literal">False</span>)<br>  )<br>  (classifier): Sequential(<br>    (0): Linear(<span class="hljs-attribute">in_features</span>=400, <span class="hljs-attribute">out_features</span>=120, <span class="hljs-attribute">bias</span>=<span class="hljs-literal">True</span>)<br>    (1): ReLU()<br>    (2): Linear(<span class="hljs-attribute">in_features</span>=120, <span class="hljs-attribute">out_features</span>=84, <span class="hljs-attribute">bias</span>=<span class="hljs-literal">True</span>)<br>    (3): ReLU()<br>    (4): Linear(<span class="hljs-attribute">in_features</span>=84, <span class="hljs-attribute">out_features</span>=2019, <span class="hljs-attribute">bias</span>=<span class="hljs-literal">True</span>)<br>  )<br>)<br></code></pre></td></tr></table></figure><h4 id="只加载模型的参数"><a href="#只加载模型的参数" class="headerlink" title="只加载模型的参数"></a>只加载模型的参数</h4><p>如果保存的时候，保存的是模型的参数，那么加载时就参数。这种方法需要事先创建一个模型对象，再使用模型的<code>load_state_dict()</code>方法把参数加载到模型中，代码如下：</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs stylus">path_state_dict = <span class="hljs-string">&quot;./model_state_dict.pkl&quot;</span><br>state_dict_load = torch<span class="hljs-selector-class">.load</span>(path_state_dict)<br>net_new = <span class="hljs-built_in">LeNet2</span>(classes=<span class="hljs-number">2019</span>)<br><br><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(<span class="hljs-string">&quot;加载前: &quot;</span>, net_new.features[<span class="hljs-number">0</span>].weight[<span class="hljs-number">0</span>, ...])</span></span><br>net_new<span class="hljs-selector-class">.load_state_dict</span>(state_dict_load)<br><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(<span class="hljs-string">&quot;加载后: &quot;</span>, net_new.features[<span class="hljs-number">0</span>].weight[<span class="hljs-number">0</span>, ...])</span></span><br></code></pre></td></tr></table></figure><h3 id="模型的断点续训练"><a href="#模型的断点续训练" class="headerlink" title="模型的断点续训练"></a>模型的断点续训练</h3><p>在训练过程中，可能由于某种意外原因如断点等导致训练终止，这时需要重新开始训练。断点续练是在训练过程中每隔一定次数的 epoch 就保存<strong>模型的参数和优化器的参数</strong>，这样如果意外终止训练了，下次就可以重新加载最新的<strong>模型参数和优化器的参数</strong>，在这个基础上继续训练。</p><p>下面的代码中，每隔 5 个 epoch 就保存一次，保存的是一个 dict，包括模型参数、优化器的参数、epoch。然后在 epoch 大于 5 时，就<code>break</code>模拟训练意外终止。关键代码如下：</p><figure class="highlight pgsql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs pgsql"><span class="hljs-keyword">if</span> (epoch+<span class="hljs-number">1</span>) % checkpoint_interval == <span class="hljs-number">0</span>:<br><br>    <span class="hljs-keyword">checkpoint</span> = &#123;&quot;model_state_dict&quot;: net.state_dict(),<br>                  &quot;optimizer_state_dict&quot;: optimizer.state_dict(),<br>                  &quot;epoch&quot;: epoch&#125;<br>    path_checkpoint = &quot;./checkpoint_&#123;&#125;_epoch.pkl&quot;.format(epoch)<br>    torch.save(<span class="hljs-keyword">checkpoint</span>, path_checkpoint)<br></code></pre></td></tr></table></figure><p>在 epoch 大于 5 时，就<code>break</code>模拟训练意外终止</p><figure class="highlight axapta"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs axapta"><span class="hljs-keyword">if</span> epoch &gt; <span class="hljs-number">5</span>:<br>    <span class="hljs-keyword">print</span>(<span class="hljs-string">&quot;训练意外中断...&quot;</span>)<br>    <span class="hljs-keyword">break</span><br></code></pre></td></tr></table></figure><p>断点续训练的恢复代码如下：</p><figure class="highlight pgsql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs pgsql">path_checkpoint = &quot;./checkpoint_4_epoch.pkl&quot;<br><span class="hljs-keyword">checkpoint</span> = torch.<span class="hljs-keyword">load</span>(path_checkpoint)<br><br>net.load_state_dict(<span class="hljs-keyword">checkpoint</span>[<span class="hljs-string">&#x27;model_state_dict&#x27;</span>])<br><br>optimizer.load_state_dict(<span class="hljs-keyword">checkpoint</span>[<span class="hljs-string">&#x27;optimizer_state_dict&#x27;</span>])<br><br>start_epoch = <span class="hljs-keyword">checkpoint</span>[<span class="hljs-string">&#x27;epoch&#x27;</span>]<br><br>scheduler.last_epoch = start_epoch<br></code></pre></td></tr></table></figure><p>需要注意的是，还要设置<code>scheduler.last_epoch</code>参数为保存的 epoch。模型训练的起始 epoch 也要修改为保存的 epoch。</p><h2 id="Finetune"><a href="#Finetune" class="headerlink" title="Finetune"></a>Finetune</h2><p>迁移学习：把在 source domain 任务上的学习到的模型应用到 target domain 的任务。</p><p><strong>Finetune（微调）</strong> 就是一种迁移学习的方法。比如做人脸识别，可以把 ImageNet 看作 source domain，人脸数据集看作 target domain。通常来说 source domain 要比 target domain 大得多。可以利用 ImageNet 训练好的网络应用到人脸识别中。</p><p>对于一个模型，通常可以分为前面的 feature extractor (卷积层)和后面的 classifier，在 Finetune 时，通常不改变 feature extractor 的权值，也就是冻结卷积层；并且改变最后一个全连接层的输出来适应目标任务，训练后面 classifier 的权值，这就是 Finetune。通常 target domain 的数据比较小，不足以训练全部参数，容易导致过拟合，因此不改变 feature extractor 的权值。</p><p>Finetune 步骤如下：</p><ol><li>获取预训练模型的参数</li><li>使用<code>load_state_dict()</code>把参数加载到模型中</li><li>修改输出层</li><li>固定 feature extractor 的参数。这部分通常有 2 种做法：<ol><li>固定卷积层的预训练参数。可以设置<code>requires_grad=False</code>或者<code>lr=0</code></li><li>可以通过<code>params_group</code>给 feature extractor 设置一个较小的学习率</li></ol></li></ol><p>下面微调 ResNet18，用于蜜蜂和蚂蚁图片的二分类。训练集每类数据各 120 张，验证集每类数据各 70 张图片。</p><p>数据下载地址：<a href="http://download.pytorch.org/tutorial/hymenoptera_data.zip">http://download.pytorch.org/tutorial/hymenoptera_data.zip</a></p><p>预训练好的模型参数下载地址：<a href="http://download.pytorch.org/models/resnet18-5c106cde.pth">http://download.pytorch.org/models/resnet18-5c106cde.pth</a></p><p>网络代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">模型finetune方法</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-keyword">import</span> os<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<br><span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> DataLoader<br><span class="hljs-keyword">import</span> torchvision.transforms <span class="hljs-keyword">as</span> transforms<br><span class="hljs-keyword">import</span> torch.optim <span class="hljs-keyword">as</span> optim<br><span class="hljs-keyword">from</span> matplotlib <span class="hljs-keyword">import</span> pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-keyword">from</span> lesson2.rmb_classification.tools.my_dataset <span class="hljs-keyword">import</span> AntsDataset<br><span class="hljs-keyword">from</span> common_tools <span class="hljs-keyword">import</span> set_seed<br><span class="hljs-keyword">import</span> torchvision.models <span class="hljs-keyword">as</span> models<br><span class="hljs-keyword">import</span> enviroments<br>BASEDIR = os.path.dirname(os.path.abspath(__file__))<br>device = torch.device(<span class="hljs-string">&quot;cuda&quot;</span> <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> <span class="hljs-string">&quot;cpu&quot;</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;use device :&#123;&#125;&quot;</span>.<span class="hljs-built_in">format</span>(device))<br><br>set_seed(<span class="hljs-number">1</span>)  <span class="hljs-comment"># 设置随机种子</span><br>label_name = &#123;<span class="hljs-string">&quot;ants&quot;</span>: <span class="hljs-number">0</span>, <span class="hljs-string">&quot;bees&quot;</span>: <span class="hljs-number">1</span>&#125;<br><br><span class="hljs-comment"># 参数设置</span><br>MAX_EPOCH = <span class="hljs-number">25</span><br>BATCH_SIZE = <span class="hljs-number">16</span><br>LR = <span class="hljs-number">0.001</span><br>log_interval = <span class="hljs-number">10</span><br>val_interval = <span class="hljs-number">1</span><br>classes = <span class="hljs-number">2</span><br>start_epoch = -<span class="hljs-number">1</span><br>lr_decay_step = <span class="hljs-number">7</span><br><br><br><span class="hljs-comment"># ============================ step 1/5 数据 ============================</span><br>data_dir = enviroments.hymenoptera_data_dir<br>train_dir = os.path.join(data_dir, <span class="hljs-string">&quot;train&quot;</span>)<br>valid_dir = os.path.join(data_dir, <span class="hljs-string">&quot;val&quot;</span>)<br><br>norm_mean = [<span class="hljs-number">0.485</span>, <span class="hljs-number">0.456</span>, <span class="hljs-number">0.406</span>]<br>norm_std = [<span class="hljs-number">0.229</span>, <span class="hljs-number">0.224</span>, <span class="hljs-number">0.225</span>]<br><br>train_transform = transforms.Compose([<br>    transforms.RandomResizedCrop(<span class="hljs-number">224</span>),<br>    transforms.RandomHorizontalFlip(),<br>    transforms.ToTensor(),<br>    transforms.Normalize(norm_mean, norm_std),<br>])<br><br>valid_transform = transforms.Compose([<br>    transforms.Resize(<span class="hljs-number">256</span>),<br>    transforms.CenterCrop(<span class="hljs-number">224</span>),<br>    transforms.ToTensor(),<br>    transforms.Normalize(norm_mean, norm_std),<br>])<br><br><span class="hljs-comment"># 构建MyDataset实例</span><br>train_data = AntsDataset(data_dir=train_dir, transform=train_transform)<br>valid_data = AntsDataset(data_dir=valid_dir, transform=valid_transform)<br><br><span class="hljs-comment"># 构建DataLoder</span><br>train_loader = DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=<span class="hljs-literal">True</span>)<br>valid_loader = DataLoader(dataset=valid_data, batch_size=BATCH_SIZE)<br><br><span class="hljs-comment"># ============================ step 2/5 模型 ============================</span><br><br><span class="hljs-comment"># 1/3 构建模型</span><br>resnet18_ft = models.resnet18()<br><br><span class="hljs-comment"># 2/3 加载参数</span><br><span class="hljs-comment"># flag = 0</span><br>flag = <span class="hljs-number">1</span><br><span class="hljs-keyword">if</span> flag:<br>    path_pretrained_model = enviroments.resnet18_path<br>    state_dict_load = torch.load(path_pretrained_model)<br>    resnet18_ft.load_state_dict(state_dict_load)<br><br><span class="hljs-comment"># 法1 : 冻结卷积层</span><br>flag_m1 = <span class="hljs-number">0</span><br><span class="hljs-comment"># flag_m1 = 1</span><br><span class="hljs-keyword">if</span> flag_m1:<br>    <span class="hljs-keyword">for</span> param <span class="hljs-keyword">in</span> resnet18_ft.parameters():<br>        param.requires_grad = <span class="hljs-literal">False</span><br>    <span class="hljs-comment"># print(&quot;conv1.weights[0, 0, ...]:\n &#123;&#125;&quot;.format(resnet18_ft.conv1.weight[0, 0, ...]))</span><br><br><br><span class="hljs-comment"># 3/3 替换fc层</span><br><span class="hljs-comment"># 首先拿到 fc 层的输入个数</span><br>num_ftrs = resnet18_ft.fc.in_features<br><span class="hljs-comment"># 然后构造新的 fc 层替换原来的 fc 层</span><br>resnet18_ft.fc = nn.Linear(num_ftrs, classes)<br><br><br>resnet18_ft.to(device)<br><span class="hljs-comment"># ============================ step 3/5 损失函数 ============================</span><br>criterion = nn.CrossEntropyLoss()                                                   <span class="hljs-comment"># 选择损失函数</span><br><br><span class="hljs-comment"># ============================ step 4/5 优化器 ============================</span><br><span class="hljs-comment"># 法2 : conv 小学习率</span><br>flag = <span class="hljs-number">0</span><br><span class="hljs-comment"># flag = 1</span><br><span class="hljs-keyword">if</span> flag:<br>    <span class="hljs-comment"># 首先获取全连接层参数的地址</span><br>    fc_params_id = <span class="hljs-built_in">list</span>(<span class="hljs-built_in">map</span>(<span class="hljs-built_in">id</span>, resnet18_ft.fc.parameters()))  <span class="hljs-comment"># 返回的是parameters的 内存地址</span><br>    <span class="hljs-comment"># 然后使用 filter 过滤不属于全连接层的参数，也就是保留卷积层的参数</span><br>    base_params = <span class="hljs-built_in">filter</span>(<span class="hljs-keyword">lambda</span> p: <span class="hljs-built_in">id</span>(p) <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> fc_params_id, resnet18_ft.parameters())<br>    <span class="hljs-comment"># 设置优化器的分组学习率，传入一个 list，包含 2 个元素，每个元素是字典。对应 2 个参数组</span><br>    optimizer = optim.SGD([&#123;<span class="hljs-string">&#x27;params&#x27;</span>: base_params, <span class="hljs-string">&#x27;lr&#x27;</span>: LR * <span class="hljs-number">0.1</span>&#125;, &#123;<span class="hljs-string">&#x27;params&#x27;</span>: resnet18_ft.fc.parameters(), <span class="hljs-string">&#x27;lr&#x27;</span>: LR&#125;],<br>                          momentum=<span class="hljs-number">0.9</span>)<br><br><span class="hljs-keyword">else</span>:<br>    optimizer = optim.SGD(resnet18_ft.parameters(), lr=LR, momentum=<span class="hljs-number">0.9</span>)               <span class="hljs-comment"># 选择优化器</span><br><br>scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=lr_decay_step, gamma=<span class="hljs-number">0.1</span>)     <span class="hljs-comment"># 设置学习率下降策略</span><br><br><br><span class="hljs-comment"># ============================ step 5/5 训练 ============================</span><br>train_curve = <span class="hljs-built_in">list</span>()<br>valid_curve = <span class="hljs-built_in">list</span>()<br><br><span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(start_epoch + <span class="hljs-number">1</span>, MAX_EPOCH):<br><br>    loss_mean = <span class="hljs-number">0.</span><br>    correct = <span class="hljs-number">0.</span><br>    total = <span class="hljs-number">0.</span><br><br>    resnet18_ft.train()<br>    <span class="hljs-keyword">for</span> i, data <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(train_loader):<br><br>        <span class="hljs-comment"># forward</span><br>        inputs, labels = data<br>        inputs, labels = inputs.to(device), labels.to(device)<br>        outputs = resnet18_ft(inputs)<br><br>        <span class="hljs-comment"># backward</span><br>        optimizer.zero_grad()<br>        loss = criterion(outputs, labels)<br>        loss.backward()<br><br>        <span class="hljs-comment"># update weights</span><br>        optimizer.step()<br><br>        <span class="hljs-comment"># 统计分类情况</span><br>        _, predicted = torch.<span class="hljs-built_in">max</span>(outputs.data, <span class="hljs-number">1</span>)<br>        total += labels.size(<span class="hljs-number">0</span>)<br>        correct += (predicted == labels).squeeze().cpu().<span class="hljs-built_in">sum</span>().numpy()<br><br>        <span class="hljs-comment"># 打印训练信息</span><br>        loss_mean += loss.item()<br>        train_curve.append(loss.item())<br>        <span class="hljs-keyword">if</span> (i+<span class="hljs-number">1</span>) % log_interval == <span class="hljs-number">0</span>:<br>            loss_mean = loss_mean / log_interval<br>            <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Training:Epoch[&#123;:0&gt;3&#125;/&#123;:0&gt;3&#125;] Iteration[&#123;:0&gt;3&#125;/&#123;:0&gt;3&#125;] Loss: &#123;:.4f&#125; Acc:&#123;:.2%&#125;&quot;</span>.<span class="hljs-built_in">format</span>(<br>                epoch, MAX_EPOCH, i+<span class="hljs-number">1</span>, <span class="hljs-built_in">len</span>(train_loader), loss_mean, correct / total))<br>            loss_mean = <span class="hljs-number">0.</span><br><br>            <span class="hljs-comment"># if flag_m1:</span><br>            <span class="hljs-comment"># print(&quot;epoch:&#123;&#125; conv1.weights[0, 0, ...] :\n &#123;&#125;&quot;.format(epoch, resnet18_ft.conv1.weight[0, 0, ...]))</span><br><br>    scheduler.step()  <span class="hljs-comment"># 更新学习率</span><br><br>    <span class="hljs-comment"># validate the model</span><br>    <span class="hljs-keyword">if</span> (epoch+<span class="hljs-number">1</span>) % val_interval == <span class="hljs-number">0</span>:<br><br>        correct_val = <span class="hljs-number">0.</span><br>        total_val = <span class="hljs-number">0.</span><br>        loss_val = <span class="hljs-number">0.</span><br>        resnet18_ft.<span class="hljs-built_in">eval</span>()<br>        <span class="hljs-keyword">with</span> torch.no_grad():<br>            <span class="hljs-keyword">for</span> j, data <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(valid_loader):<br>                inputs, labels = data<br>                inputs, labels = inputs.to(device), labels.to(device)<br><br>                outputs = resnet18_ft(inputs)<br>                loss = criterion(outputs, labels)<br><br>                _, predicted = torch.<span class="hljs-built_in">max</span>(outputs.data, <span class="hljs-number">1</span>)<br>                total_val += labels.size(<span class="hljs-number">0</span>)<br>                correct_val += (predicted == labels).squeeze().cpu().<span class="hljs-built_in">sum</span>().numpy()<br><br>                loss_val += loss.item()<br><br>            loss_val_mean = loss_val/<span class="hljs-built_in">len</span>(valid_loader)<br>            valid_curve.append(loss_val_mean)<br>            <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Valid:\t Epoch[&#123;:0&gt;3&#125;/&#123;:0&gt;3&#125;] Iteration[&#123;:0&gt;3&#125;/&#123;:0&gt;3&#125;] Loss: &#123;:.4f&#125; Acc:&#123;:.2%&#125;&quot;</span>.<span class="hljs-built_in">format</span>(<br>                epoch, MAX_EPOCH, j+<span class="hljs-number">1</span>, <span class="hljs-built_in">len</span>(valid_loader), loss_val_mean, correct_val / total_val))<br>        resnet18_ft.train()<br><br>train_x = <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(train_curve))<br>train_y = train_curve<br><br>train_iters = <span class="hljs-built_in">len</span>(train_loader)<br>valid_x = np.arange(<span class="hljs-number">1</span>, <span class="hljs-built_in">len</span>(valid_curve)+<span class="hljs-number">1</span>) * train_iters*val_interval <span class="hljs-comment"># 由于valid中记录的是epochloss，需要对记录点进行转换到iterations</span><br>valid_y = valid_curve<br><br>plt.plot(train_x, train_y, label=<span class="hljs-string">&#x27;Train&#x27;</span>)<br>plt.plot(valid_x, valid_y, label=<span class="hljs-string">&#x27;Valid&#x27;</span>)<br><br>plt.legend(loc=<span class="hljs-string">&#x27;upper right&#x27;</span>)<br>plt.ylabel(<span class="hljs-string">&#x27;loss value&#x27;</span>)<br>plt.xlabel(<span class="hljs-string">&#x27;Iteration&#x27;</span>)<br>plt.show()<br><br></code></pre></td></tr></table></figure><h3 id="不使用-Finetune"><a href="#不使用-Finetune" class="headerlink" title="不使用 Finetune"></a>不使用 Finetune</h3><p>第一次我们首先不使用 Finetune，而是从零开始训练模型，这时只需要修改全连接层即可：</p><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs ini"><span class="hljs-comment"># 首先拿到 fc 层的输入个数</span><br><span class="hljs-attr">num_ftrs</span> = resnet18_ft.fc.in_features<br><span class="hljs-comment"># 然后构造新的 fc 层替换原来的 fc 层</span><br><span class="hljs-attr">resnet18_ft.fc</span> = nn.Linear(num_ftrs, classes)<br></code></pre></td></tr></table></figure><p>输出如下：</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">use</span> device :cpu<br><span class="hljs-attribute">Training</span>:Epoch[<span class="hljs-number">000</span>/<span class="hljs-number">025</span>] Iteration[<span class="hljs-number">010</span>/<span class="hljs-number">016</span>] Loss: <span class="hljs-number">0</span>.<span class="hljs-number">7192</span> Acc:<span class="hljs-number">47</span>.<span class="hljs-number">50</span>%<br><span class="hljs-attribute">Valid</span>:     Epoch[<span class="hljs-number">000</span>/<span class="hljs-number">025</span>] Iteration[<span class="hljs-number">010</span>/<span class="hljs-number">010</span>] Loss: <span class="hljs-number">0</span>.<span class="hljs-number">6885</span> Acc:<span class="hljs-number">51</span>.<span class="hljs-number">63</span>%<br><span class="hljs-attribute">Training</span>:Epoch[<span class="hljs-number">001</span>/<span class="hljs-number">025</span>] Iteration[<span class="hljs-number">010</span>/<span class="hljs-number">016</span>] Loss: <span class="hljs-number">0</span>.<span class="hljs-number">6568</span> Acc:<span class="hljs-number">60</span>.<span class="hljs-number">62</span>%<br><span class="hljs-attribute">Valid</span>:     Epoch[<span class="hljs-number">001</span>/<span class="hljs-number">025</span>] Iteration[<span class="hljs-number">010</span>/<span class="hljs-number">010</span>] Loss: <span class="hljs-number">0</span>.<span class="hljs-number">6360</span> Acc:<span class="hljs-number">59</span>.<span class="hljs-number">48</span>%<br><span class="hljs-attribute">Training</span>:Epoch[<span class="hljs-number">002</span>/<span class="hljs-number">025</span>] Iteration[<span class="hljs-number">010</span>/<span class="hljs-number">016</span>] Loss: <span class="hljs-number">0</span>.<span class="hljs-number">6411</span> Acc:<span class="hljs-number">60</span>.<span class="hljs-number">62</span>%<br><span class="hljs-attribute">Valid</span>:     Epoch[<span class="hljs-number">002</span>/<span class="hljs-number">025</span>] Iteration[<span class="hljs-number">010</span>/<span class="hljs-number">010</span>] Loss: <span class="hljs-number">0</span>.<span class="hljs-number">6191</span> Acc:<span class="hljs-number">66</span>.<span class="hljs-number">01</span>%<br><span class="hljs-attribute">Training</span>:Epoch[<span class="hljs-number">003</span>/<span class="hljs-number">025</span>] Iteration[<span class="hljs-number">010</span>/<span class="hljs-number">016</span>] Loss: <span class="hljs-number">0</span>.<span class="hljs-number">5765</span> Acc:<span class="hljs-number">71</span>.<span class="hljs-number">25</span>%<br><span class="hljs-attribute">Valid</span>:     Epoch[<span class="hljs-number">003</span>/<span class="hljs-number">025</span>] Iteration[<span class="hljs-number">010</span>/<span class="hljs-number">010</span>] Loss: <span class="hljs-number">0</span>.<span class="hljs-number">6179</span> Acc:<span class="hljs-number">67</span>.<span class="hljs-number">32</span>%<br><span class="hljs-attribute">Training</span>:Epoch[<span class="hljs-number">004</span>/<span class="hljs-number">025</span>] Iteration[<span class="hljs-number">010</span>/<span class="hljs-number">016</span>] Loss: <span class="hljs-number">0</span>.<span class="hljs-number">6074</span> Acc:<span class="hljs-number">67</span>.<span class="hljs-number">50</span>%<br><span class="hljs-attribute">Valid</span>:     Epoch[<span class="hljs-number">004</span>/<span class="hljs-number">025</span>] Iteration[<span class="hljs-number">010</span>/<span class="hljs-number">010</span>] Loss: <span class="hljs-number">0</span>.<span class="hljs-number">6251</span> Acc:<span class="hljs-number">62</span>.<span class="hljs-number">75</span>%<br><span class="hljs-attribute">Training</span>:Epoch[<span class="hljs-number">005</span>/<span class="hljs-number">025</span>] Iteration[<span class="hljs-number">010</span>/<span class="hljs-number">016</span>] Loss: <span class="hljs-number">0</span>.<span class="hljs-number">6177</span> Acc:<span class="hljs-number">58</span>.<span class="hljs-number">75</span>%<br><span class="hljs-attribute">Valid</span>:     Epoch[<span class="hljs-number">005</span>/<span class="hljs-number">025</span>] Iteration[<span class="hljs-number">010</span>/<span class="hljs-number">010</span>] Loss: <span class="hljs-number">0</span>.<span class="hljs-number">6541</span> Acc:<span class="hljs-number">64</span>.<span class="hljs-number">71</span>%<br><span class="hljs-attribute">Training</span>:Epoch[<span class="hljs-number">006</span>/<span class="hljs-number">025</span>] Iteration[<span class="hljs-number">010</span>/<span class="hljs-number">016</span>] Loss: <span class="hljs-number">0</span>.<span class="hljs-number">6103</span> Acc:<span class="hljs-number">65</span>.<span class="hljs-number">62</span>%<br><span class="hljs-attribute">Valid</span>:     Epoch[<span class="hljs-number">006</span>/<span class="hljs-number">025</span>] Iteration[<span class="hljs-number">010</span>/<span class="hljs-number">010</span>] Loss: <span class="hljs-number">0</span>.<span class="hljs-number">7100</span> Acc:<span class="hljs-number">60</span>.<span class="hljs-number">78</span>%<br><span class="hljs-attribute">Training</span>:Epoch[<span class="hljs-number">007</span>/<span class="hljs-number">025</span>] Iteration[<span class="hljs-number">010</span>/<span class="hljs-number">016</span>] Loss: <span class="hljs-number">0</span>.<span class="hljs-number">6560</span> Acc:<span class="hljs-number">60</span>.<span class="hljs-number">62</span>%<br><span class="hljs-attribute">Valid</span>:     Epoch[<span class="hljs-number">007</span>/<span class="hljs-number">025</span>] Iteration[<span class="hljs-number">010</span>/<span class="hljs-number">010</span>] Loss: <span class="hljs-number">0</span>.<span class="hljs-number">6019</span> Acc:<span class="hljs-number">67</span>.<span class="hljs-number">32</span>%<br><span class="hljs-attribute">Training</span>:Epoch[<span class="hljs-number">008</span>/<span class="hljs-number">025</span>] Iteration[<span class="hljs-number">010</span>/<span class="hljs-number">016</span>] Loss: <span class="hljs-number">0</span>.<span class="hljs-number">5454</span> Acc:<span class="hljs-number">70</span>.<span class="hljs-number">62</span>%<br><span class="hljs-attribute">Valid</span>:     Epoch[<span class="hljs-number">008</span>/<span class="hljs-number">025</span>] Iteration[<span class="hljs-number">010</span>/<span class="hljs-number">010</span>] Loss: <span class="hljs-number">0</span>.<span class="hljs-number">5761</span> Acc:<span class="hljs-number">71</span>.<span class="hljs-number">90</span>%<br><span class="hljs-attribute">Training</span>:Epoch[<span class="hljs-number">009</span>/<span class="hljs-number">025</span>] Iteration[<span class="hljs-number">010</span>/<span class="hljs-number">016</span>] Loss: <span class="hljs-number">0</span>.<span class="hljs-number">5499</span> Acc:<span class="hljs-number">71</span>.<span class="hljs-number">25</span>%<br><span class="hljs-attribute">Valid</span>:     Epoch[<span class="hljs-number">009</span>/<span class="hljs-number">025</span>] Iteration[<span class="hljs-number">010</span>/<span class="hljs-number">010</span>] Loss: <span class="hljs-number">0</span>.<span class="hljs-number">5598</span> Acc:<span class="hljs-number">71</span>.<span class="hljs-number">90</span>%<br><span class="hljs-attribute">Training</span>:Epoch[<span class="hljs-number">010</span>/<span class="hljs-number">025</span>] Iteration[<span class="hljs-number">010</span>/<span class="hljs-number">016</span>] Loss: <span class="hljs-number">0</span>.<span class="hljs-number">5466</span> Acc:<span class="hljs-number">69</span>.<span class="hljs-number">38</span>%<br><span class="hljs-attribute">Valid</span>:     Epoch[<span class="hljs-number">010</span>/<span class="hljs-number">025</span>] Iteration[<span class="hljs-number">010</span>/<span class="hljs-number">010</span>] Loss: <span class="hljs-number">0</span>.<span class="hljs-number">5535</span> Acc:<span class="hljs-number">70</span>.<span class="hljs-number">59</span>%<br><span class="hljs-attribute">Training</span>:Epoch[<span class="hljs-number">011</span>/<span class="hljs-number">025</span>] Iteration[<span class="hljs-number">010</span>/<span class="hljs-number">016</span>] Loss: <span class="hljs-number">0</span>.<span class="hljs-number">5310</span> Acc:<span class="hljs-number">68</span>.<span class="hljs-number">12</span>%<br><span class="hljs-attribute">Valid</span>:     Epoch[<span class="hljs-number">011</span>/<span class="hljs-number">025</span>] Iteration[<span class="hljs-number">010</span>/<span class="hljs-number">010</span>] Loss: <span class="hljs-number">0</span>.<span class="hljs-number">5700</span> Acc:<span class="hljs-number">70</span>.<span class="hljs-number">59</span>%<br><span class="hljs-attribute">Training</span>:Epoch[<span class="hljs-number">012</span>/<span class="hljs-number">025</span>] Iteration[<span class="hljs-number">010</span>/<span class="hljs-number">016</span>] Loss: <span class="hljs-number">0</span>.<span class="hljs-number">5024</span> Acc:<span class="hljs-number">72</span>.<span class="hljs-number">50</span>%<br><span class="hljs-attribute">Valid</span>:     Epoch[<span class="hljs-number">012</span>/<span class="hljs-number">025</span>] Iteration[<span class="hljs-number">010</span>/<span class="hljs-number">010</span>] Loss: <span class="hljs-number">0</span>.<span class="hljs-number">5537</span> Acc:<span class="hljs-number">71</span>.<span class="hljs-number">90</span>%<br><span class="hljs-attribute">Training</span>:Epoch[<span class="hljs-number">013</span>/<span class="hljs-number">025</span>] Iteration[<span class="hljs-number">010</span>/<span class="hljs-number">016</span>] Loss: <span class="hljs-number">0</span>.<span class="hljs-number">5542</span> Acc:<span class="hljs-number">71</span>.<span class="hljs-number">25</span>%<br><span class="hljs-attribute">Valid</span>:     Epoch[<span class="hljs-number">013</span>/<span class="hljs-number">025</span>] Iteration[<span class="hljs-number">010</span>/<span class="hljs-number">010</span>] Loss: <span class="hljs-number">0</span>.<span class="hljs-number">5836</span> Acc:<span class="hljs-number">71</span>.<span class="hljs-number">90</span>%<br><span class="hljs-attribute">Training</span>:Epoch[<span class="hljs-number">014</span>/<span class="hljs-number">025</span>] Iteration[<span class="hljs-number">010</span>/<span class="hljs-number">016</span>] Loss: <span class="hljs-number">0</span>.<span class="hljs-number">5458</span> Acc:<span class="hljs-number">71</span>.<span class="hljs-number">88</span>%<br><span class="hljs-attribute">Valid</span>:     Epoch[<span class="hljs-number">014</span>/<span class="hljs-number">025</span>] Iteration[<span class="hljs-number">010</span>/<span class="hljs-number">010</span>] Loss: <span class="hljs-number">0</span>.<span class="hljs-number">5714</span> Acc:<span class="hljs-number">71</span>.<span class="hljs-number">24</span>%<br><span class="hljs-attribute">Training</span>:Epoch[<span class="hljs-number">015</span>/<span class="hljs-number">025</span>] Iteration[<span class="hljs-number">010</span>/<span class="hljs-number">016</span>] Loss: <span class="hljs-number">0</span>.<span class="hljs-number">5331</span> Acc:<span class="hljs-number">72</span>.<span class="hljs-number">50</span>%<br><span class="hljs-attribute">Valid</span>:     Epoch[<span class="hljs-number">015</span>/<span class="hljs-number">025</span>] Iteration[<span class="hljs-number">010</span>/<span class="hljs-number">010</span>] Loss: <span class="hljs-number">0</span>.<span class="hljs-number">5613</span> Acc:<span class="hljs-number">73</span>.<span class="hljs-number">20</span>%<br><span class="hljs-attribute">Training</span>:Epoch[<span class="hljs-number">016</span>/<span class="hljs-number">025</span>] Iteration[<span class="hljs-number">010</span>/<span class="hljs-number">016</span>] Loss: <span class="hljs-number">0</span>.<span class="hljs-number">5296</span> Acc:<span class="hljs-number">71</span>.<span class="hljs-number">25</span>%<br><span class="hljs-attribute">Valid</span>:     Epoch[<span class="hljs-number">016</span>/<span class="hljs-number">025</span>] Iteration[<span class="hljs-number">010</span>/<span class="hljs-number">010</span>] Loss: <span class="hljs-number">0</span>.<span class="hljs-number">5646</span> Acc:<span class="hljs-number">71</span>.<span class="hljs-number">24</span>%<br><span class="hljs-attribute">Training</span>:Epoch[<span class="hljs-number">017</span>/<span class="hljs-number">025</span>] Iteration[<span class="hljs-number">010</span>/<span class="hljs-number">016</span>] Loss: <span class="hljs-number">0</span>.<span class="hljs-number">5039</span> Acc:<span class="hljs-number">75</span>.<span class="hljs-number">00</span>%<br><span class="hljs-attribute">Valid</span>:     Epoch[<span class="hljs-number">017</span>/<span class="hljs-number">025</span>] Iteration[<span class="hljs-number">010</span>/<span class="hljs-number">010</span>] Loss: <span class="hljs-number">0</span>.<span class="hljs-number">5643</span> Acc:<span class="hljs-number">71</span>.<span class="hljs-number">24</span>%<br><span class="hljs-attribute">Training</span>:Epoch[<span class="hljs-number">018</span>/<span class="hljs-number">025</span>] Iteration[<span class="hljs-number">010</span>/<span class="hljs-number">016</span>] Loss: <span class="hljs-number">0</span>.<span class="hljs-number">5351</span> Acc:<span class="hljs-number">73</span>.<span class="hljs-number">75</span>%<br><span class="hljs-attribute">Valid</span>:     Epoch[<span class="hljs-number">018</span>/<span class="hljs-number">025</span>] Iteration[<span class="hljs-number">010</span>/<span class="hljs-number">010</span>] Loss: <span class="hljs-number">0</span>.<span class="hljs-number">5745</span> Acc:<span class="hljs-number">71</span>.<span class="hljs-number">24</span>%<br><span class="hljs-attribute">Training</span>:Epoch[<span class="hljs-number">019</span>/<span class="hljs-number">025</span>] Iteration[<span class="hljs-number">010</span>/<span class="hljs-number">016</span>] Loss: <span class="hljs-number">0</span>.<span class="hljs-number">5441</span> Acc:<span class="hljs-number">69</span>.<span class="hljs-number">38</span>%<br><span class="hljs-attribute">Valid</span>:     Epoch[<span class="hljs-number">019</span>/<span class="hljs-number">025</span>] Iteration[<span class="hljs-number">010</span>/<span class="hljs-number">010</span>] Loss: <span class="hljs-number">0</span>.<span class="hljs-number">5703</span> Acc:<span class="hljs-number">71</span>.<span class="hljs-number">90</span>%<br><span class="hljs-attribute">Training</span>:Epoch[<span class="hljs-number">020</span>/<span class="hljs-number">025</span>] Iteration[<span class="hljs-number">010</span>/<span class="hljs-number">016</span>] Loss: <span class="hljs-number">0</span>.<span class="hljs-number">5582</span> Acc:<span class="hljs-number">69</span>.<span class="hljs-number">38</span>%<br><span class="hljs-attribute">Valid</span>:     Epoch[<span class="hljs-number">020</span>/<span class="hljs-number">025</span>] Iteration[<span class="hljs-number">010</span>/<span class="hljs-number">010</span>] Loss: <span class="hljs-number">0</span>.<span class="hljs-number">5759</span> Acc:<span class="hljs-number">71</span>.<span class="hljs-number">90</span>%<br><span class="hljs-attribute">Training</span>:Epoch[<span class="hljs-number">021</span>/<span class="hljs-number">025</span>] Iteration[<span class="hljs-number">010</span>/<span class="hljs-number">016</span>] Loss: <span class="hljs-number">0</span>.<span class="hljs-number">5219</span> Acc:<span class="hljs-number">73</span>.<span class="hljs-number">75</span>%<br><span class="hljs-attribute">Valid</span>:     Epoch[<span class="hljs-number">021</span>/<span class="hljs-number">025</span>] Iteration[<span class="hljs-number">010</span>/<span class="hljs-number">010</span>] Loss: <span class="hljs-number">0</span>.<span class="hljs-number">5689</span> Acc:<span class="hljs-number">72</span>.<span class="hljs-number">55</span>%<br><span class="hljs-attribute">Training</span>:Epoch[<span class="hljs-number">022</span>/<span class="hljs-number">025</span>] Iteration[<span class="hljs-number">010</span>/<span class="hljs-number">016</span>] Loss: <span class="hljs-number">0</span>.<span class="hljs-number">5670</span> Acc:<span class="hljs-number">70</span>.<span class="hljs-number">62</span>%<br><span class="hljs-attribute">Valid</span>:     Epoch[<span class="hljs-number">022</span>/<span class="hljs-number">025</span>] Iteration[<span class="hljs-number">010</span>/<span class="hljs-number">010</span>] Loss: <span class="hljs-number">0</span>.<span class="hljs-number">6052</span> Acc:<span class="hljs-number">69</span>.<span class="hljs-number">28</span>%<br><span class="hljs-attribute">Training</span>:Epoch[<span class="hljs-number">023</span>/<span class="hljs-number">025</span>] Iteration[<span class="hljs-number">010</span>/<span class="hljs-number">016</span>] Loss: <span class="hljs-number">0</span>.<span class="hljs-number">5725</span> Acc:<span class="hljs-number">65</span>.<span class="hljs-number">62</span>%<br><span class="hljs-attribute">Valid</span>:     Epoch[<span class="hljs-number">023</span>/<span class="hljs-number">025</span>] Iteration[<span class="hljs-number">010</span>/<span class="hljs-number">010</span>] Loss: <span class="hljs-number">0</span>.<span class="hljs-number">6047</span> Acc:<span class="hljs-number">68</span>.<span class="hljs-number">63</span>%<br><span class="hljs-attribute">Training</span>:Epoch[<span class="hljs-number">024</span>/<span class="hljs-number">025</span>] Iteration[<span class="hljs-number">010</span>/<span class="hljs-number">016</span>] Loss: <span class="hljs-number">0</span>.<span class="hljs-number">5761</span> Acc:<span class="hljs-number">66</span>.<span class="hljs-number">25</span>%<br><span class="hljs-attribute">Valid</span>:     Epoch[<span class="hljs-number">024</span>/<span class="hljs-number">025</span>] Iteration[<span class="hljs-number">010</span>/<span class="hljs-number">010</span>] Loss: <span class="hljs-number">0</span>.<span class="hljs-number">5923</span> Acc:<span class="hljs-number">70</span>.<span class="hljs-number">59</span>%<br></code></pre></td></tr></table></figure><p>训练了 25 个 epoch 后的准确率为：70.59%。</p><p>训练的 loss 曲线如下：</p><p><img src="/img/pytorchtrain7/1.png"></p><h3 id="使用-Finetune"><a href="#使用-Finetune" class="headerlink" title="使用 Finetune"></a>使用 Finetune</h3><p>然后我们把下载的模型参数加载到模型中：</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs stylus">path_pretrained_model = enviroments<span class="hljs-selector-class">.resnet18_path</span><br>state_dict_load = torch<span class="hljs-selector-class">.load</span>(path_pretrained_model)<br>resnet18_ft<span class="hljs-selector-class">.load_state_dict</span>(state_dict_load)<br></code></pre></td></tr></table></figure><h4 id="不冻结卷积层"><a href="#不冻结卷积层" class="headerlink" title="不冻结卷积层"></a>不冻结卷积层</h4><p>这时我们不冻结卷积层，所有层都是用相同的学习率，输出如下：</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">use</span> device :cpu<br><span class="hljs-attribute">Training</span>:Epoch[<span class="hljs-number">000</span>/<span class="hljs-number">025</span>] Iteration[<span class="hljs-number">010</span>/<span class="hljs-number">016</span>] Loss: <span class="hljs-number">0</span>.<span class="hljs-number">6299</span> Acc:<span class="hljs-number">65</span>.<span class="hljs-number">62</span>%<br><span class="hljs-attribute">Valid</span>:     Epoch[<span class="hljs-number">000</span>/<span class="hljs-number">025</span>] Iteration[<span class="hljs-number">010</span>/<span class="hljs-number">010</span>] Loss: <span class="hljs-number">0</span>.<span class="hljs-number">3387</span> Acc:<span class="hljs-number">90</span>.<span class="hljs-number">20</span>%<br><span class="hljs-attribute">Training</span>:Epoch[<span class="hljs-number">001</span>/<span class="hljs-number">025</span>] Iteration[<span class="hljs-number">010</span>/<span class="hljs-number">016</span>] Loss: <span class="hljs-number">0</span>.<span class="hljs-number">3122</span> Acc:<span class="hljs-number">90</span>.<span class="hljs-number">00</span>%<br><span class="hljs-attribute">Valid</span>:     Epoch[<span class="hljs-number">001</span>/<span class="hljs-number">025</span>] Iteration[<span class="hljs-number">010</span>/<span class="hljs-number">010</span>] Loss: <span class="hljs-number">0</span>.<span class="hljs-number">2150</span> Acc:<span class="hljs-number">94</span>.<span class="hljs-number">12</span>%<br><span class="hljs-attribute">Training</span>:Epoch[<span class="hljs-number">002</span>/<span class="hljs-number">025</span>] Iteration[<span class="hljs-number">010</span>/<span class="hljs-number">016</span>] Loss: <span class="hljs-number">0</span>.<span class="hljs-number">2748</span> Acc:<span class="hljs-number">85</span>.<span class="hljs-number">62</span>%<br><span class="hljs-attribute">Valid</span>:     Epoch[<span class="hljs-number">002</span>/<span class="hljs-number">025</span>] Iteration[<span class="hljs-number">010</span>/<span class="hljs-number">010</span>] Loss: <span class="hljs-number">0</span>.<span class="hljs-number">2423</span> Acc:<span class="hljs-number">91</span>.<span class="hljs-number">50</span>%<br><span class="hljs-attribute">Training</span>:Epoch[<span class="hljs-number">003</span>/<span class="hljs-number">025</span>] Iteration[<span class="hljs-number">010</span>/<span class="hljs-number">016</span>] Loss: <span class="hljs-number">0</span>.<span class="hljs-number">1440</span> Acc:<span class="hljs-number">94</span>.<span class="hljs-number">38</span>%<br><span class="hljs-attribute">Valid</span>:     Epoch[<span class="hljs-number">003</span>/<span class="hljs-number">025</span>] Iteration[<span class="hljs-number">010</span>/<span class="hljs-number">010</span>] Loss: <span class="hljs-number">0</span>.<span class="hljs-number">1666</span> Acc:<span class="hljs-number">95</span>.<span class="hljs-number">42</span>%<br><span class="hljs-attribute">Training</span>:Epoch[<span class="hljs-number">004</span>/<span class="hljs-number">025</span>] Iteration[<span class="hljs-number">010</span>/<span class="hljs-number">016</span>] Loss: <span class="hljs-number">0</span>.<span class="hljs-number">1983</span> Acc:<span class="hljs-number">92</span>.<span class="hljs-number">50</span>%<br><span class="hljs-attribute">Valid</span>:     Epoch[<span class="hljs-number">004</span>/<span class="hljs-number">025</span>] Iteration[<span class="hljs-number">010</span>/<span class="hljs-number">010</span>] Loss: <span class="hljs-number">0</span>.<span class="hljs-number">1809</span> Acc:<span class="hljs-number">94</span>.<span class="hljs-number">77</span>%<br><span class="hljs-attribute">Training</span>:Epoch[<span class="hljs-number">005</span>/<span class="hljs-number">025</span>] Iteration[<span class="hljs-number">010</span>/<span class="hljs-number">016</span>] Loss: <span class="hljs-number">0</span>.<span class="hljs-number">1840</span> Acc:<span class="hljs-number">92</span>.<span class="hljs-number">50</span>%<br><span class="hljs-attribute">Valid</span>:     Epoch[<span class="hljs-number">005</span>/<span class="hljs-number">025</span>] Iteration[<span class="hljs-number">010</span>/<span class="hljs-number">010</span>] Loss: <span class="hljs-number">0</span>.<span class="hljs-number">2437</span> Acc:<span class="hljs-number">91</span>.<span class="hljs-number">50</span>%<br><span class="hljs-attribute">Training</span>:Epoch[<span class="hljs-number">006</span>/<span class="hljs-number">025</span>] Iteration[<span class="hljs-number">010</span>/<span class="hljs-number">016</span>] Loss: <span class="hljs-number">0</span>.<span class="hljs-number">1921</span> Acc:<span class="hljs-number">93</span>.<span class="hljs-number">12</span>%<br><span class="hljs-attribute">Valid</span>:     Epoch[<span class="hljs-number">006</span>/<span class="hljs-number">025</span>] Iteration[<span class="hljs-number">010</span>/<span class="hljs-number">010</span>] Loss: <span class="hljs-number">0</span>.<span class="hljs-number">2014</span> Acc:<span class="hljs-number">95</span>.<span class="hljs-number">42</span>%<br><span class="hljs-attribute">Training</span>:Epoch[<span class="hljs-number">007</span>/<span class="hljs-number">025</span>] Iteration[<span class="hljs-number">010</span>/<span class="hljs-number">016</span>] Loss: <span class="hljs-number">0</span>.<span class="hljs-number">1311</span> Acc:<span class="hljs-number">93</span>.<span class="hljs-number">12</span>%<br><span class="hljs-attribute">Valid</span>:     Epoch[<span class="hljs-number">007</span>/<span class="hljs-number">025</span>] Iteration[<span class="hljs-number">010</span>/<span class="hljs-number">010</span>] Loss: <span class="hljs-number">0</span>.<span class="hljs-number">1890</span> Acc:<span class="hljs-number">96</span>.<span class="hljs-number">08</span>%<br><span class="hljs-attribute">Training</span>:Epoch[<span class="hljs-number">008</span>/<span class="hljs-number">025</span>] Iteration[<span class="hljs-number">010</span>/<span class="hljs-number">016</span>] Loss: <span class="hljs-number">0</span>.<span class="hljs-number">1395</span> Acc:<span class="hljs-number">94</span>.<span class="hljs-number">38</span>%<br><span class="hljs-attribute">Valid</span>:     Epoch[<span class="hljs-number">008</span>/<span class="hljs-number">025</span>] Iteration[<span class="hljs-number">010</span>/<span class="hljs-number">010</span>] Loss: <span class="hljs-number">0</span>.<span class="hljs-number">1907</span> Acc:<span class="hljs-number">95</span>.<span class="hljs-number">42</span>%<br><span class="hljs-attribute">Training</span>:Epoch[<span class="hljs-number">009</span>/<span class="hljs-number">025</span>] Iteration[<span class="hljs-number">010</span>/<span class="hljs-number">016</span>] Loss: <span class="hljs-number">0</span>.<span class="hljs-number">1390</span> Acc:<span class="hljs-number">93</span>.<span class="hljs-number">75</span>%<br><span class="hljs-attribute">Valid</span>:     Epoch[<span class="hljs-number">009</span>/<span class="hljs-number">025</span>] Iteration[<span class="hljs-number">010</span>/<span class="hljs-number">010</span>] Loss: <span class="hljs-number">0</span>.<span class="hljs-number">1933</span> Acc:<span class="hljs-number">95</span>.<span class="hljs-number">42</span>%<br><span class="hljs-attribute">Training</span>:Epoch[<span class="hljs-number">010</span>/<span class="hljs-number">025</span>] Iteration[<span class="hljs-number">010</span>/<span class="hljs-number">016</span>] Loss: <span class="hljs-number">0</span>.<span class="hljs-number">1065</span> Acc:<span class="hljs-number">96</span>.<span class="hljs-number">88</span>%<br><span class="hljs-attribute">Valid</span>:     Epoch[<span class="hljs-number">010</span>/<span class="hljs-number">025</span>] Iteration[<span class="hljs-number">010</span>/<span class="hljs-number">010</span>] Loss: <span class="hljs-number">0</span>.<span class="hljs-number">1865</span> Acc:<span class="hljs-number">95</span>.<span class="hljs-number">42</span>%<br><span class="hljs-attribute">Training</span>:Epoch[<span class="hljs-number">011</span>/<span class="hljs-number">025</span>] Iteration[<span class="hljs-number">010</span>/<span class="hljs-number">016</span>] Loss: <span class="hljs-number">0</span>.<span class="hljs-number">0845</span> Acc:<span class="hljs-number">98</span>.<span class="hljs-number">12</span>%<br><span class="hljs-attribute">Valid</span>:     Epoch[<span class="hljs-number">011</span>/<span class="hljs-number">025</span>] Iteration[<span class="hljs-number">010</span>/<span class="hljs-number">010</span>] Loss: <span class="hljs-number">0</span>.<span class="hljs-number">1851</span> Acc:<span class="hljs-number">96</span>.<span class="hljs-number">08</span>%<br><span class="hljs-attribute">Training</span>:Epoch[<span class="hljs-number">012</span>/<span class="hljs-number">025</span>] Iteration[<span class="hljs-number">010</span>/<span class="hljs-number">016</span>] Loss: <span class="hljs-number">0</span>.<span class="hljs-number">1068</span> Acc:<span class="hljs-number">95</span>.<span class="hljs-number">62</span>%<br><span class="hljs-attribute">Valid</span>:     Epoch[<span class="hljs-number">012</span>/<span class="hljs-number">025</span>] Iteration[<span class="hljs-number">010</span>/<span class="hljs-number">010</span>] Loss: <span class="hljs-number">0</span>.<span class="hljs-number">1862</span> Acc:<span class="hljs-number">95</span>.<span class="hljs-number">42</span>%<br><span class="hljs-attribute">Training</span>:Epoch[<span class="hljs-number">013</span>/<span class="hljs-number">025</span>] Iteration[<span class="hljs-number">010</span>/<span class="hljs-number">016</span>] Loss: <span class="hljs-number">0</span>.<span class="hljs-number">0986</span> Acc:<span class="hljs-number">96</span>.<span class="hljs-number">25</span>%<br><span class="hljs-attribute">Valid</span>:     Epoch[<span class="hljs-number">013</span>/<span class="hljs-number">025</span>] Iteration[<span class="hljs-number">010</span>/<span class="hljs-number">010</span>] Loss: <span class="hljs-number">0</span>.<span class="hljs-number">1803</span> Acc:<span class="hljs-number">96</span>.<span class="hljs-number">73</span>%<br><span class="hljs-attribute">Training</span>:Epoch[<span class="hljs-number">014</span>/<span class="hljs-number">025</span>] Iteration[<span class="hljs-number">010</span>/<span class="hljs-number">016</span>] Loss: <span class="hljs-number">0</span>.<span class="hljs-number">1083</span> Acc:<span class="hljs-number">96</span>.<span class="hljs-number">88</span>%<br><span class="hljs-attribute">Valid</span>:     Epoch[<span class="hljs-number">014</span>/<span class="hljs-number">025</span>] Iteration[<span class="hljs-number">010</span>/<span class="hljs-number">010</span>] Loss: <span class="hljs-number">0</span>.<span class="hljs-number">1867</span> Acc:<span class="hljs-number">96</span>.<span class="hljs-number">08</span>%<br><span class="hljs-attribute">Training</span>:Epoch[<span class="hljs-number">015</span>/<span class="hljs-number">025</span>] Iteration[<span class="hljs-number">010</span>/<span class="hljs-number">016</span>] Loss: <span class="hljs-number">0</span>.<span class="hljs-number">0683</span> Acc:<span class="hljs-number">98</span>.<span class="hljs-number">12</span>%<br><span class="hljs-attribute">Valid</span>:     Epoch[<span class="hljs-number">015</span>/<span class="hljs-number">025</span>] Iteration[<span class="hljs-number">010</span>/<span class="hljs-number">010</span>] Loss: <span class="hljs-number">0</span>.<span class="hljs-number">1863</span> Acc:<span class="hljs-number">95</span>.<span class="hljs-number">42</span>%<br><span class="hljs-attribute">Training</span>:Epoch[<span class="hljs-number">016</span>/<span class="hljs-number">025</span>] Iteration[<span class="hljs-number">010</span>/<span class="hljs-number">016</span>] Loss: <span class="hljs-number">0</span>.<span class="hljs-number">1271</span> Acc:<span class="hljs-number">96</span>.<span class="hljs-number">25</span>%<br><span class="hljs-attribute">Valid</span>:     Epoch[<span class="hljs-number">016</span>/<span class="hljs-number">025</span>] Iteration[<span class="hljs-number">010</span>/<span class="hljs-number">010</span>] Loss: <span class="hljs-number">0</span>.<span class="hljs-number">1842</span> Acc:<span class="hljs-number">94</span>.<span class="hljs-number">77</span>%<br><span class="hljs-attribute">Training</span>:Epoch[<span class="hljs-number">017</span>/<span class="hljs-number">025</span>] Iteration[<span class="hljs-number">010</span>/<span class="hljs-number">016</span>] Loss: <span class="hljs-number">0</span>.<span class="hljs-number">0857</span> Acc:<span class="hljs-number">97</span>.<span class="hljs-number">50</span>%<br><span class="hljs-attribute">Valid</span>:     Epoch[<span class="hljs-number">017</span>/<span class="hljs-number">025</span>] Iteration[<span class="hljs-number">010</span>/<span class="hljs-number">010</span>] Loss: <span class="hljs-number">0</span>.<span class="hljs-number">1776</span> Acc:<span class="hljs-number">96</span>.<span class="hljs-number">08</span>%<br><span class="hljs-attribute">Training</span>:Epoch[<span class="hljs-number">018</span>/<span class="hljs-number">025</span>] Iteration[<span class="hljs-number">010</span>/<span class="hljs-number">016</span>] Loss: <span class="hljs-number">0</span>.<span class="hljs-number">1338</span> Acc:<span class="hljs-number">94</span>.<span class="hljs-number">38</span>%<br><span class="hljs-attribute">Valid</span>:     Epoch[<span class="hljs-number">018</span>/<span class="hljs-number">025</span>] Iteration[<span class="hljs-number">010</span>/<span class="hljs-number">010</span>] Loss: <span class="hljs-number">0</span>.<span class="hljs-number">1736</span> Acc:<span class="hljs-number">96</span>.<span class="hljs-number">08</span>%<br><span class="hljs-attribute">Training</span>:Epoch[<span class="hljs-number">019</span>/<span class="hljs-number">025</span>] Iteration[<span class="hljs-number">010</span>/<span class="hljs-number">016</span>] Loss: <span class="hljs-number">0</span>.<span class="hljs-number">1381</span> Acc:<span class="hljs-number">95</span>.<span class="hljs-number">62</span>%<br><span class="hljs-attribute">Valid</span>:     Epoch[<span class="hljs-number">019</span>/<span class="hljs-number">025</span>] Iteration[<span class="hljs-number">010</span>/<span class="hljs-number">010</span>] Loss: <span class="hljs-number">0</span>.<span class="hljs-number">1852</span> Acc:<span class="hljs-number">93</span>.<span class="hljs-number">46</span>%<br><span class="hljs-attribute">Training</span>:Epoch[<span class="hljs-number">020</span>/<span class="hljs-number">025</span>] Iteration[<span class="hljs-number">010</span>/<span class="hljs-number">016</span>] Loss: <span class="hljs-number">0</span>.<span class="hljs-number">0936</span> Acc:<span class="hljs-number">96</span>.<span class="hljs-number">25</span>%<br><span class="hljs-attribute">Valid</span>:     Epoch[<span class="hljs-number">020</span>/<span class="hljs-number">025</span>] Iteration[<span class="hljs-number">010</span>/<span class="hljs-number">010</span>] Loss: <span class="hljs-number">0</span>.<span class="hljs-number">1820</span> Acc:<span class="hljs-number">95</span>.<span class="hljs-number">42</span>%<br><span class="hljs-attribute">Training</span>:Epoch[<span class="hljs-number">021</span>/<span class="hljs-number">025</span>] Iteration[<span class="hljs-number">010</span>/<span class="hljs-number">016</span>] Loss: <span class="hljs-number">0</span>.<span class="hljs-number">1818</span> Acc:<span class="hljs-number">93</span>.<span class="hljs-number">75</span>%<br><span class="hljs-attribute">Valid</span>:     Epoch[<span class="hljs-number">021</span>/<span class="hljs-number">025</span>] Iteration[<span class="hljs-number">010</span>/<span class="hljs-number">010</span>] Loss: <span class="hljs-number">0</span>.<span class="hljs-number">1949</span> Acc:<span class="hljs-number">92</span>.<span class="hljs-number">81</span>%<br><span class="hljs-attribute">Training</span>:Epoch[<span class="hljs-number">022</span>/<span class="hljs-number">025</span>] Iteration[<span class="hljs-number">010</span>/<span class="hljs-number">016</span>] Loss: <span class="hljs-number">0</span>.<span class="hljs-number">1525</span> Acc:<span class="hljs-number">93</span>.<span class="hljs-number">75</span>%<br><span class="hljs-attribute">Valid</span>:     Epoch[<span class="hljs-number">022</span>/<span class="hljs-number">025</span>] Iteration[<span class="hljs-number">010</span>/<span class="hljs-number">010</span>] Loss: <span class="hljs-number">0</span>.<span class="hljs-number">1816</span> Acc:<span class="hljs-number">95</span>.<span class="hljs-number">42</span>%<br><span class="hljs-attribute">Training</span>:Epoch[<span class="hljs-number">023</span>/<span class="hljs-number">025</span>] Iteration[<span class="hljs-number">010</span>/<span class="hljs-number">016</span>] Loss: <span class="hljs-number">0</span>.<span class="hljs-number">1942</span> Acc:<span class="hljs-number">93</span>.<span class="hljs-number">12</span>%<br><span class="hljs-attribute">Valid</span>:     Epoch[<span class="hljs-number">023</span>/<span class="hljs-number">025</span>] Iteration[<span class="hljs-number">010</span>/<span class="hljs-number">010</span>] Loss: <span class="hljs-number">0</span>.<span class="hljs-number">1744</span> Acc:<span class="hljs-number">96</span>.<span class="hljs-number">08</span>%<br><span class="hljs-attribute">Training</span>:Epoch[<span class="hljs-number">024</span>/<span class="hljs-number">025</span>] Iteration[<span class="hljs-number">010</span>/<span class="hljs-number">016</span>] Loss: <span class="hljs-number">0</span>.<span class="hljs-number">1268</span> Acc:<span class="hljs-number">96</span>.<span class="hljs-number">25</span>%<br><span class="hljs-attribute">Valid</span>:     Epoch[<span class="hljs-number">024</span>/<span class="hljs-number">025</span>] Iteration[<span class="hljs-number">010</span>/<span class="hljs-number">010</span>] Loss: <span class="hljs-number">0</span>.<span class="hljs-number">1808</span> Acc:<span class="hljs-number">96</span>.<span class="hljs-number">08</span>%<br></code></pre></td></tr></table></figure><p>训练了 25 个 epoch 后的准确率为：96.08%。</p><p>训练的 loss 曲线如下：</p><p><img src="/img/pytorchtrain7/2.png"></p><h4 id="冻结卷积层"><a href="#冻结卷积层" class="headerlink" title="冻结卷积层"></a>冻结卷积层</h4><h5 id="设置requires-grad-False"><a href="#设置requires-grad-False" class="headerlink" title="设置requires_grad=False"></a>设置<code>requires_grad=False</code></h5><p>这里先冻结所有参数，然后再替换全连接层，相当于冻结了卷积层的参数：</p><figure class="highlight livecodeserver"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs livecodeserver"><span class="hljs-keyword">for</span> <span class="hljs-built_in">param</span> <span class="hljs-keyword">in</span> resnet18_ft.parameters():<br>    <span class="hljs-built_in">param</span>.requires_grad = False<br>    <span class="hljs-comment"># 首先拿到 fc 层的输入个数</span><br>num_ftrs = resnet18_ft.fc.in_features<br><span class="hljs-comment"># 然后构造新的 fc 层替换原来的 fc 层</span><br>resnet18_ft.fc = nn.Linear(num_ftrs, classes)<br></code></pre></td></tr></table></figure><p>这里不提供实验结果。</p><h5 id="设置学习率为-0"><a href="#设置学习率为-0" class="headerlink" title="设置学习率为 0"></a>设置学习率为 0</h5><p>这里把卷积层的学习率设置为 0，需要在优化器里设置不同的学习率。首先获取全连接层参数的地址，然后使用 filter 过滤不属于全连接层的参数，也就是保留卷积层的参数；接着设置优化器的分组学习率，传入一个 list，包含 2 个元素，每个元素是字典，对应 2 个参数组。其中卷积层的学习率设置为 全连接层的 0.1 倍。</p><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs ini"><span class="hljs-comment"># 首先获取全连接层参数的地址</span><br><span class="hljs-attr">fc_params_id</span> = list(map(id, resnet18_ft.fc.parameters()))     <span class="hljs-comment"># 返回的是parameters的 内存地址</span><br><span class="hljs-comment"># 然后使用 filter 过滤不属于全连接层的参数，也就是保留卷积层的参数</span><br><span class="hljs-attr">base_params</span> = filter(lambda p: id(p) not in fc_params_id, resnet18_ft.parameters())<br><span class="hljs-comment"># 设置优化器的分组学习率，传入一个 list，包含 2 个元素，每个元素是字典，对应 2 个参数组</span><br><span class="hljs-attr">optimizer</span> = optim.SGD([&#123;<span class="hljs-string">&#x27;params&#x27;</span>: base_params, <span class="hljs-string">&#x27;lr&#x27;</span>: <span class="hljs-number">0</span>&#125;, &#123;<span class="hljs-string">&#x27;params&#x27;</span>: resnet18_ft.fc.parameters(), <span class="hljs-string">&#x27;lr&#x27;</span>: LR&#125;], momentum=<span class="hljs-number">0.9</span>)<br></code></pre></td></tr></table></figure><p>这里不提供实验结果。</p><h4 id="使用分组学习率"><a href="#使用分组学习率" class="headerlink" title="使用分组学习率"></a>使用分组学习率</h4><p>这里不冻结卷积层，而是对卷积层使用较小的学习率，对全连接层使用较大的学习率，需要在优化器里设置不同的学习率。首先获取全连接层参数的地址，然后使用 filter 过滤不属于全连接层的参数，也就是保留卷积层的参数；接着设置优化器的分组学习率，传入一个 list，包含 2 个元素，每个元素是字典，对应 2 个参数组。其中卷积层的学习率设置为 全连接层的 0.1 倍。</p><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs ini"><span class="hljs-comment"># 首先获取全连接层参数的地址</span><br><span class="hljs-attr">fc_params_id</span> = list(map(id, resnet18_ft.fc.parameters()))     <span class="hljs-comment"># 返回的是parameters的 内存地址</span><br><span class="hljs-comment"># 然后使用 filter 过滤不属于全连接层的参数，也就是保留卷积层的参数</span><br><span class="hljs-attr">base_params</span> = filter(lambda p: id(p) not in fc_params_id, resnet18_ft.parameters())<br><span class="hljs-comment"># 设置优化器的分组学习率，传入一个 list，包含 2 个元素，每个元素是字典，对应 2 个参数组</span><br><span class="hljs-attr">optimizer</span> = optim.SGD([&#123;<span class="hljs-string">&#x27;params&#x27;</span>: base_params, <span class="hljs-string">&#x27;lr&#x27;</span>: LR*<span class="hljs-number">0</span>&#125;, &#123;<span class="hljs-string">&#x27;params&#x27;</span>: resnet18_ft.fc.parameters(), <span class="hljs-string">&#x27;lr&#x27;</span>: LR&#125;], momentum=<span class="hljs-number">0.9</span>)<br></code></pre></td></tr></table></figure><p>这里不提供实验结果。</p><h2 id="使用-GPU-训练模型"><a href="#使用-GPU-训练模型" class="headerlink" title="使用 GPU 训练模型"></a>使用 GPU 训练模型</h2><p>PyTorch 模型使用 GPU，可以分为 3 步：</p><ol><li>首先获取 device：<code>device = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;)</code></li><li>把模型加载到 device：<code>model.to(device)</code></li><li>在 data_loader 取数据的循环中，把每个 mini-batch 的数据和 label 加载到 device：<code>inputs, labels = inputs.to(device), labels.to(device)</code></li></ol><p>在数据运算时，两个数据进行运算，那么它们必须同时存放在同一个设备，要么同时是 CPU，要么同时是 GPU。而且数据和模型都要在同一个设备上。数据和模型可以使用<code>to()</code>方法从一个设备转移到另一个设备。而数据的<code>to()</code>方法还可以转换数据类型。</p><ul><li><p>从 CPU 到 GPU</p><figure class="highlight abnf"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs abnf"><span class="hljs-attribute">device</span> <span class="hljs-operator">=</span> torch.device(<span class="hljs-string">&quot;cuda&quot;</span>)<br><span class="hljs-attribute">tensor</span> <span class="hljs-operator">=</span> tensor.to(device)<br>module.to(device)<br></code></pre></td></tr></table></figure></li><li><p>从 GPU 到 CPU</p><figure class="highlight abnf"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs abnf"><span class="hljs-attribute">device</span> <span class="hljs-operator">=</span> torch.device(cpu)<br><span class="hljs-attribute">tensor</span> <span class="hljs-operator">=</span> tensor.to(<span class="hljs-string">&quot;cpu&quot;</span>)<br>module.to(<span class="hljs-string">&quot;cpu&quot;</span>)<br></code></pre></td></tr></table></figure><p><code>tensor</code>和<code>module</code>的 <code>to()</code>方法的区别是：<code>tensor.to()</code>执行的不是 inplace 操作，因此需要赋值；<code>module.to()</code>执行的是 inplace 操作。</p></li></ul><p>下面的代码是转换数据类型</p><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs ini"><span class="hljs-attr">x</span> = torch.<span class="hljs-literal">on</span>es((<span class="hljs-number">3</span>,<span class="hljs-number">3</span>))<br><span class="hljs-attr">x</span> = x.to(torch.float64)<br></code></pre></td></tr></table></figure><h3 id="tensor-to-和-module-to"><a href="#tensor-to-和-module-to" class="headerlink" title="tensor.to() 和 module.to()"></a><code>tensor.to()</code> 和 <code>module.to()</code></h3><p>首先导入库，获取 GPU 的 device</p><figure class="highlight elm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs elm"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<br><span class="hljs-title">device</span> = torch.device(<span class="hljs-string">&quot;cuda:0&quot;</span> <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> <span class="hljs-string">&quot;cpu&quot;</span>)<br></code></pre></td></tr></table></figure><p>下面的代码是执行<code>Tensor</code>的<code>to()</code>方法</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs stylus">x_cpu = torch<span class="hljs-selector-class">.ones</span>((<span class="hljs-number">3</span>, <span class="hljs-number">3</span>))<br><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(<span class="hljs-string">&quot;x_cpu:\ndevice: &#123;&#125; is_cuda: &#123;&#125; id: &#123;&#125;&quot;</span>.format(x_cpu.device, x_cpu.is_cuda, id(x_cpu)</span></span>))<br><br>x_gpu = x_cpu<span class="hljs-selector-class">.to</span>(device)<br><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(<span class="hljs-string">&quot;x_gpu:\ndevice: &#123;&#125; is_cuda: &#123;&#125; id: &#123;&#125;&quot;</span>.format(x_gpu.device, x_gpu.is_cuda, id(x_gpu)</span></span>))<br></code></pre></td></tr></table></figure><p>输出如下：</p><figure class="highlight nix"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs nix"><span class="hljs-params">x_cpu:</span><br><span class="hljs-params">device:</span> cpu <span class="hljs-params">is_cuda:</span> False <span class="hljs-params">id:</span> <span class="hljs-number">1415020820304</span><br><span class="hljs-params">x_gpu:</span><br><span class="hljs-params">device:</span> cpu <span class="hljs-params">is_cuda:</span> True <span class="hljs-params">id:</span> <span class="hljs-number">2700061800153</span><br></code></pre></td></tr></table></figure><p>可以看到<code>Tensor</code>的<code>to()</code>方法不是 inplace 操作，<code>x_cpu</code>和<code>x_gpu</code>的内存地址不一样。</p><p>下面代码执行的是<code>Module</code>的<code>to()</code>方法</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs stylus">net = nn<span class="hljs-selector-class">.Sequential</span>(nn<span class="hljs-selector-class">.Linear</span>(<span class="hljs-number">3</span>, <span class="hljs-number">3</span>))<br><br><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(<span class="hljs-string">&quot;\nid:&#123;&#125; is_cuda: &#123;&#125;&quot;</span>.format(id(net)</span></span>, <span class="hljs-built_in">next</span>(net<span class="hljs-selector-class">.parameters</span>()).is_cuda))<br><br>net<span class="hljs-selector-class">.to</span>(device)<br><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(<span class="hljs-string">&quot;\nid:&#123;&#125; is_cuda: &#123;&#125;&quot;</span>.format(id(net)</span></span>, <span class="hljs-built_in">next</span>(net<span class="hljs-selector-class">.parameters</span>()).is_cuda))<br></code></pre></td></tr></table></figure><p>输出如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-built_in">id</span>:<span class="hljs-number">2325748158192</span> is_cuda: <span class="hljs-literal">False</span><br><span class="hljs-built_in">id</span>:<span class="hljs-number">2325748158192</span> is_cuda: <span class="hljs-literal">True</span><br></code></pre></td></tr></table></figure><p>可以看到<code>Module</code>的<code>to()</code>方法是 inplace 操作，内存地址一样。</p><h3 id="torch-cuda常用方法"><a href="#torch-cuda常用方法" class="headerlink" title="torch.cuda常用方法"></a><code>torch.cuda</code>常用方法</h3><ul><li>torch.cuda.device_count()：返回当前可见可用的 GPU 数量</li><li>torch.cuda.get_device_name()：获取 GPU 名称</li><li>torch.cuda.manual_seed()：为当前 GPU 设置随机种子</li><li>torch.cuda.manual_seed_all()：为所有可见 GPU 设置随机种子</li><li>torch.cuda.set_device()：设置主 GPU 为哪一个物理 GPU，此方法不推荐使用</li><li>os.environ.setdefault(“CUDA_VISIBLE_DEVICES”, “2”, “3”)：设置可见 GPU</li></ul><p>在 PyTorch 中，有物理 GPU 可以逻辑 GPU 之分，可以设置它们之间的对应关系。</p><p><img src="/img/pytorchtrain7/3.png"><br> 在上图中，如果执行了<code>os.environ.setdefault(&quot;CUDA_VISIBLE_DEVICES&quot;, &quot;2&quot;, &quot;3&quot;)</code>，那么可见 GPU 数量只有 2 个。对应关系如下：</p><table><thead><tr><th align="center">逻辑 GPU</th><th align="center">物理 GPU</th></tr></thead><tbody><tr><td align="center">gpu0</td><td align="center">gpu2</td></tr><tr><td align="center">gpu1</td><td align="center">gpu3</td></tr></tbody></table><p>如果执行了<code>os.environ.setdefault(&quot;CUDA_VISIBLE_DEVICES&quot;, &quot;0&quot;, &quot;3&quot;, &quot;2&quot;)</code>，那么可见 GPU 数量只有 3 个。对应关系如下：</p><table><thead><tr><th align="center">逻辑 GPU</th><th align="center">物理 GPU</th></tr></thead><tbody><tr><td align="center">gpu0</td><td align="center">gpu0</td></tr><tr><td align="center">gpu1</td><td align="center">gpu3</td></tr><tr><td align="center">gpu2</td><td align="center">gpu2</td></tr></tbody></table><p>设置的原因是可能系统中有很多用户和任务在使用 GPU，设置 GPU 编号，可以合理分配 GPU。通常默认<code>gpu0</code>为主 GPU。主 GPU 的概念与多 GPU 的分发并行机制有关。</p><h3 id="多-GPU-的分发并行"><a href="#多-GPU-的分发并行" class="headerlink" title="多 GPU 的分发并行"></a>多 GPU 的分发并行</h3><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs routeros">torch.nn.DataParallel(module, <span class="hljs-attribute">device_ids</span>=None, <span class="hljs-attribute">output_device</span>=None, <span class="hljs-attribute">dim</span>=0)<br></code></pre></td></tr></table></figure><p>功能：包装模型，实现分发并行机制。可以把数据平均分发到各个 GPU 上，每个 GPU 实际的数据量为 $\frac{batch_size}{GPU 数量}$，实现并行计算。</p><p>主要参数：</p><ul><li>module：需要包装分发的模型</li><li>device_ids：可分发的 GPU，默认分发到所有可见可用的 GPU</li><li>output_device：结果输出设备</li></ul><p>需要注意的是：使用 <code>DataParallel</code> 时，<code>device</code> 要指定某个 GPU 为 主 GPU，否则会报错：</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">RuntimeError</span>: module must have its parameters and buffers <span class="hljs-literal">on</span> device cuda:<span class="hljs-number">1</span> (device_ids[<span class="hljs-number">0</span>]) but found one of them <span class="hljs-literal">on</span> device: cuda:<span class="hljs-number">2</span><br></code></pre></td></tr></table></figure><p>这是因为，使用多 GPU 需要有一个主 GPU，来把每个 batch 的数据分发到每个 GPU，并从每个 GPU 收集计算好的结果。如果不指定主 GPU，那么数据就直接分发到每个 GPU，会造成有些数据在某个 GPU，而另一部分数据在其他 GPU，计算出错。</p><p>详情请参考 <a href="https://stackoverflow.com/questions/59249563/runtimeerror-module-must-have-its-parameters-and-buffers-on-device-cuda1-devi">[RuntimeError: module must have its parameters and buffers on device cuda:1 (device_ids[0]) but found one of them on device: cuda:2</a>](<a href="https://stackoverflow.com/questions/59249563/runtimeerror-module-must-have-its-parameters-and-buffers-on-device-cuda1-devi">RuntimeError: module must have its parameters and buffers on device cuda:1 (device_ids[0]) but found one of them on device: cuda:2</a>)</p><p>下面的代码设置两个可见 GPU，batch_size 为 2，那么每个 GPU 每个 batch 拿到的数据数量为 8，在模型的前向传播中打印数据的数量。</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><code class="hljs routeros"><span class="hljs-comment"># 设置 2 个可见 GPU</span><br>gpu_list = [0,1]<br>gpu_list_str = <span class="hljs-string">&#x27;,&#x27;</span>.join(map(str, gpu_list))<br>os.environ.setdefault(<span class="hljs-string">&quot;CUDA_VISIBLE_DEVICES&quot;</span>, gpu_list_str)<br><span class="hljs-comment"># 这里注意，需要指定一个 GPU 作为主 GPU。</span><br><span class="hljs-comment"># 否则会报错：module must have its parameters and buffers on device cuda:1 (device_ids[0]) but found one of them on device: cuda:2</span><br><span class="hljs-comment"># 参考：https://stackoverflow.com/questions/59249563/runtimeerror-module-must-have-its-parameters-and-buffers-on-device-cuda1-devi</span><br>device = torch.device(<span class="hljs-string">&quot;cuda:0&quot;</span> <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> <span class="hljs-string">&quot;cpu&quot;</span>)<br><br>batch_size = 16<br><br><span class="hljs-comment"># data</span><br>inputs = torch.randn(batch_size, 3)<br>labels = torch.randn(batch_size, 3)<br><br>inputs, labels = inputs.<span class="hljs-keyword">to</span>(device), labels.<span class="hljs-keyword">to</span>(device)<br><br><span class="hljs-comment"># model</span><br>net = FooNet(<span class="hljs-attribute">neural_num</span>=3, <span class="hljs-attribute">layers</span>=3)<br>net = nn.DataParallel(net)<br>net.<span class="hljs-keyword">to</span>(device)<br><br><span class="hljs-comment"># training</span><br><span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> range(1):<br><br>    outputs = net(inputs)<br><br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;model outputs.size: &#123;&#125;&quot;</span>.format(outputs.size()))<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;CUDA_VISIBLE_DEVICES :&#123;&#125;&quot;</span>.format(os.environ[<span class="hljs-string">&quot;CUDA_VISIBLE_DEVICES&quot;</span>]))<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;device_count :&#123;&#125;&quot;</span>.format(torch.cuda.device_count()))<br></code></pre></td></tr></table></figure><p>输出如下：</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">batch</span> size in forward: <span class="hljs-number">8</span><br><span class="hljs-attribute">model</span> outputs.size: torch.Size([<span class="hljs-number">16</span>, <span class="hljs-number">3</span>])<br><span class="hljs-attribute">CUDA_VISIBLE_DEVICES</span> :<span class="hljs-number">0</span>,<span class="hljs-number">1</span><br><span class="hljs-attribute">device_count</span> :<span class="hljs-number">2</span><br></code></pre></td></tr></table></figure><p>下面的代码是根据 GPU 剩余内存来排序。</p><figure class="highlight scss"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs scss">def <span class="hljs-built_in">get_gpu_memory</span>():<br>    import platform<br>    if <span class="hljs-string">&#x27;Windows&#x27;</span> != platform.<span class="hljs-built_in">system</span>():<br>        import os<br>        os.<span class="hljs-built_in">system</span>(<span class="hljs-string">&#x27;nvidia-smi -q -d Memory | grep -A4 GPU | grep Free &gt; tmp.txt&#x27;</span>)<br>        memory_gpu = [<span class="hljs-built_in">int</span>(x.<span class="hljs-built_in">split</span>()[<span class="hljs-number">2</span>]) for x in <span class="hljs-built_in">open</span>(<span class="hljs-string">&#x27;tmp.txt&#x27;</span>, <span class="hljs-string">&#x27;r&#x27;</span>).<span class="hljs-built_in">readlines</span>()]<br>        os.<span class="hljs-built_in">system</span>(<span class="hljs-string">&#x27;rm tmp.txt&#x27;</span>)<br>    else:<br>        memory_gpu = False<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;显存计算功能暂不支持windows操作系统&quot;</span>)<br>    return memory_gpu<br><br><br>gpu_memory = <span class="hljs-built_in">get_gpu_memory</span>()<br>if not gpu_memory:<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;\ngpu free memory: &#123;&#125;&quot;</span>.<span class="hljs-built_in">format</span>(gpu_memory))<br>    gpu_list = np.<span class="hljs-built_in">argsort</span>(gpu_memory)[::-<span class="hljs-number">1</span>]<br><br>    gpu_list_str = <span class="hljs-string">&#x27;,&#x27;</span>.<span class="hljs-built_in">join</span>(<span class="hljs-built_in">map</span>(str, gpu_list))<br>    os.environ.<span class="hljs-built_in">setdefault</span>(<span class="hljs-string">&quot;CUDA_VISIBLE_DEVICES&quot;</span>, gpu_list_str)<br>    device = torch.<span class="hljs-built_in">device</span>(<span class="hljs-string">&quot;cuda&quot;</span> if torch.cuda.<span class="hljs-built_in">is_available</span>() else <span class="hljs-string">&quot;cpu&quot;</span>)<br></code></pre></td></tr></table></figure><p>其中<code>nvidia-smi -q -d Memory</code>是查询所有 GPU 的内存信息，<code>-q</code>表示查询，<code>-d</code>是指定查询的内容。</p><p><code>nvidia-smi -q -d Memory | grep -A4 GPU</code>是截取 GPU 开始的 4 行，如下：</p><figure class="highlight asciidoc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs asciidoc">Attached GPUs                       : 2<br>GPU 00000000:1A:00.0<br><span class="hljs-code">    FB Memory Usage</span><br><span class="hljs-code">        Total                       : 24220 MiB</span><br><span class="hljs-code">        Used                        : 845 MiB</span><br><span class="hljs-section">        Free                        : 23375 MiB</span><br><span class="hljs-section">--</span><br>GPU 00000000:68:00.0<br><span class="hljs-code">    FB Memory Usage</span><br><span class="hljs-code">        Total                       : 24217 MiB</span><br><span class="hljs-code">        Used                        : 50 MiB</span><br><span class="hljs-code">        Free                        : 24167 MiB</span><br></code></pre></td></tr></table></figure><p><code>nvidia-smi -q -d Memory | grep -A4 GPU | grep Free</code>是提取<code>Free</code>所在的行，也就是提取剩余内存的信息，如下：</p><figure class="highlight ada"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs ada">Free                        : 23375 <span class="hljs-type">MiB</span><br>Free                        : 24167 <span class="hljs-type">MiB</span><br></code></pre></td></tr></table></figure><p><code>nvidia-smi -q -d Memory | grep -A4 GPU | grep Free &gt; tmp.txt</code>是把剩余内存的信息保存到<code>tmp.txt</code>中。</p><p><code>[int(x.split()[2]) for x in open(&#39;tmp.txt&#39;, &#39;r&#39;).readlines()]</code>是用列表表达式对每行进行处理。</p><p>假设<code>x=&quot; Free : 23375 MiB&quot;</code>，那么<code>x.split()</code>默认以空格分割，结果是：</p><figure class="highlight scheme"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs scheme">[<span class="hljs-symbol">&#x27;Free</span>&#x27;, <span class="hljs-symbol">&#x27;:</span>&#x27;, <span class="hljs-symbol">&#x27;23375</span>&#x27;, <span class="hljs-symbol">&#x27;MiB</span>&#x27;]<br></code></pre></td></tr></table></figure><p><code>x.split()[2]</code>的结果是<code>23375</code>。</p><p>假设<code>gpu_memory=[&#39;5&#39;,&#39;9&#39;,&#39;3&#39;]</code>，<code>np.argsort(gpu_memory)</code>的结果是<code>array([2, 0, 1], dtype=int64)</code>，是从小到大取排好序后的索引。<code>np.argsort(gpu_memory)[::-1]</code>的结果是<code>array([1, 0, 2], dtype=int64)</code>，也就是把元素的顺序反过来。</p><p>在 Python 中，<code>list[&lt;start&gt;:&lt;stop&gt;:&lt;step&gt;]</code>表示从<code>start</code>到<code>stop</code>取出元素，间隔为<code>step</code>，<code>step=-1</code>表示从<code>stop</code>到<code>start</code>取出元素。<code>start</code>默认为第一个元素的位置，<code>stop</code>默认为最后一个元素的位置。</p><p><code>&#39;,&#39;.join(map(str, gpu_list))</code>的结果是<code>&#39;1,0,2&#39;</code>。</p><p>最后<code>os.environ.setdefault(&quot;CUDA_VISIBLE_DEVICES&quot;, gpu_list_str)</code>就是根据 GPU 剩余内存从大到小设置对应关系，这样默认最大剩余内存的 GPU 为主 GPU。</p><h3 id="提高-GPU-的利用率"><a href="#提高-GPU-的利用率" class="headerlink" title="提高 GPU 的利用率"></a>提高 GPU 的利用率</h3><p><code>nvidia-smi</code>命令查看可以 GPU 的利用率，如下图所示。</p><p><img src="/img/pytorchtrain7/4.png"><br> 上面的截图中，有两张显卡（GPU），其中<strong>上半部分显示的是显卡的信息</strong>，<strong>下半部分显示的是每张显卡运行的进程</strong>。可以看到编号为 0 的 GPU 运行的是 PID 为 14383 进程。<code>Memory Usage</code>表示显存的使用率，编号为 0 的 GPU 使用了 <code>16555 MB</code> 显存，显存的利用率大概是70% 左右。<code>Volatile GPU-Util</code>表示计算 GPU 实际运算能力的利用率，编号为 0 的 GPU 只有 27% 的使用率。</p><p>虽然使用 GPU 可以加速训练模型，但是如果GPU 的 <code>Memory Usage</code> 和 <code>Volatile GPU-Util</code> 太低，表示并没有充分利用 GPU。</p><p>因此，使用 GPU 训练模型，需要尽量提高 GPU 的 <code>Memory Usage</code> 和 <code>Volatile GPU-Util</code> 这两个指标，可以更进一步加速你的训练过程。</p><p>下面谈谈如何提高这两个指标。</p><h4 id="Memory-Usage"><a href="#Memory-Usage" class="headerlink" title="Memory Usage"></a>Memory Usage</h4><p>这个指标是由数据量主要是由模型大小，以及数据量的大小决定的。</p><p>模型大小是由网络的参数和网络结构决定的，模型越大，训练反而越慢。</p><p>我们主要调整的是每个 batch 训练的数据量的大小，也就是 <strong>batch_size</strong>。</p><p>在模型结构固定的情况下，尽量将<code>batch size</code>设置得比较大，充分利用 GPU 的内存。</p><h4 id="Volatile-GPU-Util"><a href="#Volatile-GPU-Util" class="headerlink" title="Volatile GPU-Util"></a>Volatile GPU-Util</h4><p>上面设置比较大的 <code>batch size</code>可以提高 GPU 的内存使用率，却不一定能提高 GPU 运算单元的使用率。</p><p>从前面可以看到，我们的数据首先读取到 CPU 中的，并在循环训练的时候，通过<code>tensor.to()</code>方法从 CPU 加载到 CPU 中，如下代码所示。</p><figure class="highlight haskell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs haskell"><span class="hljs-meta"># 遍历 train_loader 取数据</span><br><span class="hljs-title">for</span> i, <span class="hljs-class"><span class="hljs-keyword">data</span> in enumerate(<span class="hljs-title">train_loader</span>):</span><br>    inputs, labels = <span class="hljs-class"><span class="hljs-keyword">data</span></span><br>    inputs = inputs.to(device) # 把数据从 <span class="hljs-type">CPU</span> 加载到 <span class="hljs-type">GPU</span><br>    labels = labels.to(device) # 把数据从 <span class="hljs-type">CPU</span> 加载到 <span class="hljs-type">GPU</span><br>    .<br>    .<br>    .<br></code></pre></td></tr></table></figure><p>如果<code>batch size</code>得比较大，那么在 <code>Dataset</code>和 <code>DataLoader</code> ，CPU 处理一个 batch 的数据就会很慢，这时你会发现<code>Volatile GPU-Util</code>的值会在 <code>0%，20%，70%，95%，0%</code> 之间不断变化。</p><blockquote><p><code>nvidia-smi</code>命令查看可以 GPU 的利用率，但不能动态刷新显示。如果你想每隔一秒刷新显示 GPU 信息，可以使用<code>watch -n 1 nvidia-smi</code> 。</p></blockquote><p>其实这是因为 GPU 处理数据非常快，而 CPU 处理数据较慢。GPU 每接收到一个 batch 的数据，使用率就跳到逐渐升高，处理完这个 batch 的数据后，使用率又逐渐降低，等到 CPU 把下一个 batch 的数据传过来。</p><p>解决方法是：设置 <code>Dataloader</code>的两个参数：</p><ul><li>num_workers：默认只使用一个 CPU 读取和处理数据。可以设置为 4、8、16 等参数。但线程数<strong>并不是越大越好</strong>。因为，多核处理需要把数据分发到每个 CPU，处理完成后需要从多个 CPU 收集数据，这个过程也是需要时间的。如果设置<code>num_workers</code>过大，分发和收集数据等操作占用了太多时间，反而会降低效率。</li><li>pin_memory：如果内存较大，<strong>建议设置为 True</strong>。<ul><li>设置为 True，表示把数据直接映射到 GPU 的相关内存块上，省掉了一点数据传输时间。</li><li>设置为 False，表示从 CPU 传入到缓存 RAM 里面，再给传输到 GPU 上。</li></ul></li></ul><h3 id="GPU-相关的报错"><a href="#GPU-相关的报错" class="headerlink" title="GPU 相关的报错"></a>GPU 相关的报错</h3><h4 id="1"><a href="#1" class="headerlink" title="1."></a>1.</h4><p>如果模型是在 GPU 上保存的，在无 GPU 设备上加载模型时<code>torch.load(path_state_dict)</code>,会出现下面的报错：</p><figure class="highlight pgsql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs pgsql">RuntimeError: Attempting <span class="hljs-keyword">to</span> deserialize <span class="hljs-keyword">object</span> <span class="hljs-keyword">on</span> a CUDA device but torch.cuda.is_available() <span class="hljs-keyword">is</span> <span class="hljs-keyword">False</span>. <span class="hljs-keyword">If</span> you are running <span class="hljs-keyword">on</span> a CPU-<span class="hljs-keyword">only</span> machine, please use torch.<span class="hljs-keyword">load</span> <span class="hljs-keyword">with</span> map_location=torch.device(<span class="hljs-string">&#x27;cpu&#x27;</span>) <span class="hljs-keyword">to</span> map your storages <span class="hljs-keyword">to</span> the CPU.<br></code></pre></td></tr></table></figure><p>可能的原因：gpu训练的模型保存后，在无gpu设备上无法直接加载。解决方法是设置<code>map_location=&quot;cpu&quot;</code>：<code>torch.load(path_state_dict, map_location=&quot;cpu&quot;)</code></p><h4 id="2"><a href="#2" class="headerlink" title="2."></a>2.</h4><p>如果模型经过<code>net = nn.DataParallel(net)</code>包装后，那么所有网络层的名称前面都会加上<code>mmodule.</code>。保存模型后再次加载时没有使用<code>nn.DataParallel()</code>包装，就会加载失败，因为<code>state_dict</code>中参数的名称对应不上。</p><figure class="highlight sas"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs sas"><span class="hljs-keyword">Missing</span> <span class="hljs-keyword">key</span>(s) <span class="hljs-keyword">in</span> state_dict: xxxxxxxxxx<br><br>Unexpected <span class="hljs-keyword">key</span>(s) <span class="hljs-keyword">in</span> state_dict:xxxxxxxxxx<br></code></pre></td></tr></table></figure><p>解决方法是加载参数后，遍历 state_dict 的参数，如果名字是以<code>module.</code>开头，则去掉<code>module.</code>。代码如下：</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs stylus">from collections import OrderedDict<br>new_state_dict = <span class="hljs-built_in">OrderedDict</span>()<br><span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> state_dict<span class="hljs-selector-class">.items</span>():<br>    namekey = k<span class="hljs-selector-attr">[7:]</span> <span class="hljs-keyword">if</span> k<span class="hljs-selector-class">.startswith</span>(<span class="hljs-string">&#x27;module.&#x27;</span>) <span class="hljs-keyword">else</span> k<br>    new_state_dict<span class="hljs-selector-attr">[namekey]</span> = v<br></code></pre></td></tr></table></figure><p>然后再把参数加载到模型中。</p><h2 id="PyTorch常见报错汇总"><a href="#PyTorch常见报错汇总" class="headerlink" title="PyTorch常见报错汇总"></a>PyTorch常见报错汇总</h2><p>请查看：<a href="https://shimo.im/docs/bdV4DBxQwUMLrfX5/read">https://shimo.im/docs/bdV4DBxQwUMLrfX5/read</a></p>]]></content>
    
    
    <categories>
      
      <category>深度学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>pytorch</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>深度学习-13-pytorch-6-正则化</title>
    <link href="/2021/08/14/pytorchtrain6/"/>
    <url>/2021/08/14/pytorchtrain6/</url>
    
    <content type="html"><![CDATA[<blockquote><p>创作声明：主要内容参考于张贤同学<a href="https://zhuanlan.zhihu.com/p/265394674">https://zhuanlan.zhihu.com/p/265394674</a></p></blockquote><h2 id="Regularization"><a href="#Regularization" class="headerlink" title="Regularization"></a>Regularization</h2><p>Regularization 中文是正则化，可以理解为一种减少方差的策略。</p><p>在机器学习中，误差可以分解为：偏差，方差与噪声之和。即误差&#x3D;偏差+方差+噪声</p><p>偏差度量了学习算法的期望预测与真实结果的偏离程度，即刻画了学习算法本身的拟合能力。</p><p>方差度量了同样大小的训练集的变动所导致的学习性能的变化，即刻画了数据扰动所造成的影响。</p><p>噪声则表达了在当前任务上学习任何算法所能达到的期望泛化误差的下界。</p><p><img src="/img/pytorchtrain6/1.png"><br> 正则化方式有 L1 和 L2 正则项两种。其中 L2 正则项又被称为权值衰减(weight decay)。</p><p>当没有正则项时：$\boldsymbol{O} \boldsymbol{b} \boldsymbol{j}&#x3D;\boldsymbol{L} \boldsymbol{o} \boldsymbol{s} \boldsymbol{s}$，$w_{i+1}&#x3D;w_{i}-\frac{\partial o b j}{\partial w_{i}}&#x3D;w_{i}-\frac{\partial L o s s}{\partial w_{i}}$。</p><p>当使用 L2 正则项时，$\boldsymbol{O} \boldsymbol{b} \boldsymbol{j}&#x3D;\boldsymbol{L} \boldsymbol{o} \boldsymbol{s} \boldsymbol{s}+\frac{\lambda}{2}  \sum{i}^{N} \boldsymbol{w}{i}^{2}$，$\begin{aligned} w{i+1}&#x3D;w{i}-\frac{\partial o b j}{\partial w{i}} &amp;&#x3D;w{i}-\left(\frac{\partial L o s s}{\partial w_{i}}+\lambda  w{i}\right) &#x3D;w{i}(1-\lambda)-\frac{\partial L o s s}{\partial w_{i}} \end{aligned}$，其中$0 &lt; \lambda &lt; 1$，所以具有权值衰减的作用。</p><h3 id="weight-decay"><a href="#weight-decay" class="headerlink" title="weight decay"></a>weight decay</h3><p>在 PyTorch 中，L2 正则项是在优化器中实现的，在构造优化器时可以传入 weight decay 参数，对应的是公式中的$\lambda $。</p><p>下面代码对比了没有 weight decay 的优化器和 weight decay 为 0.01 的优化器的训练情况，在线性回归的数据集上进行实验，模型使用 3 层的全连接网络，并使用 TensorBoard 可视化每层权值的变化情况。代码如下：</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br></pre></td><td class="code"><pre><code class="hljs routeros">import torch<br>import torch.nn as nn<br>import matplotlib.pyplot as plt<br><span class="hljs-keyword">from</span> common_tools import set_seed<br><span class="hljs-keyword">from</span> tensorboardX import SummaryWriter<br><br>set_seed(1)  # 设置随机种子<br>n_hidden = 200<br>max_iter = 2000<br>disp_interval = 200<br>lr_init = 0.01<br><br><br><span class="hljs-comment"># ============================ step 1/5 数据 ============================</span><br>def gen_data(<span class="hljs-attribute">num_data</span>=10, x_range=(-1, 1)):<br><br>    w = 1.5<br>    train_x = torch.linspace(*x_range, num_data).unsqueeze_(1)<br>    train_y = w*train_x + torch.normal(0, 0.5, <span class="hljs-attribute">size</span>=train_x.size())<br>    test_x = torch.linspace(*x_range, num_data).unsqueeze_(1)<br>    test_y = w*test_x + torch.normal(0, 0.3, <span class="hljs-attribute">size</span>=test_x.size())<br><br>    return train_x, train_y, test_x, test_y<br><br><br>train_x, train_y, test_x, test_y = gen_data(x_range=(-1, 1))<br><br><br><span class="hljs-comment"># ============================ step 2/5 模型 ============================</span><br>class MLP(nn.Module):<br>    def __init__(self, neural_num):<br>        super(MLP, self).__init__()<br>        self.linears = nn.Sequential(<br>            nn.Linear(1, neural_num),<br>            nn.ReLU(<span class="hljs-attribute">inplace</span>=<span class="hljs-literal">True</span>),<br>            nn.Linear(neural_num, neural_num),<br>            nn.ReLU(<span class="hljs-attribute">inplace</span>=<span class="hljs-literal">True</span>),<br>            nn.Linear(neural_num, neural_num),<br>            nn.ReLU(<span class="hljs-attribute">inplace</span>=<span class="hljs-literal">True</span>),<br>            nn.Linear(neural_num, 1),<br>        )<br><br>    def forward(self, x):<br>        return self.linears(x)<br><br><br>net_normal = MLP(<span class="hljs-attribute">neural_num</span>=n_hidden)<br>net_weight_decay = MLP(<span class="hljs-attribute">neural_num</span>=n_hidden)<br><br><span class="hljs-comment"># ============================ step 3/5 优化器 ============================</span><br>optim_normal = torch.optim.SGD(net_normal.parameters(), <span class="hljs-attribute">lr</span>=lr_init, <span class="hljs-attribute">momentum</span>=0.9)<br>optim_wdecay = torch.optim.SGD(net_weight_decay.parameters(), <span class="hljs-attribute">lr</span>=lr_init, <span class="hljs-attribute">momentum</span>=0.9, <span class="hljs-attribute">weight_decay</span>=1e-2)<br><br><span class="hljs-comment"># ============================ step 4/5 损失函数 ============================</span><br>loss_func = torch.nn.MSELoss()<br><br><span class="hljs-comment"># ============================ step 5/5 迭代训练 ============================</span><br><br>writer = SummaryWriter(<span class="hljs-attribute">comment</span>=<span class="hljs-string">&#x27;_test_tensorboard&#x27;</span>, <span class="hljs-attribute">filename_suffix</span>=<span class="hljs-string">&quot;12345678&quot;</span>)<br><span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> range(max_iter):<br><br>    # forward<br>    pred_normal, pred_wdecay = net_normal(train_x), net_weight_decay(train_x)<br>    loss_normal, loss_wdecay = loss_func(pred_normal, train_y), loss_func(pred_wdecay, train_y)<br><br>    optim_normal.zero_grad()<br>    optim_wdecay.zero_grad()<br><br>    loss_normal.backward()<br>    loss_wdecay.backward()<br><br>    optim_normal.<span class="hljs-keyword">step</span>()<br>    optim_wdecay.<span class="hljs-keyword">step</span>()<br><br>    <span class="hljs-keyword">if</span> (epoch+1) % disp_interval == 0:<br><br>        # 可视化<br>        <span class="hljs-keyword">for</span> name, layer <span class="hljs-keyword">in</span> net_normal.named_parameters():<br>            writer.add_histogram(name + <span class="hljs-string">&#x27;_grad_normal&#x27;</span>, layer.grad, epoch)<br>            writer.add_histogram(name + <span class="hljs-string">&#x27;_data_normal&#x27;</span>, layer, epoch)<br><br>        <span class="hljs-keyword">for</span> name, layer <span class="hljs-keyword">in</span> net_weight_decay.named_parameters():<br>            writer.add_histogram(name + <span class="hljs-string">&#x27;_grad_weight_decay&#x27;</span>, layer.grad, epoch)<br>            writer.add_histogram(name + <span class="hljs-string">&#x27;_data_weight_decay&#x27;</span>, layer, epoch)<br><br>        test_pred_normal, test_pred_wdecay = net_normal(test_x), net_weight_decay(test_x)<br><br>        # 绘图<br>        plt.scatter(train_x.data.numpy(), train_y.data.numpy(), <span class="hljs-attribute">c</span>=<span class="hljs-string">&#x27;blue&#x27;</span>, <span class="hljs-attribute">s</span>=50, <span class="hljs-attribute">alpha</span>=0.3, <span class="hljs-attribute">label</span>=<span class="hljs-string">&#x27;train&#x27;</span>)<br>        plt.scatter(test_x.data.numpy(), test_y.data.numpy(), <span class="hljs-attribute">c</span>=<span class="hljs-string">&#x27;red&#x27;</span>, <span class="hljs-attribute">s</span>=50, <span class="hljs-attribute">alpha</span>=0.3, <span class="hljs-attribute">label</span>=<span class="hljs-string">&#x27;test&#x27;</span>)<br>        plt.plot(test_x.data.numpy(), test_pred_normal.data.numpy(), <span class="hljs-string">&#x27;r-&#x27;</span>, <span class="hljs-attribute">lw</span>=3, <span class="hljs-attribute">label</span>=<span class="hljs-string">&#x27;no weight decay&#x27;</span>)<br>        plt.plot(test_x.data.numpy(), test_pred_wdecay.data.numpy(), <span class="hljs-string">&#x27;b--&#x27;</span>, <span class="hljs-attribute">lw</span>=3, <span class="hljs-attribute">label</span>=<span class="hljs-string">&#x27;weight decay&#x27;</span>)<br>        plt.text(-0.25, -1.5, <span class="hljs-string">&#x27;no weight decay loss=&#123;:.6f&#125;&#x27;</span>.format(loss_normal.item()), fontdict=&#123;<span class="hljs-string">&#x27;size&#x27;</span>: 15, <span class="hljs-string">&#x27;color&#x27;</span>: <span class="hljs-string">&#x27;red&#x27;</span>&#125;)<br>        plt.text(-0.25, -2, <span class="hljs-string">&#x27;weight decay loss=&#123;:.6f&#125;&#x27;</span>.format(loss_wdecay.item()), fontdict=&#123;<span class="hljs-string">&#x27;size&#x27;</span>: 15, <span class="hljs-string">&#x27;color&#x27;</span>: <span class="hljs-string">&#x27;red&#x27;</span>&#125;)<br><br>        plt.ylim((-2.5, 2.5))<br>        plt.legend(<span class="hljs-attribute">loc</span>=<span class="hljs-string">&#x27;upper left&#x27;</span>)<br>        plt.title(<span class="hljs-string">&quot;Epoch: &#123;&#125;&quot;</span>.format(epoch+1))<br>        plt.show()<br>        plt.close()<br></code></pre></td></tr></table></figure><p>训练 2000 个 epoch 后，模型如下：</p><p><img src="/img/pytorchtrain6/2.png"><br> 可以看到使用了 weight decay 的模型虽然在训练集的 loss 更高，但是更加平滑，泛化能力更强。</p><p>下面是使用 Tensorboard 可视化的分析。首先查看不带 weight decay 的权值变化过程，第一层权值变化如下：</p><p><img src="/img/pytorchtrain6/3.png"><br> 可以看到从开始到结束，权值的分布都没有什么变化。</p><p>然后查看带 weight decay 的权值变化过程，第一层权值变化如下：</p><p><img src="/img/pytorchtrain6/4.png"><br> 可以看到，加上了 weight decay 后，随便训练次数的增加，权值的分布逐渐靠近 0 均值附近，这就是 L2 正则化的作用，约束权值尽量靠近 0。</p><p>第二层不带 weight decay 的权值变化如下：</p><p><img src="/img/pytorchtrain6/5.png"><br> 第二层带 weight decay 的权值变化如下：</p><p><img src="/img/pytorchtrain6/6.png">  </p><p>由于 weight decay 是在优化器的一个参数，因此在执行<code>optim_wdecay.step()</code>时，会计算 weight decay 后的梯度，具体代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">step</span>(<span class="hljs-params">self, closure=<span class="hljs-literal">None</span></span>):<br>    <span class="hljs-string">&quot;&quot;&quot;Performs a single optimization step.</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Arguments:</span><br><span class="hljs-string">        closure (callable, optional): A closure that reevaluates the model</span><br><span class="hljs-string">            and returns the loss.</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    loss = <span class="hljs-literal">None</span><br>    <span class="hljs-keyword">if</span> closure <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>        loss = closure()<br><br>    <span class="hljs-keyword">for</span> group <span class="hljs-keyword">in</span> <span class="hljs-variable language_">self</span>.param_groups:<br>        weight_decay = group[<span class="hljs-string">&#x27;weight_decay&#x27;</span>]<br>        momentum = group[<span class="hljs-string">&#x27;momentum&#x27;</span>]<br>        dampening = group[<span class="hljs-string">&#x27;dampening&#x27;</span>]<br>        nesterov = group[<span class="hljs-string">&#x27;nesterov&#x27;</span>]<br><br>        <span class="hljs-keyword">for</span> p <span class="hljs-keyword">in</span> group[<span class="hljs-string">&#x27;params&#x27;</span>]:<br>            <span class="hljs-keyword">if</span> p.grad <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>                <span class="hljs-keyword">continue</span><br>            d_p = p.grad.data<br>            <span class="hljs-keyword">if</span> weight_decay != <span class="hljs-number">0</span>:<br>                d_p.add_(weight_decay, p.data)<br>                ...<br>                ...<br>                ...<br>            p.data.add_(-group[<span class="hljs-string">&#x27;lr&#x27;</span>], d_p)<br></code></pre></td></tr></table></figure><p>可以看到：d_p 是计算得到的梯度，如果 weight decay 不为 0，那么更新 $d_p&#x3D;dp+weight_decay \times p.data$，对应公式：$\left(\frac{\partial L o s s}{\partial w_{i}}+\lambda * w_{i}\right)$。最后一行是根据梯度更新权值。</p><h3 id="Dropout"><a href="#Dropout" class="headerlink" title="Dropout"></a>Dropout</h3><p>Dropout 是另一种抑制过拟合的方法。在使用 dropout 时，数据尺度会发生变化，如果设置 dropout_prob &#x3D;0.3，那么在训练时，数据尺度会变为原来的 70%；而在测试时，执行了 model.eval() 后，dropout 是关闭的，因此所有权重需要乘以 (1-dropout_prob)，把数据尺度也缩放到 70%。</p><p>PyTorch 中 Dropout 层如下，通常放在每个网路层的最前面：</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs routeros">torch.nn.Dropout(<span class="hljs-attribute">p</span>=0.5, <span class="hljs-attribute">inplace</span>=<span class="hljs-literal">False</span>)<br></code></pre></td></tr></table></figure><p>参数：</p><ul><li>p：主力需要注意的是，p 是被舍弃的概率，也叫失活概率</li></ul><p>下面实验使用的依然是线性回归的例子，两个网络均是 3 层的全连接层，每层前面都设置 dropout，一个网络的 dropout 设置为 0，另一个网络的 dropout 设置为 0.5，并使用 TensorBoard 可视化每层权值的变化情况。代码如下：</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br></pre></td><td class="code"><pre><code class="hljs routeros">import torch<br>import torch.nn as nn<br>import matplotlib.pyplot as plt<br><span class="hljs-keyword">from</span> common_tools import set_seed<br><span class="hljs-keyword">from</span> tensorboardX import SummaryWriter<br><br>set_seed(1)  # 设置随机种子<br>n_hidden = 200<br>max_iter = 2000<br>disp_interval = 400<br>lr_init = 0.01<br><br><br><span class="hljs-comment"># ============================ step 1/5 数据 ============================</span><br>def gen_data(<span class="hljs-attribute">num_data</span>=10, x_range=(-1, 1)):<br><br>    w = 1.5<br>    train_x = torch.linspace(*x_range, num_data).unsqueeze_(1)<br>    train_y = w*train_x + torch.normal(0, 0.5, <span class="hljs-attribute">size</span>=train_x.size())<br>    test_x = torch.linspace(*x_range, num_data).unsqueeze_(1)<br>    test_y = w*test_x + torch.normal(0, 0.3, <span class="hljs-attribute">size</span>=test_x.size())<br><br>    return train_x, train_y, test_x, test_y<br><br><br>train_x, train_y, test_x, test_y = gen_data(x_range=(-1, 1))<br><br><br><span class="hljs-comment"># ============================ step 2/5 模型 ============================</span><br>class MLP(nn.Module):<br>    def __init__(self, neural_num, <span class="hljs-attribute">d_prob</span>=0.5):<br>        super(MLP, self).__init__()<br>        self.linears = nn.Sequential(<br><br>            nn.Linear(1, neural_num),<br>            nn.ReLU(<span class="hljs-attribute">inplace</span>=<span class="hljs-literal">True</span>),<br><br>            nn.Dropout(d_prob),<br>            nn.Linear(neural_num, neural_num),<br>            nn.ReLU(<span class="hljs-attribute">inplace</span>=<span class="hljs-literal">True</span>),<br><br>            nn.Dropout(d_prob),<br>            nn.Linear(neural_num, neural_num),<br>            nn.ReLU(<span class="hljs-attribute">inplace</span>=<span class="hljs-literal">True</span>),<br><br>            nn.Dropout(d_prob),<br>            nn.Linear(neural_num, 1),<br>        )<br><br>    def forward(self, x):<br>        return self.linears(x)<br><br><br>net_prob_0 = MLP(<span class="hljs-attribute">neural_num</span>=n_hidden, <span class="hljs-attribute">d_prob</span>=0.)<br>net_prob_05 = MLP(<span class="hljs-attribute">neural_num</span>=n_hidden, <span class="hljs-attribute">d_prob</span>=0.5)<br><br><span class="hljs-comment"># ============================ step 3/5 优化器 ============================</span><br>optim_normal = torch.optim.SGD(net_prob_0.parameters(), <span class="hljs-attribute">lr</span>=lr_init, <span class="hljs-attribute">momentum</span>=0.9)<br>optim_reglar = torch.optim.SGD(net_prob_05.parameters(), <span class="hljs-attribute">lr</span>=lr_init, <span class="hljs-attribute">momentum</span>=0.9)<br><br><span class="hljs-comment"># ============================ step 4/5 损失函数 ============================</span><br>loss_func = torch.nn.MSELoss()<br><br><span class="hljs-comment"># ============================ step 5/5 迭代训练 ============================</span><br><br>writer = SummaryWriter(<span class="hljs-attribute">comment</span>=<span class="hljs-string">&#x27;_test_tensorboard&#x27;</span>, <span class="hljs-attribute">filename_suffix</span>=<span class="hljs-string">&quot;12345678&quot;</span>)<br><span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> range(max_iter):<br><br>    pred_normal, pred_wdecay = net_prob_0(train_x), net_prob_05(train_x)<br>    loss_normal, loss_wdecay = loss_func(pred_normal, train_y), loss_func(pred_wdecay, train_y)<br><br>    optim_normal.zero_grad()<br>    optim_reglar.zero_grad()<br><br>    loss_normal.backward()<br>    loss_wdecay.backward()<br><br>    optim_normal.<span class="hljs-keyword">step</span>()<br>    optim_reglar.<span class="hljs-keyword">step</span>()<br><br>    <span class="hljs-keyword">if</span> (epoch+1) % disp_interval == 0:<br><br>        net_prob_0.eval()<br>        net_prob_05.eval()<br><br>        # 可视化<br>        <span class="hljs-keyword">for</span> name, layer <span class="hljs-keyword">in</span> net_prob_0.named_parameters():<br>            writer.add_histogram(name + <span class="hljs-string">&#x27;_grad_normal&#x27;</span>, layer.grad, epoch)<br>            writer.add_histogram(name + <span class="hljs-string">&#x27;_data_normal&#x27;</span>, layer, epoch)<br><br>        <span class="hljs-keyword">for</span> name, layer <span class="hljs-keyword">in</span> net_prob_05.named_parameters():<br>            writer.add_histogram(name + <span class="hljs-string">&#x27;_grad_regularization&#x27;</span>, layer.grad, epoch)<br>            writer.add_histogram(name + <span class="hljs-string">&#x27;_data_regularization&#x27;</span>, layer, epoch)<br><br>        test_pred_prob_0, test_pred_prob_05 = net_prob_0(test_x), net_prob_05(test_x)<br><br>        # 绘图<br>        plt.scatter(train_x.data.numpy(), train_y.data.numpy(), <span class="hljs-attribute">c</span>=<span class="hljs-string">&#x27;blue&#x27;</span>, <span class="hljs-attribute">s</span>=50, <span class="hljs-attribute">alpha</span>=0.3, <span class="hljs-attribute">label</span>=<span class="hljs-string">&#x27;train&#x27;</span>)<br>        plt.scatter(test_x.data.numpy(), test_y.data.numpy(), <span class="hljs-attribute">c</span>=<span class="hljs-string">&#x27;red&#x27;</span>, <span class="hljs-attribute">s</span>=50, <span class="hljs-attribute">alpha</span>=0.3, <span class="hljs-attribute">label</span>=<span class="hljs-string">&#x27;test&#x27;</span>)<br>        plt.plot(test_x.data.numpy(), test_pred_prob_0.data.numpy(), <span class="hljs-string">&#x27;r-&#x27;</span>, <span class="hljs-attribute">lw</span>=3, <span class="hljs-attribute">label</span>=<span class="hljs-string">&#x27;d_prob_0&#x27;</span>)<br>        plt.plot(test_x.data.numpy(), test_pred_prob_05.data.numpy(), <span class="hljs-string">&#x27;b--&#x27;</span>, <span class="hljs-attribute">lw</span>=3, <span class="hljs-attribute">label</span>=<span class="hljs-string">&#x27;d_prob_05&#x27;</span>)<br>        plt.text(-0.25, -1.5, <span class="hljs-string">&#x27;d_prob_0 loss=&#123;:.8f&#125;&#x27;</span>.format(loss_normal.item()), fontdict=&#123;<span class="hljs-string">&#x27;size&#x27;</span>: 15, <span class="hljs-string">&#x27;color&#x27;</span>: <span class="hljs-string">&#x27;red&#x27;</span>&#125;)<br>        plt.text(-0.25, -2, <span class="hljs-string">&#x27;d_prob_05 loss=&#123;:.6f&#125;&#x27;</span>.format(loss_wdecay.item()), fontdict=&#123;<span class="hljs-string">&#x27;size&#x27;</span>: 15, <span class="hljs-string">&#x27;color&#x27;</span>: <span class="hljs-string">&#x27;red&#x27;</span>&#125;)<br><br>        plt.ylim((-2.5, 2.5))<br>        plt.legend(<span class="hljs-attribute">loc</span>=<span class="hljs-string">&#x27;upper left&#x27;</span>)<br>        plt.title(<span class="hljs-string">&quot;Epoch: &#123;&#125;&quot;</span>.format(epoch+1))<br>        plt.show()<br>        plt.close()<br><br>        net_prob_0.train()<br>        net_prob_05.train()<br></code></pre></td></tr></table></figure><p>训练 2000 次后，模型的曲线如下：</p><p><img src="/img/pytorchtrain6/7.png"><br> 我们使用 TensorBoard 查看第三层网络的权值变化情况。</p><p>dropout &#x3D;0 的权值变化如下：</p><p><img src="/img/pytorchtrain6/8.png"><br> dropout &#x3D;0.5 的权值变化如下：</p><p><img src="/img/pytorchtrain6/9.png"><br> 可以看到，加了 dropout 之后，权值更加集中在 0 附近，使得神经元之间的依赖性不至于过大。</p><h3 id="model-eval-和-model-trian"><a href="#model-eval-和-model-trian" class="headerlink" title="model.eval() 和 model.trian()"></a>model.eval() 和 model.trian()</h3><p>有些网络层在训练状态和测试状态是不一样的，如 dropout 层，在训练时 dropout 层是有效的，但是数据尺度会缩放，为了保持数据尺度不变，所有的权重需要除以 1-p。而在测试时 dropout 层是关闭的。因此在测试时需要先调用<code>model.eval()</code>设置各个网络层的的<code>training</code>属性为 False，在训练时需要先调用<code>model.train()</code>设置各个网络层的的<code>training</code>属性为 True。</p><p>下面是对比 dropout 层的在 eval 和 train 模式下的输出值。</p><p>首先构造一层全连接网络，输入是 10000 个神经元，输出是 1 个神经元，权值全设为 1，dropout 设置为 0.5。输入是全为 1 的向量。分别测试网络在 train 模式和 eval 模式下的输出，代码如下：</p><figure class="highlight scss"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><code class="hljs scss">import torch<br>import torch<span class="hljs-selector-class">.nn</span> as nn<br><br>class <span class="hljs-built_in">Net</span>(nn.Module):<br>    def <span class="hljs-built_in">__init__</span>(self, neural_num, d_prob=<span class="hljs-number">0.5</span>):<br>        <span class="hljs-built_in">super</span>(Net, self).<span class="hljs-built_in">__init__</span>()<br><br>        self.linears = nn.<span class="hljs-built_in">Sequential</span>(<br><br>            nn.<span class="hljs-built_in">Dropout</span>(d_prob),<br>            nn.<span class="hljs-built_in">Linear</span>(neural_num, <span class="hljs-number">1</span>, bias=False),<br>            nn.<span class="hljs-built_in">ReLU</span>(inplace=True)<br>        )<br><br>    def <span class="hljs-built_in">forward</span>(self, x):<br>        return self.<span class="hljs-built_in">linears</span>(x)<br><br>input_num = <span class="hljs-number">10000</span><br>x = torch.<span class="hljs-built_in">ones</span>((input_num, ), dtype=torch.float32)<br><br>net = <span class="hljs-built_in">Net</span>(input_num, d_prob=<span class="hljs-number">0.5</span>)<br>net.linears[<span class="hljs-number">1</span>].weight.<span class="hljs-built_in">detach</span>().<span class="hljs-built_in">fill_</span>(<span class="hljs-number">1</span>.)<br><br>net.<span class="hljs-built_in">train</span>()<br>y = <span class="hljs-built_in">net</span>(x)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;output in training mode&quot;</span>, y)<br><br>net.<span class="hljs-built_in">eval</span>()<br>y = <span class="hljs-built_in">net</span>(x)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;output in eval mode&quot;</span>, y)<br></code></pre></td></tr></table></figure><p>输出如下：</p><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs vim">output in training <span class="hljs-keyword">mode</span> tensor([<span class="hljs-number">9868</span>.], grad_fn=<span class="hljs-symbol">&lt;ReluBackward1&gt;</span>)<br>output in <span class="hljs-built_in">eval</span> <span class="hljs-keyword">mode</span> tensor([<span class="hljs-number">10000</span>.], grad_fn=<span class="hljs-symbol">&lt;ReluBackward1&gt;</span>)<br></code></pre></td></tr></table></figure><p>在训练时，由于 dropout 为 0.5，因此理论上输出值是 5000，而由于在训练时，dropout 层会把权值除以 1-p&#x3D;0.5，也就是乘以 2，因此在 train 模式的输出是10000 附近的数(上下随机浮动是由于概率的不确定性引起的) 。而在 eval 模式下，关闭了 dropout，因此输出值是 10000。这种方式在训练时对权值进行缩放，在测试时就不用对权值进行缩放，加快了测试的速度。</p><h2 id="Batch-Normalization"><a href="#Batch-Normalization" class="headerlink" title="Batch Normalization"></a>Batch Normalization</h2><p>称为批标准化。批是指一批数据，通常为 mini-batch；标准化是处理后的数据服从$N(0,1)$的正态分布。</p><p>批标准化的优点有如下：</p><ul><li>可以使用更大的学习率，加速模型收敛</li><li>可以不用精心设计权值初始化</li><li>可以不用 dropout 或者较小的 dropout</li><li>可以不用 L2 或者较小的 weight decay</li><li>可以不用 LRN (local response normalization)</li></ul><p>假设输入的 mini-batch 数据是<img src="/img/pytorchtrain6/10.png">，Batch Normalization 的可学习参数是$\gamma, \beta$，步骤如下：</p><ul><li>求 mini-batch 的均值：$\mu_{\mathcal{B}} \leftarrow \frac{1}{m} \sum_{i&#x3D;1}^{m} x_{i}$</li><li>求 mini-batch 的方差：$\sigma_{\mathcal{B}}^{2} \leftarrow \frac{1}{m} \sum_{i&#x3D;1}\left(x_{i}-\mu_{\mathcal{B}}\right)^{2}$</li><li>标准化：$\widehat{x}{i} \leftarrow \frac{x{i}-\mu{\mathcal{B}}}{\sqrt{\sigma{B}^{2}+\epsilon}}$，其中$\epsilon$ 是放置分母为 0 的一个数</li><li>affine transform(缩放和平移)：$y{i} \leftarrow \gamma \widehat{x}{i}+\beta \equiv \mathrm{B} \mathrm{N}{\gamma, \beta}\left(x{i}\right)$，这个操作可以增强模型的 capacity，也就是让模型自己判断是否要对数据进行标准化，进行多大程度的标准化。如果$\gamma&#x3D; \sqrt{\sigma_{B}^{2}}$，$\beta&#x3D;\mu_{\mathcal{B}}$，那么就实现了恒等映射。</li></ul><p>Batch Normalization 的提出主要是为了解决 Internal Covariate Shift (ICS)。在训练过程中，数据需要经过多层的网络，如果数据在前向传播的过程中，尺度发生了变化，可能会导致梯度爆炸或者梯度消失，从而导致模型难以收敛。</p><p>Batch Normalization 层一般在激活函数前一层。</p><p>下面的代码打印一个网络的每个网络层的输出，在没有进行初始化时，数据尺度越来越小。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<br><span class="hljs-keyword">from</span> common_tools <span class="hljs-keyword">import</span> set_seed<br><br>set_seed(<span class="hljs-number">1</span>)  <span class="hljs-comment"># 设置随机种子</span><br><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">MLP</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, neural_num, layers=<span class="hljs-number">100</span></span>):<br>        <span class="hljs-built_in">super</span>(MLP, <span class="hljs-variable language_">self</span>).__init__()<br>        <span class="hljs-variable language_">self</span>.linears = nn.ModuleList([nn.Linear(neural_num, neural_num, bias=<span class="hljs-literal">False</span>) <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(layers)])<br>        <span class="hljs-variable language_">self</span>.bns = nn.ModuleList([nn.BatchNorm1d(neural_num) <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(layers)])<br>        <span class="hljs-variable language_">self</span>.neural_num = neural_num<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br><br>        <span class="hljs-keyword">for</span> (i, linear), bn <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(<span class="hljs-built_in">enumerate</span>(<span class="hljs-variable language_">self</span>.linears), <span class="hljs-variable language_">self</span>.bns):<br>            x = linear(x)<br>            <span class="hljs-comment"># x = bn(x)</span><br>            x = torch.relu(x)<br><br>            <span class="hljs-keyword">if</span> torch.isnan(x.std()):<br>                <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;output is nan in &#123;&#125; layers&quot;</span>.<span class="hljs-built_in">format</span>(i))<br>                <span class="hljs-keyword">break</span><br><br>            <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;layers:&#123;&#125;, std:&#123;&#125;&quot;</span>.<span class="hljs-built_in">format</span>(i, x.std().item()))<br><br>        <span class="hljs-keyword">return</span> x<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">initialize</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-keyword">for</span> m <span class="hljs-keyword">in</span> <span class="hljs-variable language_">self</span>.modules():<br>            <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(m, nn.Linear):<br><br>                <span class="hljs-comment"># method 1</span><br>                <span class="hljs-comment"># nn.init.normal_(m.weight.data, std=1)    # normal: mean=0, std=1</span><br><br>                <span class="hljs-comment"># method 2 kaiming</span><br>                nn.init.kaiming_normal_(m.weight.data)<br><br><br>neural_nums = <span class="hljs-number">256</span><br>layer_nums = <span class="hljs-number">100</span><br>batch_size = <span class="hljs-number">16</span><br><br>net = MLP(neural_nums, layer_nums)<br><span class="hljs-comment"># net.initialize()</span><br><br>inputs = torch.randn((batch_size, neural_nums))  <span class="hljs-comment"># normal: mean=0, std=1</span><br><br>output = net(inputs)<br><span class="hljs-built_in">print</span>(output)<br></code></pre></td></tr></table></figure><p>当使用<code>nn.init.kaiming_normal_()</code>初始化后，数据的标准差尺度稳定在 [0.6, 0.9]。</p><p>当我们不对网络层进行权值初始化，而是在每个激活函数层之前使用 bn 层，查看数据的标准差尺度稳定在 [0.58, 0.59]。因此 Batch Normalization 可以不用精心设计权值初始化。</p><p>下面以人民币二分类实验中的 LeNet 为例，添加 bn 层，对比不带 bn 层的网络和带 bn 层的网络的训练过程。</p><p>不带 bn 层的网络，并且使用 kaiming 初始化权值，训练过程如下：</p><p><img src="/img/pytorchtrain6/11.png"><br> 可以看到训练过程中，训练集的 loss 在中间激增到 1.4，不够稳定。</p><p>带有 bn 层的 LeNet 定义如下：</p><figure class="highlight haskell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><code class="hljs haskell"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-type">LeNet_bn</span>(<span class="hljs-title">nn</span>.<span class="hljs-type">Module</span>):</span><br><span class="hljs-class">    def __init__(<span class="hljs-title">self</span>, <span class="hljs-title">classes</span>):</span><br><span class="hljs-class">        super(<span class="hljs-type">LeNet_bn</span>, <span class="hljs-title">self</span>).__init__()</span><br><span class="hljs-class">        self.conv1 = nn.<span class="hljs-type">Conv2d</span>(3, 6, 5)</span><br><span class="hljs-class">        self.bn1 = nn.<span class="hljs-type">BatchNorm2d</span>(<span class="hljs-title">num_features</span>=6)</span><br><span class="hljs-class"></span><br><span class="hljs-class">        self.conv2 = nn.<span class="hljs-type">Conv2d</span>(6, 16, 5)</span><br><span class="hljs-class">        self.bn2 = nn.<span class="hljs-type">BatchNorm2d</span>(<span class="hljs-title">num_features</span>=16)</span><br><span class="hljs-class"></span><br><span class="hljs-class">        self.fc1 = nn.<span class="hljs-type">Linear</span>(16 * 5 * 5, 120)</span><br><span class="hljs-class">        self.bn3 = nn.<span class="hljs-type">BatchNorm1d</span>(<span class="hljs-title">num_features</span>=120)</span><br><span class="hljs-class"></span><br><span class="hljs-class">        self.fc2 = nn.<span class="hljs-type">Linear</span>(120, 84)</span><br><span class="hljs-class">        self.fc3 = nn.<span class="hljs-type">Linear</span>(84, <span class="hljs-title">classes</span>)</span><br><span class="hljs-class"></span><br><span class="hljs-class">    def forward(<span class="hljs-title">self</span>, <span class="hljs-title">x</span>):</span><br><span class="hljs-class">        out = self.conv1(<span class="hljs-title">x</span>)</span><br><span class="hljs-class">        out = self.bn1(<span class="hljs-title">out</span>)</span><br><span class="hljs-class">        out = <span class="hljs-type">F</span>.relu(<span class="hljs-title">out</span>)</span><br><span class="hljs-class"></span><br><span class="hljs-class">        out = <span class="hljs-type">F</span>.max_pool2d(<span class="hljs-title">out</span>, 2)</span><br><span class="hljs-class"></span><br><span class="hljs-class">        out = self.conv2(<span class="hljs-title">out</span>)</span><br><span class="hljs-class">        out = self.bn2(<span class="hljs-title">out</span>)</span><br><span class="hljs-class">        out = <span class="hljs-type">F</span>.relu(<span class="hljs-title">out</span>)</span><br><span class="hljs-class"></span><br><span class="hljs-class">        out = <span class="hljs-type">F</span>.max_pool2d(<span class="hljs-title">out</span>, 2)</span><br><span class="hljs-class"></span><br><span class="hljs-class">        out = out.view(<span class="hljs-title">out</span>.<span class="hljs-title">size</span>(0), -1)</span><br><span class="hljs-class"></span><br><span class="hljs-class">        out = self.fc1(<span class="hljs-title">out</span>)</span><br><span class="hljs-class">        out = self.bn3(<span class="hljs-title">out</span>)</span><br><span class="hljs-class">        out = <span class="hljs-type">F</span>.relu(<span class="hljs-title">out</span>)</span><br><span class="hljs-class"></span><br><span class="hljs-class">        out = <span class="hljs-type">F</span>.relu(<span class="hljs-title">self</span>.<span class="hljs-title">fc2</span>(<span class="hljs-title">out</span>))</span><br><span class="hljs-class">        out = self.fc3(<span class="hljs-title">out</span>)</span><br><span class="hljs-class">        return out</span><br></code></pre></td></tr></table></figure><p>带 bn 层的网络，并且不使用 kaiming 初始化权值，训练过程如下：</p><p><img src="/img/pytorchtrain6/12.png"><br> 虽然训练过程中，训练集的 loss 也有激增，但只是增加到 0.4，非常稳定。</p><h3 id="Batch-Normalization-in-PyTorch"><a href="#Batch-Normalization-in-PyTorch" class="headerlink" title="Batch Normalization in PyTorch"></a>Batch Normalization in PyTorch</h3><p>在 PyTorch 中，有 3 个 Batch Normalization 类</p><ul><li>nn.BatchNorm1d()，输入数据的形状是 $B \times C \times 1D_feature$</li><li>nn.BatchNorm2d()，输入数据的形状是 $B \times C \times 2D_feature$</li><li>nn.BatchNorm3d()，输入数据的形状是 $B \times C \times 3D_feature$</li></ul><p>以<code>nn.BatchNorm1d()</code>为例，如下：</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs routeros">torch.nn.BatchNorm1d(num_features, <span class="hljs-attribute">eps</span>=1e-05, <span class="hljs-attribute">momentum</span>=0.1, <span class="hljs-attribute">affine</span>=<span class="hljs-literal">True</span>, <span class="hljs-attribute">track_running_stats</span>=<span class="hljs-literal">True</span>)<br></code></pre></td></tr></table></figure><p>参数：</p><ul><li>num_features：一个样本的特征数量，这个参数最重要</li><li>eps：在进行标准化操作时的分布修正项</li><li>momentum：指数加权平均估计当前的均值和方差</li><li>affine：是否需要 affine transform，默认为 True</li><li>track_running_stats：True 为训练状态，此时均值和方差会根据每个 mini-batch 改变。False 为测试状态，此时均值和方差会固定</li></ul><p>主要属性：</p><ul><li>runninng_mean：均值</li><li>running_var：方差</li><li>weight：affine transform 中的 $\gamma$</li><li>bias：affine transform 中的 $\beta$</li></ul><p>在训练时，均值和方差采用指数加权平均计算，也就是不仅考虑当前 mini-batch 的值均值和方差还考虑前面的 mini-batch 的均值和方差。</p><p>在训练时，均值方差固定为当前统计值。</p><p>所有的 bn 层都是根据<strong>特征维度</strong>计算上面 4 个属性，详情看下面例子。</p><h3 id="nn-BatchNorm1d"><a href="#nn-BatchNorm1d" class="headerlink" title="nn.BatchNorm1d()"></a>nn.BatchNorm1d()</h3><p>输入数据的形状是 $B \times C \times 1D_feature$。在下面的例子中，数据的维度是：(3, 5, 1)，表示一个 mini-batch 有 3 个样本，每个样本有 5 个特征，每个特征的维度是 1。那么就会计算 5 个均值和方差，分别对应每个特征维度。momentum 设置为 0.3，第一次的均值和方差默认为 0 和 1。输入两次 mini-batch 的数据。</p><p>数据如下图：</p><p><img src="/img/pytorchtrain6/13.png"><br> 代码如下所示：</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">batch_size</span> = <span class="hljs-number">3</span><br><span class="hljs-attribute">num_features</span> = <span class="hljs-number">5</span><br><span class="hljs-attribute">momentum</span> = <span class="hljs-number">0</span>.<span class="hljs-number">3</span><br><br><span class="hljs-attribute">features_shape</span> = (<span class="hljs-number">1</span>)<br><br><span class="hljs-attribute">feature_map</span> = torch.ones(features_shape)                                                    # <span class="hljs-number">1</span>D<br><span class="hljs-attribute">feature_maps</span> = torch.stack([feature_map*(i+<span class="hljs-number">1</span>) for i in range(num_features)], dim=<span class="hljs-number">0</span>)         # <span class="hljs-number">2</span>D<br><span class="hljs-attribute">feature_maps_bs</span> = torch.stack([feature_maps for i in range(batch_size)], dim=<span class="hljs-number">0</span>)             # <span class="hljs-number">3</span>D<br><span class="hljs-attribute">print</span>(<span class="hljs-string">&quot;input data:\n&#123;&#125; shape is &#123;&#125;&quot;</span>.format(feature_maps_bs, feature_maps_bs.shape))<br><br><span class="hljs-attribute">bn</span> = nn.BatchNorm1d(num_features=num_features, momentum=momentum)<br><br><span class="hljs-attribute">running_mean</span>, running_var = <span class="hljs-number">0</span>, <span class="hljs-number">1</span><br><span class="hljs-attribute">mean_t</span>, var_t = <span class="hljs-number">2</span>, <span class="hljs-number">0</span><br><span class="hljs-attribute">for</span> i in range(<span class="hljs-number">2</span>):<br>    <span class="hljs-attribute">outputs</span> = bn(feature_maps_bs)<br><br>    <span class="hljs-attribute">print</span>(<span class="hljs-string">&quot;\niteration:&#123;&#125;, running mean: &#123;&#125; &quot;</span>.format(i, bn.running_mean))<br>    <span class="hljs-attribute">print</span>(<span class="hljs-string">&quot;iteration:&#123;&#125;, running var:&#123;&#125; &quot;</span>.format(i, bn.running_var))<br><br><br><br>    <span class="hljs-attribute">running_mean</span> = (<span class="hljs-number">1</span> - momentum) * running_mean + momentum * mean_t<br>    <span class="hljs-attribute">running_var</span> = (<span class="hljs-number">1</span> - momentum) * running_var + momentum * var_t<br><br>    <span class="hljs-attribute">print</span>(<span class="hljs-string">&quot;iteration:&#123;&#125;, 第二个特征的running mean: &#123;&#125; &quot;</span>.format(i, running_mean))<br>    <span class="hljs-attribute">print</span>(<span class="hljs-string">&quot;iteration:&#123;&#125;, 第二个特征的running var:&#123;&#125;&quot;</span>.format(i, running_var))<br></code></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><code class="hljs stylus"><span class="hljs-selector-tag">input</span> data:<br><span class="hljs-built_in">tensor</span>(<span class="hljs-selector-attr">[[[1.]</span>,<br>         <span class="hljs-selector-attr">[2.]</span>,<br>         <span class="hljs-selector-attr">[3.]</span>,<br>         <span class="hljs-selector-attr">[4.]</span>,<br>         <span class="hljs-selector-attr">[5.]</span>],<br>        <span class="hljs-selector-attr">[[1.]</span>,<br>         <span class="hljs-selector-attr">[2.]</span>,<br>         <span class="hljs-selector-attr">[3.]</span>,<br>         <span class="hljs-selector-attr">[4.]</span>,<br>         <span class="hljs-selector-attr">[5.]</span>],<br>        <span class="hljs-selector-attr">[[1.]</span>,<br>         <span class="hljs-selector-attr">[2.]</span>,<br>         <span class="hljs-selector-attr">[3.]</span>,<br>         <span class="hljs-selector-attr">[4.]</span>,<br>         <span class="hljs-selector-attr">[5.]</span>]]) shape is torch<span class="hljs-selector-class">.Size</span>(<span class="hljs-selector-attr">[3, 5, 1]</span>)<br>iteration:<span class="hljs-number">0</span>, running mean: <span class="hljs-built_in">tensor</span>(<span class="hljs-selector-attr">[0.3000, 0.6000, 0.9000, 1.2000, 1.5000]</span>) <br>iteration:<span class="hljs-number">0</span>, running <span class="hljs-selector-tag">var</span>:<span class="hljs-built_in">tensor</span>(<span class="hljs-selector-attr">[0.7000, 0.7000, 0.7000, 0.7000, 0.7000]</span>) <br>iteration:<span class="hljs-number">0</span>, 第二个特征的running mean: <span class="hljs-number">0.6</span> <br>iteration:<span class="hljs-number">0</span>, 第二个特征的running <span class="hljs-selector-tag">var</span>:<span class="hljs-number">0.7</span><br>iteration:<span class="hljs-number">1</span>, running mean: <span class="hljs-built_in">tensor</span>(<span class="hljs-selector-attr">[0.5100, 1.0200, 1.5300, 2.0400, 2.5500]</span>) <br>iteration:<span class="hljs-number">1</span>, running <span class="hljs-selector-tag">var</span>:<span class="hljs-built_in">tensor</span>(<span class="hljs-selector-attr">[0.4900, 0.4900, 0.4900, 0.4900, 0.4900]</span>) <br>iteration:<span class="hljs-number">1</span>, 第二个特征的running mean: <span class="hljs-number">1.02</span> <br>iteration:<span class="hljs-number">1</span>, 第二个特征的running <span class="hljs-selector-tag">var</span>:<span class="hljs-number">0.48999999999999994</span><br></code></pre></td></tr></table></figure><p>虽然两个 mini-batch 的数据是一样的，但是 bn 层的均值和方差却不一样。以第二个特征的均值计算为例，值都是 2。</p><ul><li>第一次 bn 层的均值计算：$running_mean&#x3D;(1-momentum) \times pre_running_mean + momentum \times mean_t &#x3D;(1-0.3) \times 0 + 0.3 \times 2 &#x3D;0.6$</li><li>第二次 bn 层的均值计算：$running_mean&#x3D;(1-momentum) \times pre_running_mean + momentum \times mean_t &#x3D;(1-0.3) \times 0.6 + 0.3 \times 2 &#x3D;1.02$</li></ul><p>网络还没进行前向传播之前，断点查看 bn 层的属性如下：</p><p><img src="/img/pytorchtrain6/14.png"></p><h3 id="nn-BatchNorm2d"><a href="#nn-BatchNorm2d" class="headerlink" title="nn.BatchNorm2d()"></a>nn.BatchNorm2d()</h3><p>输入数据的形状是 $B \times C \times 2D_feature$。在下面的例子中，数据的维度是：(3, 3, 2, 2)，表示一个 mini-batch 有 3 个样本，每个样本有 3 个特征，每个特征的维度是 $1 \times 2$。那么就会计算 3 个均值和方差，分别对应每个特征维度。momentum 设置为 0.3，第一次的均值和方差默认为 0 和 1。输入两次 mini-batch 的数据。</p><p>数据如下图：</p><p><img src="/img/pytorchtrain6/15.png"><br> 代码如下：</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><code class="hljs routeros">batch_size = 3<br>num_features = 3<br>momentum = 0.3<br><br>features_shape = (2, 2)<br><br>feature_map = torch.ones(features_shape)                                                    # 2D<br>feature_maps = torch.stack([feature_map*(i+1) <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(num_features)], <span class="hljs-attribute">dim</span>=0)         # 3D<br>feature_maps_bs = torch.stack([feature_maps <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(batch_size)], <span class="hljs-attribute">dim</span>=0)             # 4D<br><br><span class="hljs-comment"># print(&quot;input data:\n&#123;&#125; shape is &#123;&#125;&quot;.format(feature_maps_bs, feature_maps_bs.shape))</span><br><br>bn = nn.BatchNorm2d(<span class="hljs-attribute">num_features</span>=num_features, <span class="hljs-attribute">momentum</span>=momentum)<br><br>running_mean, running_var = 0, 1<br><br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(2):<br>    outputs = bn(feature_maps_bs)<br><br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;\niter:&#123;&#125;, running_mean: &#123;&#125;&quot;</span>.format(i, bn.running_mean))<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;iter:&#123;&#125;, running_var: &#123;&#125;&quot;</span>.format(i, bn.running_var))<br><br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;iter:&#123;&#125;, weight: &#123;&#125;&quot;</span>.format(i, bn.weight.data.numpy()))<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;iter:&#123;&#125;, bias: &#123;&#125;&quot;</span>.format(i, bn.bias.data.numpy()))<br></code></pre></td></tr></table></figure><p>输出如下：</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">iter</span>:<span class="hljs-number">0</span>, running_mean: tensor([<span class="hljs-number">0</span>.<span class="hljs-number">3000</span>, <span class="hljs-number">0</span>.<span class="hljs-number">6000</span>, <span class="hljs-number">0</span>.<span class="hljs-number">9000</span>])<br><span class="hljs-attribute">iter</span>:<span class="hljs-number">0</span>, running_var: tensor([<span class="hljs-number">0</span>.<span class="hljs-number">7000</span>, <span class="hljs-number">0</span>.<span class="hljs-number">7000</span>, <span class="hljs-number">0</span>.<span class="hljs-number">7000</span>])<br><span class="hljs-attribute">iter</span>:<span class="hljs-number">0</span>, weight:<span class="hljs-meta"> [1. 1. 1.]</span><br><span class="hljs-attribute">iter</span>:<span class="hljs-number">0</span>, bias:<span class="hljs-meta"> [0. 0. 0.]</span><br><span class="hljs-attribute">iter</span>:<span class="hljs-number">1</span>, running_mean: tensor([<span class="hljs-number">0</span>.<span class="hljs-number">5100</span>, <span class="hljs-number">1</span>.<span class="hljs-number">0200</span>, <span class="hljs-number">1</span>.<span class="hljs-number">5300</span>])<br><span class="hljs-attribute">iter</span>:<span class="hljs-number">1</span>, running_var: tensor([<span class="hljs-number">0</span>.<span class="hljs-number">4900</span>, <span class="hljs-number">0</span>.<span class="hljs-number">4900</span>, <span class="hljs-number">0</span>.<span class="hljs-number">4900</span>])<br><span class="hljs-attribute">iter</span>:<span class="hljs-number">1</span>, weight:<span class="hljs-meta"> [1. 1. 1.]</span><br><span class="hljs-attribute">iter</span>:<span class="hljs-number">1</span>, bias:<span class="hljs-meta"> [0. 0. 0.]</span><br></code></pre></td></tr></table></figure><h3 id="nn-BatchNorm3d"><a href="#nn-BatchNorm3d" class="headerlink" title="nn.BatchNorm3d()"></a>nn.BatchNorm3d()</h3><p>输入数据的形状是 $B \times C \times 3D_feature$。在下面的例子中，数据的维度是：(3, 2, 2, 2, 3)，表示一个 mini-batch 有 3 个样本，每个样本有 2 个特征，每个特征的维度是 $2 \times 2 \times 3$。那么就会计算 2 个均值和方差，分别对应每个特征维度。momentum 设置为 0.3，第一次的均值和方差默认为 0 和 1。输入两次 mini-batch 的数据。</p><p>数据如下图：</p><p><img src="/img/pytorchtrain6/16.png"><br> 代码如下：</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">batch_size</span> = <span class="hljs-number">3</span><br><span class="hljs-attribute">num_features</span> = <span class="hljs-number">3</span><br><span class="hljs-attribute">momentum</span> = <span class="hljs-number">0</span>.<span class="hljs-number">3</span><br><br><span class="hljs-attribute">features_shape</span> = (<span class="hljs-number">2</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>)<br><br><span class="hljs-attribute">feature</span> = torch.ones(features_shape)                                                # <span class="hljs-number">3</span>D<br><span class="hljs-attribute">feature_map</span> = torch.stack([feature * (i + <span class="hljs-number">1</span>) for i in range(num_features)], dim=<span class="hljs-number">0</span>)  # <span class="hljs-number">4</span>D<br><span class="hljs-attribute">feature_maps</span> = torch.stack([feature_map for i in range(batch_size)], dim=<span class="hljs-number">0</span>)         # <span class="hljs-number">5</span>D<br><br><span class="hljs-comment"># print(&quot;input data:\n&#123;&#125; shape is &#123;&#125;&quot;.format(feature_maps, feature_maps.shape))</span><br><br><span class="hljs-attribute">bn</span> = nn.BatchNorm3d(num_features=num_features, momentum=momentum)<br><br><span class="hljs-attribute">running_mean</span>, running_var = <span class="hljs-number">0</span>, <span class="hljs-number">1</span><br><br><span class="hljs-attribute">for</span> i in range(<span class="hljs-number">2</span>):<br>    <span class="hljs-attribute">outputs</span> = bn(feature_maps)<br><br>    <span class="hljs-attribute">print</span>(<span class="hljs-string">&quot;\niter:&#123;&#125;, running_mean.shape: &#123;&#125;&quot;</span>.format(i, bn.running_mean.shape))<br>    <span class="hljs-attribute">print</span>(<span class="hljs-string">&quot;iter:&#123;&#125;, running_var.shape: &#123;&#125;&quot;</span>.format(i, bn.running_var.shape))<br><br>    <span class="hljs-attribute">print</span>(<span class="hljs-string">&quot;iter:&#123;&#125;, weight.shape: &#123;&#125;&quot;</span>.format(i, bn.weight.shape))<br>    <span class="hljs-attribute">print</span>(<span class="hljs-string">&quot;iter:&#123;&#125;, bias.shape: &#123;&#125;&quot;</span>.format(i, bn.bias.shape))<br></code></pre></td></tr></table></figure><p>输出如下：</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs stylus">iter:<span class="hljs-number">0</span>, running_mean<span class="hljs-selector-class">.shape</span>: torch<span class="hljs-selector-class">.Size</span>(<span class="hljs-selector-attr">[3]</span>)<br>iter:<span class="hljs-number">0</span>, running_var<span class="hljs-selector-class">.shape</span>: torch<span class="hljs-selector-class">.Size</span>(<span class="hljs-selector-attr">[3]</span>)<br>iter:<span class="hljs-number">0</span>, weight<span class="hljs-selector-class">.shape</span>: torch<span class="hljs-selector-class">.Size</span>(<span class="hljs-selector-attr">[3]</span>)<br>iter:<span class="hljs-number">0</span>, bias<span class="hljs-selector-class">.shape</span>: torch<span class="hljs-selector-class">.Size</span>(<span class="hljs-selector-attr">[3]</span>)<br>iter:<span class="hljs-number">1</span>, running_mean<span class="hljs-selector-class">.shape</span>: torch<span class="hljs-selector-class">.Size</span>(<span class="hljs-selector-attr">[3]</span>)<br>iter:<span class="hljs-number">1</span>, running_var<span class="hljs-selector-class">.shape</span>: torch<span class="hljs-selector-class">.Size</span>(<span class="hljs-selector-attr">[3]</span>)<br>iter:<span class="hljs-number">1</span>, weight<span class="hljs-selector-class">.shape</span>: torch<span class="hljs-selector-class">.Size</span>(<span class="hljs-selector-attr">[3]</span>)<br>iter:<span class="hljs-number">1</span>, bias<span class="hljs-selector-class">.shape</span>: torch<span class="hljs-selector-class">.Size</span>(<span class="hljs-selector-attr">[3]</span>)<br></code></pre></td></tr></table></figure><h2 id="Layer-Normalization"><a href="#Layer-Normalization" class="headerlink" title="Layer Normalization"></a>Layer Normalization</h2><p>提出的原因：Batch Normalization 不适用于变长的网络，如 RNN</p><p>思路：每个网络层计算均值和方差</p><p>注意事项：</p><ul><li>不再有 running_mean 和 running_var</li><li>$\gamma$ 和 $\beta$ 为逐样本的</li></ul><p><img src="/img/pytorchtrain6/17.png"></p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs routeros">torch.nn.LayerNorm(normalized_shape, <span class="hljs-attribute">eps</span>=1e-05, <span class="hljs-attribute">elementwise_affine</span>=<span class="hljs-literal">True</span>)<br></code></pre></td></tr></table></figure><p>参数：</p><ul><li>normalized_shape：该层特征的形状，可以取$C \times H \times W$、$H \times W$、$W$</li><li>eps：标准化时的分母修正项</li><li>elementwise_affine：是否需要逐个样本 affine transform</li></ul><p>下面代码中，输入数据的形状是 $B \times C \times feature$，(8, 2, 3, 4)，表示一个 mini-batch 有 8 个样本，每个样本有 2 个特征，每个特征的维度是 $3 \times 4$。那么就会计算 8 个均值和方差，分别对应每个样本。</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">batch_size</span> = <span class="hljs-number">8</span><br><span class="hljs-attribute">num_features</span> = <span class="hljs-number">2</span><br><br><span class="hljs-attribute">features_shape</span> = (<span class="hljs-number">3</span>, <span class="hljs-number">4</span>)<br><br><span class="hljs-attribute">feature_map</span> = torch.ones(features_shape)  # <span class="hljs-number">2</span>D<br><span class="hljs-attribute">feature_maps</span> = torch.stack([feature_map * (i + <span class="hljs-number">1</span>) for i in range(num_features)], dim=<span class="hljs-number">0</span>)  # <span class="hljs-number">3</span>D<br><span class="hljs-attribute">feature_maps_bs</span> = torch.stack([feature_maps for i in range(batch_size)], dim=<span class="hljs-number">0</span>)  # <span class="hljs-number">4</span>D<br><br><span class="hljs-comment"># feature_maps_bs shape is [8, 6, 3, 4],  B * C * H * W</span><br><span class="hljs-comment"># ln = nn.LayerNorm(feature_maps_bs.size()[1:], elementwise_affine=True)</span><br><span class="hljs-comment"># ln = nn.LayerNorm(feature_maps_bs.size()[1:], elementwise_affine=False)</span><br><span class="hljs-comment"># ln = nn.LayerNorm([6, 3, 4])</span><br><span class="hljs-attribute">ln</span> = nn.LayerNorm([<span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>])<br><br><span class="hljs-attribute">output</span> = ln(feature_maps_bs)<br><br><span class="hljs-attribute">print</span>(<span class="hljs-string">&quot;Layer Normalization&quot;</span>)<br><span class="hljs-attribute">print</span>(ln.weight.shape)<br><span class="hljs-attribute">print</span>(feature_maps_bs[<span class="hljs-number">0</span>, ...])<br><span class="hljs-attribute">print</span>(output[<span class="hljs-number">0</span>, ...])<br></code></pre></td></tr></table></figure><figure class="highlight lua"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs lua">Layer Normalization<br>torch.Size([<span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>])<br>tensor(<span class="hljs-string">[[[1., 1., 1., 1.],</span><br><span class="hljs-string">         [1., 1., 1., 1.],</span><br><span class="hljs-string">         [1., 1., 1., 1.]]</span>,<br>        <span class="hljs-string">[[2., 2., 2., 2.],</span><br><span class="hljs-string">         [2., 2., 2., 2.],</span><br><span class="hljs-string">         [2., 2., 2., 2.]]</span>])<br>tensor(<span class="hljs-string">[[[-1.0000, -1.0000, -1.0000, -1.0000],</span><br><span class="hljs-string">         [-1.0000, -1.0000, -1.0000, -1.0000],</span><br><span class="hljs-string">         [-1.0000, -1.0000, -1.0000, -1.0000]]</span>,<br>        <span class="hljs-string">[[ 1.0000,  1.0000,  1.0000,  1.0000],</span><br><span class="hljs-string">         [ 1.0000,  1.0000,  1.0000,  1.0000],</span><br><span class="hljs-string">         [ 1.0000,  1.0000,  1.0000,  1.0000]]</span>], grad_fn=&lt;SelectBackward&gt;)<br></code></pre></td></tr></table></figure><p>Layer Normalization 可以设置 normalized_shape 为 (3, 4) 或者 (4)。</p><h2 id="Instance-Normalization"><a href="#Instance-Normalization" class="headerlink" title="Instance Normalization"></a>Instance Normalization</h2><p>提出的原因：Batch Normalization 不适用于图像生成。因为在一个 mini-batch 中的图像有不同的风格，不能把这个 batch 里的数据都看作是同一类取标准化。</p><p>思路：逐个 instance 的 channel 计算均值和方差。也就是每个 feature map 计算一个均值和方差。</p><p>包括 InstanceNorm1d、InstanceNorm2d、InstanceNorm3d。</p><p>以<code>InstanceNorm1d</code>为例，定义如下：</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs routeros">torch.nn.InstanceNorm1d(num_features, <span class="hljs-attribute">eps</span>=1e-05, <span class="hljs-attribute">momentum</span>=0.1, <span class="hljs-attribute">affine</span>=<span class="hljs-literal">False</span>, <span class="hljs-attribute">track_running_stats</span>=<span class="hljs-literal">False</span>)<br></code></pre></td></tr></table></figure><p>参数：</p><ul><li>num_features：一个样本的特征数，这个参数最重要</li><li>eps：分母修正项</li><li>momentum：指数加权平均估计当前的的均值和方差</li><li>affine：是否需要 affine transform</li><li>track_running_stats：True 为训练状态，此时均值和方差会根据每个 mini-batch 改变。False 为测试状态，此时均值和方差会固定</li></ul><p>下面代码中，输入数据的形状是 $B \times C \times 2D_feature$，(3, 3, 2, 2)，表示一个 mini-batch 有 3 个样本，每个样本有 3 个特征，每个特征的维度是 $2 \times 2 $。那么就会计算 $3 \times 3 $ 个均值和方差，分别对应每个样本的每个特征。如下图所示：</p><p><img src="/img/pytorchtrain6/18.png"><br> 下面是代码：</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs routeros">batch_size = 3<br>num_features = 3<br>momentum = 0.3<br><br>features_shape = (2, 2)<br><br>feature_map = torch.ones(features_shape)    # 2D<br>feature_maps = torch.stack([feature_map * (i + 1) <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(num_features)], <span class="hljs-attribute">dim</span>=0)  # 3D<br>feature_maps_bs = torch.stack([feature_maps <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(batch_size)], <span class="hljs-attribute">dim</span>=0)  # 4D<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Instance Normalization&quot;</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;input data:\n&#123;&#125; shape is &#123;&#125;&quot;</span>.format(feature_maps_bs, feature_maps_bs.shape))<br><br>instance_n = nn.InstanceNorm2d(<span class="hljs-attribute">num_features</span>=num_features, <span class="hljs-attribute">momentum</span>=momentum)<br><br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(1):<br>    outputs = instance_n(feature_maps_bs)<br><br>    <span class="hljs-built_in">print</span>(outputs)<br></code></pre></td></tr></table></figure><p>输出如下：</p><figure class="highlight inform7"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><code class="hljs inform7">Instance Normalization<br>input data:<br>tensor(<span class="hljs-comment">[<span class="hljs-comment">[<span class="hljs-comment">[<span class="hljs-comment">[1., 1.]</span>,</span></span></span><br><span class="hljs-comment"><span class="hljs-comment"><span class="hljs-comment">          <span class="hljs-comment">[1., 1.]</span>]</span>,</span></span><br><span class="hljs-comment"><span class="hljs-comment">         <span class="hljs-comment">[<span class="hljs-comment">[2., 2.]</span>,</span></span></span><br><span class="hljs-comment"><span class="hljs-comment"><span class="hljs-comment">          <span class="hljs-comment">[2., 2.]</span>]</span>,</span></span><br><span class="hljs-comment"><span class="hljs-comment">         <span class="hljs-comment">[<span class="hljs-comment">[3., 3.]</span>,</span></span></span><br><span class="hljs-comment"><span class="hljs-comment"><span class="hljs-comment">          <span class="hljs-comment">[3., 3.]</span>]</span>]</span>,</span><br><span class="hljs-comment">        <span class="hljs-comment">[<span class="hljs-comment">[<span class="hljs-comment">[1., 1.]</span>,</span></span></span><br><span class="hljs-comment"><span class="hljs-comment"><span class="hljs-comment">          <span class="hljs-comment">[1., 1.]</span>]</span>,</span></span><br><span class="hljs-comment"><span class="hljs-comment">         <span class="hljs-comment">[<span class="hljs-comment">[2., 2.]</span>,</span></span></span><br><span class="hljs-comment"><span class="hljs-comment"><span class="hljs-comment">          <span class="hljs-comment">[2., 2.]</span>]</span>,</span></span><br><span class="hljs-comment"><span class="hljs-comment">         <span class="hljs-comment">[<span class="hljs-comment">[3., 3.]</span>,</span></span></span><br><span class="hljs-comment"><span class="hljs-comment"><span class="hljs-comment">          <span class="hljs-comment">[3., 3.]</span>]</span>]</span>,</span><br><span class="hljs-comment">        <span class="hljs-comment">[<span class="hljs-comment">[<span class="hljs-comment">[1., 1.]</span>,</span></span></span><br><span class="hljs-comment"><span class="hljs-comment"><span class="hljs-comment">          <span class="hljs-comment">[1., 1.]</span>]</span>,</span></span><br><span class="hljs-comment"><span class="hljs-comment">         <span class="hljs-comment">[<span class="hljs-comment">[2., 2.]</span>,</span></span></span><br><span class="hljs-comment"><span class="hljs-comment"><span class="hljs-comment">          <span class="hljs-comment">[2., 2.]</span>]</span>,</span></span><br><span class="hljs-comment"><span class="hljs-comment">         <span class="hljs-comment">[<span class="hljs-comment">[3., 3.]</span>,</span></span></span><br><span class="hljs-comment"><span class="hljs-comment"><span class="hljs-comment">          <span class="hljs-comment">[3., 3.]</span>]</span>]</span>]</span>) shape <span class="hljs-keyword">is</span> torch.Size(<span class="hljs-comment">[3, 3, 2, 2]</span>)<br>tensor(<span class="hljs-comment">[<span class="hljs-comment">[<span class="hljs-comment">[<span class="hljs-comment">[0., 0.]</span>,</span></span></span><br><span class="hljs-comment"><span class="hljs-comment"><span class="hljs-comment">          <span class="hljs-comment">[0., 0.]</span>]</span>,</span></span><br><span class="hljs-comment"><span class="hljs-comment">         <span class="hljs-comment">[<span class="hljs-comment">[0., 0.]</span>,</span></span></span><br><span class="hljs-comment"><span class="hljs-comment"><span class="hljs-comment">          <span class="hljs-comment">[0., 0.]</span>]</span>,</span></span><br><span class="hljs-comment"><span class="hljs-comment">         <span class="hljs-comment">[<span class="hljs-comment">[0., 0.]</span>,</span></span></span><br><span class="hljs-comment"><span class="hljs-comment"><span class="hljs-comment">          <span class="hljs-comment">[0., 0.]</span>]</span>]</span>,</span><br><span class="hljs-comment">        <span class="hljs-comment">[<span class="hljs-comment">[<span class="hljs-comment">[0., 0.]</span>,</span></span></span><br><span class="hljs-comment"><span class="hljs-comment"><span class="hljs-comment">          <span class="hljs-comment">[0., 0.]</span>]</span>,</span></span><br><span class="hljs-comment"><span class="hljs-comment">         <span class="hljs-comment">[<span class="hljs-comment">[0., 0.]</span>,</span></span></span><br><span class="hljs-comment"><span class="hljs-comment"><span class="hljs-comment">          <span class="hljs-comment">[0., 0.]</span>]</span>,</span></span><br><span class="hljs-comment"><span class="hljs-comment">         <span class="hljs-comment">[<span class="hljs-comment">[0., 0.]</span>,</span></span></span><br><span class="hljs-comment"><span class="hljs-comment"><span class="hljs-comment">          <span class="hljs-comment">[0., 0.]</span>]</span>]</span>,</span><br><span class="hljs-comment">        <span class="hljs-comment">[<span class="hljs-comment">[<span class="hljs-comment">[0., 0.]</span>,</span></span></span><br><span class="hljs-comment"><span class="hljs-comment"><span class="hljs-comment">          <span class="hljs-comment">[0., 0.]</span>]</span>,</span></span><br><span class="hljs-comment"><span class="hljs-comment">         <span class="hljs-comment">[<span class="hljs-comment">[0., 0.]</span>,</span></span></span><br><span class="hljs-comment"><span class="hljs-comment"><span class="hljs-comment">          <span class="hljs-comment">[0., 0.]</span>]</span>,</span></span><br><span class="hljs-comment"><span class="hljs-comment">         <span class="hljs-comment">[<span class="hljs-comment">[0., 0.]</span>,</span></span></span><br><span class="hljs-comment"><span class="hljs-comment"><span class="hljs-comment">          <span class="hljs-comment">[0., 0.]</span>]</span>]</span>]</span>)<br></code></pre></td></tr></table></figure><h2 id="Group-Normalization"><a href="#Group-Normalization" class="headerlink" title="Group Normalization"></a>Group Normalization</h2><p>提出的原因：在小 batch 的样本中，Batch Normalization 估计的值不准。一般用在很大的模型中，这时 batch size 就很小。</p><p>思路：数据不够，通道来凑。 每个样本的特征分为几组，每组特征分别计算均值和方差。可以看作是 Layer Normalization 的基础上添加了特征分组。</p><p>注意事项：</p><ul><li>不再有 running_mean 和 running_var</li><li>$\gamma$ 和 $\beta$ 为逐通道的</li></ul><p>定义如下：</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs routeros">torch.nn.GroupNorm(num_groups, num_channels, <span class="hljs-attribute">eps</span>=1e-05, <span class="hljs-attribute">affine</span>=<span class="hljs-literal">True</span>)<br></code></pre></td></tr></table></figure><p>参数：</p><ul><li>num_groups：特征的分组数量</li><li>num_channels：特征数，通道数。注意 num_channels 要可以整除 num_groups</li><li>eps：分母修正项</li><li>affine：是否需要 affine transform</li></ul><p>下面代码中，输入数据的形状是 $B \times C \times 2D_feature$，(2, 4, 3, 3)，表示一个 mini-batch 有 2 个样本，每个样本有 4 个特征，每个特征的维度是 $3 \times 3 $。num_groups 设置为 2，那么就会计算 $2 \times (4 \div 2) $ 个均值和方差，分别对应每个样本的每个特征。</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">batch_size</span> = <span class="hljs-number">2</span><br> <span class="hljs-attribute">num_features</span> = <span class="hljs-number">4</span><br> <span class="hljs-attribute">num_groups</span> = <span class="hljs-number">2</span>   <br> <span class="hljs-attribute">features_shape</span> = (<span class="hljs-number">2</span>, <span class="hljs-number">2</span>)<br><br> <span class="hljs-attribute">feature_map</span> = torch.ones(features_shape)    # <span class="hljs-number">2</span>D<br> <span class="hljs-attribute">feature_maps</span> = torch.stack([feature_map * (i + <span class="hljs-number">1</span>) for i in range(num_features)], dim=<span class="hljs-number">0</span>)  # <span class="hljs-number">3</span>D<br> <span class="hljs-attribute">feature_maps_bs</span> = torch.stack([feature_maps * (i + <span class="hljs-number">1</span>) for i in range(batch_size)], dim=<span class="hljs-number">0</span>)  # <span class="hljs-number">4</span>D<br><br> <span class="hljs-attribute">gn</span> = nn.GroupNorm(num_groups, num_features)<br> <span class="hljs-attribute">outputs</span> = gn(feature_maps_bs)<br><br> <span class="hljs-attribute">print</span>(<span class="hljs-string">&quot;Group Normalization&quot;</span>)<br> <span class="hljs-attribute">print</span>(gn.weight.shape)<br> <span class="hljs-attribute">print</span>(outputs[<span class="hljs-number">0</span>])<br></code></pre></td></tr></table></figure><p>输出如下：</p><figure class="highlight lua"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs lua">Group Normalization<br>torch.Size([<span class="hljs-number">4</span>])<br>tensor(<span class="hljs-string">[[[-1.0000, -1.0000],</span><br><span class="hljs-string">         [-1.0000, -1.0000]]</span>,<br>        <span class="hljs-string">[[ 1.0000,  1.0000],</span><br><span class="hljs-string">         [ 1.0000,  1.0000]]</span>,<br>        <span class="hljs-string">[[-1.0000, -1.0000],</span><br><span class="hljs-string">         [-1.0000, -1.0000]]</span>,<br>        <span class="hljs-string">[[ 1.0000,  1.0000],</span><br><span class="hljs-string">         [ 1.0000,  1.0000]]</span>], grad_fn=&lt;SelectBackward&gt;)<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>深度学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>pytorch</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>深度学习-12-pytorch-5-可视化与 Hook</title>
    <link href="/2021/08/12/pytorchtrain5/"/>
    <url>/2021/08/12/pytorchtrain5/</url>
    
    <content type="html"><![CDATA[<blockquote><p>创作声明：主要内容参考于张贤同学<a href="https://zhuanlan.zhihu.com/p/265394674">https://zhuanlan.zhihu.com/p/265394674</a></p></blockquote><h2 id="Tensorboard"><a href="#Tensorboard" class="headerlink" title="Tensorboard"></a>Tensorboard</h2><h3 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h3><p>原本是tensorflow的可视化工具，pytorch从1.2.0开始支持tensorboard。之前的版本也可以使用tensorboardX代替。</p><p>在使用1.2.0版本以上的PyTorch的情况下，直接使用pip安装即可。</p><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs cmake">pip <span class="hljs-keyword">install</span> tensorboard<br></code></pre></td></tr></table></figure><p>这样直接安装之后，有可能打开的tensorboard网页是全白的，如果有这种问题，解决方法是卸载之后安装更低版本</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">tensorboard</span>。pip uninstall tensorboard<br><span class="hljs-attribute">pip</span> install tensorboard==<span class="hljs-number">2</span>.<span class="hljs-number">0</span>.<span class="hljs-number">2</span><br></code></pre></td></tr></table></figure><h3 id="Tensorboard的使用逻辑"><a href="#Tensorboard的使用逻辑" class="headerlink" title="Tensorboard的使用逻辑"></a>Tensorboard的使用逻辑</h3><p>Tensorboard的工作流程简单来说是</p><ul><li>将代码运行过程中的某些你关心的数据保存在一个文件夹中：<br>这一步由代码中的writer完成 </li><li>再读取这个文件夹中的数据，用浏览器显示出来：<br>这一步通过在命令行运行tensorboard完成。</li></ul><h3 id="代码体中要做的事"><a href="#代码体中要做的事" class="headerlink" title="代码体中要做的事"></a>代码体中要做的事</h3><p>首先导入tensorboard</p><figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs angelscript"><span class="hljs-keyword">from</span> torch.utils.tensorboard <span class="hljs-keyword">import</span> SummaryWriter   <br></code></pre></td></tr></table></figure><p>这里的SummaryWriter的作用就是，将数据以特定的格式存储到刚刚提到的那个文件夹中。</p><p>首先我们将其实例化</p><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs ini"><span class="hljs-attr">writer</span> = SummaryWriter(<span class="hljs-string">&#x27;./path/to/log&#x27;</span>)<br></code></pre></td></tr></table></figure><p>这里传入的参数就是指向文件夹的路径，之后我们使用这个writer对象“拿出来”的任何数据都保存在这个路径之下。</p><p>这个对象包含多个方法，比如针对数值，我们可以调用</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs routeros">writer.add_scalar(tag, scalar_value, <span class="hljs-attribute">global_step</span>=None, <span class="hljs-attribute">walltime</span>=None)<br></code></pre></td></tr></table></figure><p>这里的tag指定可视化时这个变量的名字，scalar_value是你要存的值，global_step可以理解为x轴坐标。</p><p>举一个简单的例子：</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs stylus"><span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">100</span>)<br>    mAP = <span class="hljs-built_in">eval</span>(model)<br>    writer<span class="hljs-selector-class">.add_scalar</span>(<span class="hljs-string">&#x27;mAP&#x27;</span>, mAP, epoch)<br></code></pre></td></tr></table></figure><p>这样就会生成一个x轴跨度为100的折线图，y轴坐标代表着每一个epoch的mAP。这个折线图会保存在指定的路径下（但是现在还看不到）</p><p>同理，除了数值，我们可能还会想看到模型训练过程中的图像。 </p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs routeros">writer.add_image(tag, img_tensor, <span class="hljs-attribute">global_step</span>=None, <span class="hljs-attribute">walltime</span>=None, <span class="hljs-attribute">dataformats</span>=<span class="hljs-string">&#x27;CHW&#x27;</span>)<br>writer.add_images(tag, img_tensor, <span class="hljs-attribute">global_step</span>=None, <span class="hljs-attribute">walltime</span>=None, <span class="hljs-attribute">dataformats</span>=<span class="hljs-string">&#x27;NCHW&#x27;</span>)<br></code></pre></td></tr></table></figure><h3 id="可视化"><a href="#可视化" class="headerlink" title="可视化"></a>可视化</h3><p>我们已经将关心的数据拿出来了，接下来我们只需要在命令行运行：</p><figure class="highlight jboss-cli"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs jboss-cli">tensorboard <span class="hljs-params">--logdir=</span><span class="hljs-string">./path/to/the/folder</span> <span class="hljs-params">--port</span> 8123<br></code></pre></td></tr></table></figure><p>然后打开浏览器，访问地址<a href="http://localhost:8123/">http://localhost:8123/</a>  即可。这里的8123只是随便一个例子，用其他的未被占用端口也没有任何问题，注意命令行的端口与浏览器访问的地址同步。</p><p>如果发现不显示数据，注意检查一下路径是否正确，命令行这里注意是</p><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs awk">--logdir=.<span class="hljs-regexp">/path/</span>to<span class="hljs-regexp">/the/</span>folder<br></code></pre></td></tr></table></figure><p>而不是</p><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs ini"><span class="hljs-attr">--logdir</span>= <span class="hljs-string">&#x27;./path/to/the/folder &#x27;</span><br></code></pre></td></tr></table></figure><p>另一点要注意的是tensorboard并不是实时显示（visdom是完全实时的），而是默认30秒刷新一次。</p><h3 id="细节"><a href="#细节" class="headerlink" title="细节"></a>细节</h3><h4 id="变量归类"><a href="#变量归类" class="headerlink" title="变量归类"></a>变量归类</h4><p>命名变量的时候可以使用形如</p><figure class="highlight sml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs sml">writer.add_scalar(<span class="hljs-symbol">&#x27;loss</span>/loss1&#x27;, loss1, epoch)<br>writer.add_scalar(<span class="hljs-symbol">&#x27;loss</span>/loss2&#x27;, loss2, epoch)<br>writer.add_scalar(<span class="hljs-symbol">&#x27;loss</span>/loss3&#x27;, loss3, epoch)<br></code></pre></td></tr></table></figure><p>的格式，这样3个loss就会被显示在同一个section。</p><h4 id="同时显示多个折线图"><a href="#同时显示多个折线图" class="headerlink" title="同时显示多个折线图"></a>同时显示多个折线图</h4><p>假如使用了两种学习率去训练同一个网络，想要比较它们训练过程中的loss曲线，只需要将两个日志文件夹放到同一目录下，并在命令行运行</p><figure class="highlight jboss-cli"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs jboss-cli">tensorboard <span class="hljs-params">--logdir=</span><span class="hljs-string">./path/to/the/root</span> <span class="hljs-params">--port</span> 8123<br></code></pre></td></tr></table></figure><h2 id="Hook-函数"><a href="#Hook-函数" class="headerlink" title="Hook 函数"></a>Hook 函数</h2><p>Hook 函数是在不改变主体的情况下，实现额外功能。由于 PyTorch 是基于动态图实现的，因此在一次迭代运算结束后，一些中间变量如非叶子节点的梯度和特征图，会被释放掉。在这种情况下想要提取和记录这些中间变量，就需要使用 Hook 函数。</p><p>PyTorch 提供了 4 种 Hook 函数。</p><h3 id="torch-Tensor-register-hook-hook"><a href="#torch-Tensor-register-hook-hook" class="headerlink" title="torch.Tensor.register_hook(hook)"></a>torch.Tensor.register_hook(hook)</h3><p>功能：注册一个反向传播 hook 函数，仅输入一个参数，为张量的梯度。</p><p><code>hook</code>函数：</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs stylus"><span class="hljs-function"><span class="hljs-title">hook</span><span class="hljs-params">(grad)</span></span><br></code></pre></td></tr></table></figure><p>参数：</p><ul><li>grad：张量的梯度</li></ul><p>代码如下：</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs routeros">w = torch.tensor([1.], <span class="hljs-attribute">requires_grad</span>=<span class="hljs-literal">True</span>)<br>x = torch.tensor([2.], <span class="hljs-attribute">requires_grad</span>=<span class="hljs-literal">True</span>)<br>a = torch.<span class="hljs-built_in">add</span>(w, x)<br>b = torch.<span class="hljs-built_in">add</span>(w, 1)<br>y = torch.mul(a, b)<br><br><span class="hljs-comment"># 保存梯度的 list</span><br>a_grad = list()<br><br><span class="hljs-comment"># 定义 hook 函数，把梯度添加到 list 中</span><br>def grad_hook(grad):<br>    a_grad.append(grad)<br><br><span class="hljs-comment"># 一个张量注册 hook 函数</span><br>handle = a.register_hook(grad_hook)<br><br>y.backward()<br><br><span class="hljs-comment"># 查看梯度</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;gradient:&quot;</span>, w.grad, x.grad, a.grad, b.grad, y.grad)<br><span class="hljs-comment"># 查看在 hook 函数里 list 记录的梯度</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;a_grad[0]: &quot;</span>, a_grad[0])<br>handle.<span class="hljs-built_in">remove</span>()<br></code></pre></td></tr></table></figure><p>结果如下：</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs stylus">gradient: <span class="hljs-built_in">tensor</span>(<span class="hljs-selector-attr">[5.]</span>) <span class="hljs-built_in">tensor</span>(<span class="hljs-selector-attr">[2.]</span>) None None None<br>a_grad<span class="hljs-selector-attr">[0]</span>:  <span class="hljs-built_in">tensor</span>(<span class="hljs-selector-attr">[2.]</span>)<br></code></pre></td></tr></table></figure><p>在反向传播结束后，非叶子节点张量的梯度被清空了。而通过<code>hook</code>函数记录的梯度仍然可以查看。</p><p><code>hook</code>函数里面可以修改梯度的值，无需返回也可以作为新的梯度赋值给原来的梯度。代码如下：</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs routeros">w = torch.tensor([1.], <span class="hljs-attribute">requires_grad</span>=<span class="hljs-literal">True</span>)<br>x = torch.tensor([2.], <span class="hljs-attribute">requires_grad</span>=<span class="hljs-literal">True</span>)<br>a = torch.<span class="hljs-built_in">add</span>(w, x)<br>b = torch.<span class="hljs-built_in">add</span>(w, 1)<br>y = torch.mul(a, b)<br><br>a_grad = list()<br><br>def grad_hook(grad):<br>    grad *= 2<br>    return grad<span class="hljs-number">*3</span><br><br>handle = w.register_hook(grad_hook)<br><br>y.backward()<br><br><span class="hljs-comment"># 查看梯度</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;w.grad: &quot;</span>, w.grad)<br>handle.<span class="hljs-built_in">remove</span>()<br></code></pre></td></tr></table></figure><p>结果是：</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs stylus">w<span class="hljs-selector-class">.grad</span>:  <span class="hljs-built_in">tensor</span>(<span class="hljs-selector-attr">[30.]</span>)<br></code></pre></td></tr></table></figure><h3 id="torch-nn-Module-register-forward-hook-hook"><a href="#torch-nn-Module-register-forward-hook-hook" class="headerlink" title="torch.nn.Module.register_forward_hook(hook)"></a>torch.nn.Module.register_forward_hook(hook)</h3><p>功能：注册 module 的前向传播<code>hook</code>函数，可用于获取中间的 feature map。</p><p><code>hook</code>函数：</p><figure class="highlight lua"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs lua">hook(<span class="hljs-built_in">module</span>, <span class="hljs-built_in">input</span>, <span class="hljs-built_in">output</span>)<br></code></pre></td></tr></table></figure><p>参数：</p><ul><li>module：当前网络层</li><li>input：当前网络层输入数据</li><li>output：当前网络层输出数据</li></ul><p>下面代码执行的功能是 $3 \times 3$ 的卷积和 $2 \times 2$ 的池化。我们使用<code>register_forward_hook()</code>记录中间卷积层输入和输出的 feature map。</p><p><img src="/img/pytorchtrain5/1.png"></p><figure class="highlight scss"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><code class="hljs scss">class <span class="hljs-built_in">Net</span>(nn.Module):<br>    def <span class="hljs-built_in">__init__</span>(self):<br>        <span class="hljs-built_in">super</span>(Net, self).<span class="hljs-built_in">__init__</span>()<br>        self.conv1 = nn.<span class="hljs-built_in">Conv2d</span>(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>)<br>        self.pool1 = nn.<span class="hljs-built_in">MaxPool2d</span>(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>)<br><br>    def <span class="hljs-built_in">forward</span>(self, x):<br>        x = self.<span class="hljs-built_in">conv1</span>(x)<br>        x = self.<span class="hljs-built_in">pool1</span>(x)<br>        return x<br><br>def <span class="hljs-built_in">forward_hook</span>(module, data_input, data_output):<br>    fmap_block.<span class="hljs-built_in">append</span>(data_output)<br>    input_block.<span class="hljs-built_in">append</span>(data_input)<br><br># 初始化网络<br>net = <span class="hljs-built_in">Net</span>()<br>net.conv1.weight[<span class="hljs-number">0</span>].<span class="hljs-built_in">detach</span>().<span class="hljs-built_in">fill_</span>(<span class="hljs-number">1</span>)<br>net.conv1.weight[<span class="hljs-number">1</span>].<span class="hljs-built_in">detach</span>().<span class="hljs-built_in">fill_</span>(<span class="hljs-number">2</span>)<br>net.conv1.bias.data.<span class="hljs-built_in">detach</span>().<span class="hljs-built_in">zero_</span>()<br><br># 注册hook<br>fmap_block = <span class="hljs-built_in">list</span>()<br>input_block = <span class="hljs-built_in">list</span>()<br>net.conv1.<span class="hljs-built_in">register_forward_hook</span>(forward_hook)<br><br># inference<br>fake_img = torch.<span class="hljs-built_in">ones</span>((<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">4</span>, <span class="hljs-number">4</span>))   # batch size * channel * H * W<br>output = <span class="hljs-built_in">net</span>(fake_img)<br><br><br># 观察<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;output shape: &#123;&#125;\noutput value: &#123;&#125;\n&quot;</span>.<span class="hljs-built_in">format</span>(output.shape, output))<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;feature maps shape: &#123;&#125;\noutput value: &#123;&#125;\n&quot;</span>.<span class="hljs-built_in">format</span>(fmap_block[<span class="hljs-number">0</span>].shape, fmap_block[<span class="hljs-number">0</span>]))<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;input shape: &#123;&#125;\ninput value: &#123;&#125;&quot;</span>.<span class="hljs-built_in">format</span>(input_block[<span class="hljs-number">0</span>][<span class="hljs-number">0</span>].shape, input_block[<span class="hljs-number">0</span>]))<br></code></pre></td></tr></table></figure><p>输出如下：</p><figure class="highlight lua"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs lua"><span class="hljs-built_in">output</span> shape: torch.Size([<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>])<br><span class="hljs-built_in">output</span> value: tensor(<span class="hljs-string">[[[[ 9.]],</span><br><span class="hljs-string">         [[18.]]]]</span>, grad_fn=&lt;MaxPool2DWithIndicesBackward&gt;)<br>feature maps shape: torch.Size([<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>])<br><span class="hljs-built_in">output</span> value: tensor(<span class="hljs-string">[[[[ 9.,  9.],</span><br><span class="hljs-string">          [ 9.,  9.]],</span><br><span class="hljs-string">         [[18., 18.],</span><br><span class="hljs-string">          [18., 18.]]]]</span>, grad_fn=&lt;ThnnConv2DBackward&gt;)<br><span class="hljs-built_in">input</span> shape: torch.Size([<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">4</span>, <span class="hljs-number">4</span>])<br><span class="hljs-built_in">input</span> value: (tensor(<span class="hljs-string">[[[[1., 1., 1., 1.],</span><br><span class="hljs-string">          [1., 1., 1., 1.],</span><br><span class="hljs-string">          [1., 1., 1., 1.],</span><br><span class="hljs-string">          [1., 1., 1., 1.]]]]</span>),)<br></code></pre></td></tr></table></figure><h3 id="torch-Tensor-register-forward-pre-hook"><a href="#torch-Tensor-register-forward-pre-hook" class="headerlink" title="torch.Tensor.register_forward_pre_hook()"></a>torch.Tensor.register_forward_pre_hook()</h3><p>功能：注册 module 的前向传播前的<code>hook</code>函数，可用于获取输入数据。</p><p><code>hook</code>函数：</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs stylus"><span class="hljs-function"><span class="hljs-title">hook</span><span class="hljs-params">(module, input)</span></span><br></code></pre></td></tr></table></figure><p>参数：</p><ul><li>module：当前网络层</li><li>input：当前网络层输入数据</li></ul><h3 id="torch-Tensor-register-backward-hook"><a href="#torch-Tensor-register-backward-hook" class="headerlink" title="torch.Tensor.register_backward_hook()"></a>torch.Tensor.register_backward_hook()</h3><p>功能：注册 module 的反向传播的<code>hook</code>函数，可用于获取梯度。</p><p><code>hook</code>函数：</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs stylus"><span class="hljs-function"><span class="hljs-title">hook</span><span class="hljs-params">(module, grad_input, grad_output)</span></span><br></code></pre></td></tr></table></figure><p>参数：</p><ul><li>module：当前网络层</li><li>input：当前网络层输入的梯度数据</li><li>output：当前网络层输出的梯度数据</li></ul><p>代码如下：</p><figure class="highlight scss"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><code class="hljs scss">class <span class="hljs-built_in">Net</span>(nn.Module):<br>    def <span class="hljs-built_in">__init__</span>(self):<br>        <span class="hljs-built_in">super</span>(Net, self).<span class="hljs-built_in">__init__</span>()<br>        self.conv1 = nn.<span class="hljs-built_in">Conv2d</span>(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>)<br>        self.pool1 = nn.<span class="hljs-built_in">MaxPool2d</span>(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>)<br><br>    def <span class="hljs-built_in">forward</span>(self, x):<br>        x = self.<span class="hljs-built_in">conv1</span>(x)<br>        x = self.<span class="hljs-built_in">pool1</span>(x)<br>        return x<br><br>def <span class="hljs-built_in">forward_hook</span>(module, data_input, data_output):<br>    fmap_block.<span class="hljs-built_in">append</span>(data_output)<br>    input_block.<span class="hljs-built_in">append</span>(data_input)<br><br>def <span class="hljs-built_in">forward_pre_hook</span>(module, data_input):<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;forward_pre_hook input:&#123;&#125;&quot;</span>.<span class="hljs-built_in">format</span>(data_input))<br><br>def <span class="hljs-built_in">backward_hook</span>(module, grad_input, grad_output):<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;backward hook input:&#123;&#125;&quot;</span>.<span class="hljs-built_in">format</span>(grad_input))<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;backward hook output:&#123;&#125;&quot;</span>.<span class="hljs-built_in">format</span>(grad_output))<br><br># 初始化网络<br>net = <span class="hljs-built_in">Net</span>()<br>net.conv1.weight[<span class="hljs-number">0</span>].<span class="hljs-built_in">detach</span>().<span class="hljs-built_in">fill_</span>(<span class="hljs-number">1</span>)<br>net.conv1.weight[<span class="hljs-number">1</span>].<span class="hljs-built_in">detach</span>().<span class="hljs-built_in">fill_</span>(<span class="hljs-number">2</span>)<br>net.conv1.bias.data.<span class="hljs-built_in">detach</span>().<span class="hljs-built_in">zero_</span>()<br><br># 注册hook<br>fmap_block = <span class="hljs-built_in">list</span>()<br>input_block = <span class="hljs-built_in">list</span>()<br>net.conv1.<span class="hljs-built_in">register_forward_hook</span>(forward_hook)<br>net.conv1.<span class="hljs-built_in">register_forward_pre_hook</span>(forward_pre_hook)<br>net.conv1.<span class="hljs-built_in">register_backward_hook</span>(backward_hook)<br><br># inference<br>fake_img = torch.<span class="hljs-built_in">ones</span>((<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">4</span>, <span class="hljs-number">4</span>))   # batch size * channel * H * W<br>output = <span class="hljs-built_in">net</span>(fake_img)<br><br>loss_fnc = nn.<span class="hljs-built_in">L1Loss</span>()<br>target = torch.<span class="hljs-built_in">randn_like</span>(output)<br>loss = <span class="hljs-built_in">loss_fnc</span>(target, output)<br>loss.<span class="hljs-built_in">backward</span>()<br></code></pre></td></tr></table></figure><p>输出如下：</p><figure class="highlight inform7"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs inform7">forward_pre_hook input:(tensor(<span class="hljs-comment">[<span class="hljs-comment">[<span class="hljs-comment">[<span class="hljs-comment">[1., 1., 1., 1.]</span>,</span></span></span><br><span class="hljs-comment"><span class="hljs-comment"><span class="hljs-comment">          <span class="hljs-comment">[1., 1., 1., 1.]</span>,</span></span></span><br><span class="hljs-comment"><span class="hljs-comment"><span class="hljs-comment">          <span class="hljs-comment">[1., 1., 1., 1.]</span>,</span></span></span><br><span class="hljs-comment"><span class="hljs-comment"><span class="hljs-comment">          <span class="hljs-comment">[1., 1., 1., 1.]</span>]</span>]</span>]</span>),)<br>backward hook input:(None, tensor(<span class="hljs-comment">[<span class="hljs-comment">[<span class="hljs-comment">[<span class="hljs-comment">[0.5000, 0.5000, 0.5000]</span>,</span></span></span><br><span class="hljs-comment"><span class="hljs-comment"><span class="hljs-comment">          <span class="hljs-comment">[0.5000, 0.5000, 0.5000]</span>,</span></span></span><br><span class="hljs-comment"><span class="hljs-comment"><span class="hljs-comment">          <span class="hljs-comment">[0.5000, 0.5000, 0.5000]</span>]</span>]</span>,</span><br><span class="hljs-comment">        <span class="hljs-comment">[<span class="hljs-comment">[<span class="hljs-comment">[0.5000, 0.5000, 0.5000]</span>,</span></span></span><br><span class="hljs-comment"><span class="hljs-comment"><span class="hljs-comment">          <span class="hljs-comment">[0.5000, 0.5000, 0.5000]</span>,</span></span></span><br><span class="hljs-comment"><span class="hljs-comment"><span class="hljs-comment">          <span class="hljs-comment">[0.5000, 0.5000, 0.5000]</span>]</span>]</span>]</span>), tensor(<span class="hljs-comment">[0.5000, 0.5000]</span>))<br>backward hook output:(tensor(<span class="hljs-comment">[<span class="hljs-comment">[<span class="hljs-comment">[<span class="hljs-comment">[0.5000, 0.0000]</span>,</span></span></span><br><span class="hljs-comment"><span class="hljs-comment"><span class="hljs-comment">          <span class="hljs-comment">[0.0000, 0.0000]</span>]</span>,</span></span><br><span class="hljs-comment"><span class="hljs-comment">         <span class="hljs-comment">[<span class="hljs-comment">[0.5000, 0.0000]</span>,</span></span></span><br><span class="hljs-comment"><span class="hljs-comment"><span class="hljs-comment">          <span class="hljs-comment">[0.0000, 0.0000]</span>]</span>]</span>]</span>),)<br></code></pre></td></tr></table></figure><h3 id="hook函数实现机制"><a href="#hook函数实现机制" class="headerlink" title="hook函数实现机制"></a><code>hook</code>函数实现机制</h3><p><code>hook</code>函数实现的原理是在<code>module</code>的<code>__call()__</code>函数进行拦截，<code>__call()__</code>函数可以分为 4 个部分：</p><ul><li>第 1 部分是实现 _forward_pre_hooks</li><li>第 2 部分是实现 forward 前向传播 </li><li>第 3 部分是实现 _forward_hooks  </li><li>第 4 部分是实现 _backward_hooks</li></ul><p>由于卷积层也是一个<code>module</code>，因此可以记录<code>_forward_hooks</code>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">__call__</span>(<span class="hljs-params">self, *<span class="hljs-built_in">input</span>, **kwargs</span>):<br>    <span class="hljs-comment"># 第 1 部分是实现 _forward_pre_hooks</span><br>    <span class="hljs-keyword">for</span> hook <span class="hljs-keyword">in</span> <span class="hljs-variable language_">self</span>._forward_pre_hooks.values():<br>        result = hook(<span class="hljs-variable language_">self</span>, <span class="hljs-built_in">input</span>)<br>        <span class="hljs-keyword">if</span> result <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>            <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> <span class="hljs-built_in">isinstance</span>(result, <span class="hljs-built_in">tuple</span>):<br>                result = (result,)<br>            <span class="hljs-built_in">input</span> = result<br><br>    <span class="hljs-comment"># 第 2 部分是实现 forward 前向传播       </span><br>    <span class="hljs-keyword">if</span> torch._C._get_tracing_state():<br>        result = <span class="hljs-variable language_">self</span>._slow_forward(*<span class="hljs-built_in">input</span>, **kwargs)<br>    <span class="hljs-keyword">else</span>:<br>        result = <span class="hljs-variable language_">self</span>.forward(*<span class="hljs-built_in">input</span>, **kwargs)<br><br>    <span class="hljs-comment"># 第 3 部分是实现 _forward_hooks   </span><br>    <span class="hljs-keyword">for</span> hook <span class="hljs-keyword">in</span> <span class="hljs-variable language_">self</span>._forward_hooks.values():<br>        hook_result = hook(<span class="hljs-variable language_">self</span>, <span class="hljs-built_in">input</span>, result)<br>        <span class="hljs-keyword">if</span> hook_result <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>            result = hook_result<br><br>    <span class="hljs-comment"># 第 4 部分是实现 _backward_hooks</span><br>    <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(<span class="hljs-variable language_">self</span>._backward_hooks) &gt; <span class="hljs-number">0</span>:<br>        var = result<br>        <span class="hljs-keyword">while</span> <span class="hljs-keyword">not</span> <span class="hljs-built_in">isinstance</span>(var, torch.Tensor):<br>            <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(var, <span class="hljs-built_in">dict</span>):<br>                var = <span class="hljs-built_in">next</span>((v <span class="hljs-keyword">for</span> v <span class="hljs-keyword">in</span> var.values() <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(v, torch.Tensor)))<br>            <span class="hljs-keyword">else</span>:<br>                var = var[<span class="hljs-number">0</span>]<br>        grad_fn = var.grad_fn<br>        <span class="hljs-keyword">if</span> grad_fn <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>            <span class="hljs-keyword">for</span> hook <span class="hljs-keyword">in</span> <span class="hljs-variable language_">self</span>._backward_hooks.values():<br>                wrapper = functools.partial(hook, <span class="hljs-variable language_">self</span>)<br>                functools.update_wrapper(wrapper, hook)<br>                grad_fn.register_hook(wrapper)<br>    <span class="hljs-keyword">return</span> result<br></code></pre></td></tr></table></figure><h2 id="Hook-函数提取网络的特征图"><a href="#Hook-函数提取网络的特征图" class="headerlink" title="Hook 函数提取网络的特征图"></a>Hook 函数提取网络的特征图</h2><p>下面通过<code>hook</code>函数获取 AlexNet 每个卷积层的所有卷积核参数，以形状作为 key，value 对应该层多个卷积核的 list。然后取出每层的第一个卷积核，形状是 [1, in_channle, h, w]，转换为 [in_channle, 1, h, w]，使用 TensorBoard 进行可视化，代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><code class="hljs python">writer = SummaryWriter(comment=<span class="hljs-string">&#x27;test_your_comment&#x27;</span>, filename_suffix=<span class="hljs-string">&quot;_test_your_filename_suffix&quot;</span>)<br><br><span class="hljs-comment"># 数据</span><br>path_img = <span class="hljs-string">&quot;imgs/lena.png&quot;</span>     <span class="hljs-comment"># your path to image</span><br>normMean = [<span class="hljs-number">0.49139968</span>, <span class="hljs-number">0.48215827</span>, <span class="hljs-number">0.44653124</span>]<br>normStd = [<span class="hljs-number">0.24703233</span>, <span class="hljs-number">0.24348505</span>, <span class="hljs-number">0.26158768</span>]<br><br>norm_transform = transforms.Normalize(normMean, normStd)<br>img_transforms = transforms.Compose([<br>    transforms.Resize((<span class="hljs-number">224</span>, <span class="hljs-number">224</span>)),<br>    transforms.ToTensor(),<br>    norm_transform<br>])<br><br>img_pil = Image.<span class="hljs-built_in">open</span>(path_img).convert(<span class="hljs-string">&#x27;RGB&#x27;</span>)<br><span class="hljs-keyword">if</span> img_transforms <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>    img_tensor = img_transforms(img_pil)<br>img_tensor.unsqueeze_(<span class="hljs-number">0</span>)    <span class="hljs-comment"># chw --&gt; bchw</span><br><br><span class="hljs-comment"># 模型</span><br>alexnet = models.alexnet(pretrained=<span class="hljs-literal">True</span>)<br><br><span class="hljs-comment"># 注册hook</span><br>fmap_dict = <span class="hljs-built_in">dict</span>()<br><span class="hljs-keyword">for</span> name, sub_module <span class="hljs-keyword">in</span> alexnet.named_modules():<br><br>    <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(sub_module, nn.Conv2d):<br>        key_name = <span class="hljs-built_in">str</span>(sub_module.weight.shape)<br>        fmap_dict.setdefault(key_name, <span class="hljs-built_in">list</span>())<br>        <span class="hljs-comment"># 由于AlexNet 使用 nn.Sequantial 包装，所以 name 的形式是：features.0  features.1</span><br>        n1, n2 = name.split(<span class="hljs-string">&quot;.&quot;</span>)<br><br>        <span class="hljs-keyword">def</span> <span class="hljs-title function_">hook_func</span>(<span class="hljs-params">m, i, o</span>):<br>            key_name = <span class="hljs-built_in">str</span>(m.weight.shape)<br>            fmap_dict[key_name].append(o)<br><br>        alexnet._modules[n1]._modules[n2].register_forward_hook(hook_func)<br><br><span class="hljs-comment"># forward</span><br>output = alexnet(img_tensor)<br><br><span class="hljs-comment"># add image</span><br><span class="hljs-keyword">for</span> layer_name, fmap_list <span class="hljs-keyword">in</span> fmap_dict.items():<br>    fmap = fmap_list[<span class="hljs-number">0</span>]<span class="hljs-comment"># 取出第一个卷积核的参数</span><br>    fmap.transpose_(<span class="hljs-number">0</span>, <span class="hljs-number">1</span>) <span class="hljs-comment"># 把 BCHW 转换为 CBHW</span><br><br>    nrow = <span class="hljs-built_in">int</span>(np.sqrt(fmap.shape[<span class="hljs-number">0</span>]))<br>    fmap_grid = vutils.make_grid(fmap, normalize=<span class="hljs-literal">True</span>, scale_each=<span class="hljs-literal">True</span>, nrow=nrow)<br>    writer.add_image(<span class="hljs-string">&#x27;feature map in &#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(layer_name), fmap_grid, global_step=<span class="hljs-number">322</span>)<br></code></pre></td></tr></table></figure><p>使用 TensorBoard 进行可视化如下：</p><p><img src="/img/pytorchtrain5/2.png"></p>]]></content>
    
    
    <categories>
      
      <category>深度学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>pytorch</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>深度学习-11-pytorch-4-损失函数与优化器</title>
    <link href="/2021/08/11/pytorchtrain4/"/>
    <url>/2021/08/11/pytorchtrain4/</url>
    
    <content type="html"><![CDATA[<blockquote><p>创作声明：主要内容参考于张贤同学<a href="https://zhuanlan.zhihu.com/p/265394674">https://zhuanlan.zhihu.com/p/265394674</a></p></blockquote><h2 id="权值初始化"><a href="#权值初始化" class="headerlink" title="权值初始化"></a>权值初始化</h2><p>在搭建好网络模型之后，一个重要的步骤就是对网络模型中的权值进行初始化。适当的权值初始化可以加快模型的收敛，而不恰当的权值初始化可能引发梯度消失或者梯度爆炸，最终导致模型无法收敛。下面分 3 部分介绍。第一部分介绍不恰当的权值初始化是如何引发梯度消失与梯度爆炸的，第二部分介绍常用的 Xavier 方法与 Kaiming 方法，第三部分介绍 PyTorch 中的 10 种初始化方法。</p><h4 id="梯度消失与梯度爆炸"><a href="#梯度消失与梯度爆炸" class="headerlink" title="梯度消失与梯度爆炸"></a>梯度消失与梯度爆炸</h4><p>考虑一个 3 层的全连接网络。<br>$H{1}&#x3D;X \times W{1}$，$H{2}&#x3D;H{1} \times W{2}$，$Out&#x3D;H{2} \times W_{3}$<br><img src="/img/pytorchtrain4/1.png"><br>其中第 2 层的权重梯度如下<br><img src="/img/pytorchtrain4/2.png"><br>所以$\Delta \mathrm{W}{2}$依赖于前一层的输出$H{1}$。如果$H{1}$ 趋近于零，那么$\Delta \mathrm{W}{2}$也接近于 0，造成梯度消失。如果$H{1}$ 趋近于无穷大，那么$\Delta \mathrm{W}{2}$也接近于无穷大，造成梯度爆炸。要避免梯度爆炸或者梯度消失，就要严格控制网络层输出的数值范围。<br>下面构建 100 层全连接网络，先不使用非线性激活函数，每层的权重初始化为服从$N(0,1)$的正态分布，输出数据使用随机初始化的数据。</p><figure class="highlight haskell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><code class="hljs haskell"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<br><span class="hljs-title">from</span> common_tools <span class="hljs-keyword">import</span> set_seed<br><br><span class="hljs-title">set_seed</span>(<span class="hljs-number">1</span>)  # 设置随机种子<br><span class="hljs-class"></span><br><span class="hljs-class"></span><br><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-type">MLP</span>(<span class="hljs-title">nn</span>.<span class="hljs-type">Module</span>):</span><br><span class="hljs-class">    def __init__(<span class="hljs-title">self</span>, <span class="hljs-title">neural_num</span>, <span class="hljs-title">layers</span>):</span><br><span class="hljs-class">        super(<span class="hljs-type">MLP</span>, <span class="hljs-title">self</span>).__init__()</span><br><span class="hljs-class">        self.linears = nn.<span class="hljs-type">ModuleList</span>([<span class="hljs-title">nn</span>.<span class="hljs-type">Linear</span>(<span class="hljs-title">neural_num</span>, <span class="hljs-title">neural_num</span>, <span class="hljs-title">bias</span>=<span class="hljs-type">False</span>) for i in range(<span class="hljs-title">layers</span>)])</span><br><span class="hljs-class">        self.neural_num = neural_num</span><br><span class="hljs-class"></span><br><span class="hljs-class">    def forward(<span class="hljs-title">self</span>, <span class="hljs-title">x</span>):</span><br><span class="hljs-class">        for (<span class="hljs-title">i</span>, <span class="hljs-title">linear</span>) in enumerate(<span class="hljs-title">self</span>.<span class="hljs-title">linears</span>):</span><br><span class="hljs-class">            x = linear(<span class="hljs-title">x</span>)</span><br><span class="hljs-class"></span><br><span class="hljs-class"></span><br><span class="hljs-class">        return x</span><br><span class="hljs-class"></span><br><span class="hljs-class">    def initialize(<span class="hljs-title">self</span>):</span><br><span class="hljs-class">        for m in self.modules():</span><br><span class="hljs-class">            # 判断这一层是否为线性层，如果为线性层则初始化权值</span><br><span class="hljs-class">            if isinstance(<span class="hljs-title">m</span>, <span class="hljs-title">nn</span>.<span class="hljs-type">Linear</span>):</span><br><span class="hljs-class">                nn.init.normal_(<span class="hljs-title">m</span>.<span class="hljs-title">weight</span>.<span class="hljs-title">data</span>)    # normal: mean=0, std=1</span><br><span class="hljs-class"></span><br><span class="hljs-class">layer_nums = 100</span><br><span class="hljs-class">neural_nums = 256</span><br><span class="hljs-class">batch_size = 16</span><br><span class="hljs-class"></span><br><span class="hljs-class">net = <span class="hljs-type">MLP</span>(<span class="hljs-title">neural_nums</span>, <span class="hljs-title">layer_nums</span>)</span><br><span class="hljs-class">net.initialize()</span><br><span class="hljs-class"></span><br><span class="hljs-class">inputs = torch.randn((<span class="hljs-title">batch_size</span>, <span class="hljs-title">neural_nums</span>))  # normal: mean=0, std=1</span><br><span class="hljs-class"></span><br><span class="hljs-class">output = net(<span class="hljs-title">inputs</span>)</span><br><span class="hljs-class">print(<span class="hljs-title">output</span>)</span><br></code></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight arcade"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs arcade">tensor([[<span class="hljs-literal">nan</span>, <span class="hljs-literal">nan</span>, <span class="hljs-literal">nan</span>,  ..., <span class="hljs-literal">nan</span>, <span class="hljs-literal">nan</span>, <span class="hljs-literal">nan</span>],<br>        [<span class="hljs-literal">nan</span>, <span class="hljs-literal">nan</span>, <span class="hljs-literal">nan</span>,  ..., <span class="hljs-literal">nan</span>, <span class="hljs-literal">nan</span>, <span class="hljs-literal">nan</span>],<br>        [<span class="hljs-literal">nan</span>, <span class="hljs-literal">nan</span>, <span class="hljs-literal">nan</span>,  ..., <span class="hljs-literal">nan</span>, <span class="hljs-literal">nan</span>, <span class="hljs-literal">nan</span>],<br>        ...,<br>        [<span class="hljs-literal">nan</span>, <span class="hljs-literal">nan</span>, <span class="hljs-literal">nan</span>,  ..., <span class="hljs-literal">nan</span>, <span class="hljs-literal">nan</span>, <span class="hljs-literal">nan</span>],<br>        [<span class="hljs-literal">nan</span>, <span class="hljs-literal">nan</span>, <span class="hljs-literal">nan</span>,  ..., <span class="hljs-literal">nan</span>, <span class="hljs-literal">nan</span>, <span class="hljs-literal">nan</span>],<br>        [<span class="hljs-literal">nan</span>, <span class="hljs-literal">nan</span>, <span class="hljs-literal">nan</span>,  ..., <span class="hljs-literal">nan</span>, <span class="hljs-literal">nan</span>, <span class="hljs-literal">nan</span>]], grad_fn=&lt;MmBackward&gt;)<br></code></pre></td></tr></table></figure><p>也就是数据太大(梯度爆炸)或者太小(梯度消失)了。接下来我们在forward()函数中判断每一次前向传播的输出的标准差是否为 nan，如果是 nan 则停止前向传播。</p><figure class="highlight scss"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs scss">def <span class="hljs-built_in">forward</span>(self, x):<br>    for (i, linear) in <span class="hljs-built_in">enumerate</span>(self.linears):<br>        x = <span class="hljs-built_in">linear</span>(x)<br><br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;layer:&#123;&#125;, std:&#123;&#125;&quot;</span>.<span class="hljs-built_in">format</span>(i, x.<span class="hljs-built_in">std</span>()))<br>        if torch.<span class="hljs-built_in">isnan</span>(x.<span class="hljs-built_in">std</span>()):<br>            <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;output is nan in &#123;&#125; layers&quot;</span>.<span class="hljs-built_in">format</span>(i))<br>            break<br><br>    return x<br></code></pre></td></tr></table></figure><p>输出如下：</p><figure class="highlight avrasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs avrasm"><span class="hljs-symbol">layer:</span><span class="hljs-number">0</span>, <span class="hljs-keyword">std</span>:<span class="hljs-number">15.959932327270508</span><br><span class="hljs-symbol">layer:</span><span class="hljs-number">1</span>, <span class="hljs-keyword">std</span>:<span class="hljs-number">256.6237487792969</span><br><span class="hljs-symbol">layer:</span><span class="hljs-number">2</span>, <span class="hljs-keyword">std</span>:<span class="hljs-number">4107.24560546875</span><br>.<br>.<br>.<br><span class="hljs-symbol">layer:</span><span class="hljs-number">29</span>, <span class="hljs-keyword">std</span>:<span class="hljs-number">1.322983152787379e+36</span><br><span class="hljs-symbol">layer:</span><span class="hljs-number">30</span>, <span class="hljs-keyword">std</span>:<span class="hljs-number">2.0786820453988485e+37</span><br><span class="hljs-symbol">layer:</span><span class="hljs-number">31</span>, <span class="hljs-keyword">std</span>:nan<br>output is nan <span class="hljs-keyword">in</span> <span class="hljs-number">31</span> layers<br></code></pre></td></tr></table></figure><p>可以看到每一层的标准差是越来越大的，并在在 31 层时超出了数据可以表示的范围。<br>下面推导为什么网络层输出的标准差越来越大。<br>首先给出 3 个公式：</p><ul><li>$E(X \times Y)&#x3D;E(X) \times E(Y)$：两个相互独立的随机变量的乘积的期望等于它们的期望的乘积。</li><li>$D(X)&#x3D;E(X^{2}) - [E(X)]^{2}$：一个随机变量的方差等于它的平方的期望减去期望的平方</li><li>$D(X+Y)&#x3D;D(X)+D(Y)$：两个相互独立的随机变量之和的方差等于它们的方差的和。</li></ul><p>可以推导出两个随机变量的乘积的方差如下：</p><p>$D(X \times Y)&#x3D;E[(XY)^{2}] - [E(XY)]^{2}&#x3D;D(X) \times D(Y) + D(X) \times [E(Y)]^{2} + D(Y) \times [E(X)]^{2}$</p><p>如果$E(X)&#x3D;0$，$E(Y)&#x3D;0$，那么$D(X \times Y)&#x3D;D(X) \times D(Y)$</p><p>我们以输入层第一个神经元为例：</p><p>$\mathrm{H}{11}&#x3D;\sum{i&#x3D;0}^{n} X{i} \times W{1 i}$</p><p>其中输入 X 和权值 W 都是服从$N(0,1)$的正态分布，所以这个神经元的方差为：</p><p>$\begin{aligned} \mathbf{D}\left(\mathrm{H}{11}\right) &amp;&#x3D;\sum{i&#x3D;0}^{n} \boldsymbol{D}\left(X{i}\right) * \boldsymbol{D}\left(W{1 i}\right) \ &amp;&#x3D;n (1  1) \ &amp;&#x3D;n \end{aligned}$</p><p>标准差为：</p><p>$\operatorname{std}\left(\mathrm{H}{11}\right)&#x3D;\sqrt{\mathbf{D}\left(\mathrm{H}{11}\right)}&#x3D;\sqrt{n}$</p><p>所以每经过一个网络层，方差就会扩大 n 倍，标准差就会扩大$\sqrt{n}$倍，n 为每层神经元个数，直到超出数值表示范围。对比上面的代码可以看到，每层神经元个数为 256，输出数据的标准差为 1，所以第一个网络层输出的标准差为 16 左右，第二个网络层输出的标准差为 256 左右，以此类推，直到 31 层超出数据表示范围。可以把每层神经元个数改为 400，那么每层标准差扩大 20 倍左右。从$D(\mathrm{H}{11})&#x3D;\sum{i&#x3D;0}^{n} D(X{i}) \times D(W{1 i})$，可以看出，每一层网络输出的方差与神经元个数、输入数据的方差、权值方差有关，其中比较好改变的是权值的方差$D(W)$，所以$D(W)&#x3D; \frac{1}{n}$，标准差为$std(W)&#x3D;\sqrt\frac{1}{n}$。因此修改权值初始化代码为nn.init.normal_(m.weight.data, std&#x3D;np.sqrt(1&#x2F;self.neural_num)),结果如下：</p><figure class="highlight avrasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs avrasm"><span class="hljs-symbol">layer:</span><span class="hljs-number">0</span>, <span class="hljs-keyword">std</span>:<span class="hljs-number">0.9974957704544067</span><br><span class="hljs-symbol">layer:</span><span class="hljs-number">1</span>, <span class="hljs-keyword">std</span>:<span class="hljs-number">1.0024365186691284</span><br><span class="hljs-symbol">layer:</span><span class="hljs-number">2</span>, <span class="hljs-keyword">std</span>:<span class="hljs-number">1.002745509147644</span><br>.<br>.<br>.<br><span class="hljs-symbol">layer:</span><span class="hljs-number">94</span>, <span class="hljs-keyword">std</span>:<span class="hljs-number">1.031973123550415</span><br><span class="hljs-symbol">layer:</span><span class="hljs-number">95</span>, <span class="hljs-keyword">std</span>:<span class="hljs-number">1.0413124561309814</span><br><span class="hljs-symbol">layer:</span><span class="hljs-number">96</span>, <span class="hljs-keyword">std</span>:<span class="hljs-number">1.0817031860351562</span><br></code></pre></td></tr></table></figure><p>修改之后，没有出现梯度消失或者梯度爆炸的情况，每层神经元输出的方差均在 1 左右。通过恰当的权值初始化，可以保持权值在更新过程中维持在一定范围之内，不过过大，也不会过小。</p><p>上述是没有使用非线性变换的实验结果，如果在forward()中添加非线性变换tanh，每一层的输出方差会越来越小，会导致梯度消失。因此出现了 Xavier 初始化方法与 Kaiming 初始化方法。</p><h3 id="Xavier-方法"><a href="#Xavier-方法" class="headerlink" title="Xavier 方法"></a>Xavier 方法</h3><p>Xavier 是 2010 年提出的，针对有非线性激活函数时的权值初始化方法，目标是保持数据的方差维持在 1 左右，主要针对饱和激活函数如 sigmoid 和 tanh 等。同时考虑前向传播和反向传播，需要满足两个等式：$\boldsymbol{n}{\boldsymbol{i}} * \boldsymbol{D}(\boldsymbol{W})&#x3D;\mathbf{1}$和$\boldsymbol{n}{\boldsymbol{i+1}} * \boldsymbol{D}(\boldsymbol{W})&#x3D;\mathbf{1}$，可得：$D(W)&#x3D;\frac{2}{n{i}+n{i+1}}$。</p><p>为了使 Xavier 方法初始化的权值服从均匀分布，假设$W$服从均匀分布$U[-a, a]$，那么方差 $D(W)&#x3D;\frac{(-a-a)^{2}}{12}&#x3D;\frac{(2 a)^{2}}{12}&#x3D;\frac{a^{2}}{3}$，令$\frac{2}{n{i}+n{i+1}}&#x3D;\frac{a^{2}}{3}$，解得：$\boldsymbol{a}&#x3D;\frac{\sqrt{6}}{\sqrt{n{i}+n{i+1}}}$，所以$W$服从分布$U\left[-\frac{\sqrt{6}}{\sqrt{n{i}+n{i+1}}}, \frac{\sqrt{6}}{\sqrt{n{i}+n{i+1}}}\right]$<br>所以初始化方法改为：</p><figure class="highlight livecodeserver"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs livecodeserver"><span class="hljs-keyword">a</span> = np.<span class="hljs-built_in">sqrt</span>(<span class="hljs-number">6</span> / (self.neural_num + self.neural_num))<br><span class="hljs-comment"># 把 a 变换到 tanh，计算增益</span><br>tanh_gain = nn.init.calculate_gain(<span class="hljs-string">&#x27;tanh&#x27;</span>)<br><span class="hljs-keyword">a</span> *= tanh_gain<br><br>nn.init.uniform_(m.weight.data, -<span class="hljs-keyword">a</span>, <span class="hljs-keyword">a</span>)<br></code></pre></td></tr></table></figure><p>并且每一层的激活函数都使用 tanh，输出如下：</p><figure class="highlight avrasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs avrasm"><span class="hljs-symbol">layer:</span><span class="hljs-number">0</span>, <span class="hljs-keyword">std</span>:<span class="hljs-number">0.7571136355400085</span><br><span class="hljs-symbol">layer:</span><span class="hljs-number">1</span>, <span class="hljs-keyword">std</span>:<span class="hljs-number">0.6924336552619934</span><br><span class="hljs-symbol">layer:</span><span class="hljs-number">2</span>, <span class="hljs-keyword">std</span>:<span class="hljs-number">0.6677976846694946</span><br>.<br>.<br>.<br><span class="hljs-symbol">layer:</span><span class="hljs-number">97</span>, <span class="hljs-keyword">std</span>:<span class="hljs-number">0.6426210403442383</span><br><span class="hljs-symbol">layer:</span><span class="hljs-number">98</span>, <span class="hljs-keyword">std</span>:<span class="hljs-number">0.6407480835914612</span><br><span class="hljs-symbol">layer:</span><span class="hljs-number">99</span>, <span class="hljs-keyword">std</span>:<span class="hljs-number">0.6442216038703918</span><br></code></pre></td></tr></table></figure><p>可以看到每层输出的方差都维持在 0.6 左右。</p><p>PyTorch 也提供了 Xavier 初始化方法，可以直接调用：</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs stylus">tanh_gain = nn<span class="hljs-selector-class">.init</span><span class="hljs-selector-class">.calculate_gain</span>(<span class="hljs-string">&#x27;tanh&#x27;</span>)<br>nn<span class="hljs-selector-class">.init</span><span class="hljs-selector-class">.xavier_uniform_</span>(m<span class="hljs-selector-class">.weight</span><span class="hljs-selector-class">.data</span>, gain=tanh_gain)<br></code></pre></td></tr></table></figure><h3 id="nn-init-calculate-gain"><a href="#nn-init-calculate-gain" class="headerlink" title="nn.init.calculate_gain()"></a>nn.init.calculate_gain()</h3><p>上面的初始化方法都使用了<code>tanh_gain = nn.init.calculate_gain(&#39;tanh&#39;)</code>。</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs routeros">nn.init.calculate_gain(nonlinearity,<span class="hljs-attribute">param</span>=**None**)<br></code></pre></td></tr></table></figure><p>主要功能是计算经过一个分布的方差经过激活函数后的变化尺度，主要有两个参数：</p><ul><li>nonlinearity：激活函数名称</li><li>param：激活函数的参数，如 Leaky ReLU 的 negative_slop。</li></ul><p>下面是计算标准差经过激活函数的变化尺度的代码。</p><figure class="highlight sas"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs sas"><span class="hljs-keyword">x</span> = torch.rand<span class="hljs-meta">n</span>(10000) <br><span class="hljs-keyword">out</span> = torch.<span class="hljs-meta">tanh</span>(<span class="hljs-keyword">x</span>)<br>gain = <span class="hljs-keyword">x</span>.<span class="hljs-meta">std</span>() / <span class="hljs-keyword">out</span>.<span class="hljs-meta">std</span>() <br>pr<span class="hljs-meta">int</span>(<span class="hljs-string">&#x27;gain:&#123;&#125;&#x27;</span>.<span class="hljs-keyword">format</span>(gain))<br>tanh_gain = nn.init.calculate_gai<span class="hljs-meta">n</span>(<span class="hljs-string">&#x27;tanh&#x27;</span>) <br>pr<span class="hljs-meta">int</span>(<span class="hljs-string">&#x27;tanh_gain in PyTorch:&#x27;</span>, tanh_gain)<br></code></pre></td></tr></table></figure><p>输出如下：</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">gain</span>:<span class="hljs-number">1</span>.<span class="hljs-number">5982500314712524</span> <br><span class="hljs-attribute">tanh_gain</span> in PyTorch: <span class="hljs-number">1</span>.<span class="hljs-number">6666666666666667</span><br></code></pre></td></tr></table></figure><p>结果表示，原有数据分布的方差经过 tanh 之后，标准差会变小 1.6 倍左右。</p><h3 id="Kaiming-方法"><a href="#Kaiming-方法" class="headerlink" title="Kaiming 方法"></a>Kaiming 方法</h3><p>虽然 Xavier 方法提出了针对饱和激活函数的权值初始化方法，但是 AlexNet 出现后，大量网络开始使用非饱和的激活函数如 ReLU 等，这时 Xavier 方法不再适用。2015 年针对 ReLU 及其变种等激活函数提出了 Kaiming 初始化方法。</p><p>针对 ReLU，方差应该满足：$\mathrm{D}(W)&#x3D;\frac{2}{n{i}}$；针对 ReLu 的变种，方差应该满足：$\mathrm{D}(W)&#x3D;\frac{2}{n{i}}$，a 表示负半轴的斜率，如 PReLU 方法，标准差满足$\operatorname{std}(W)&#x3D;\sqrt{\frac{2}{\left(1+a^{2}\right) * n{i}}}$。</p><p>代码如下：</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs stylus">nn<span class="hljs-selector-class">.init</span><span class="hljs-selector-class">.normal</span>(m<span class="hljs-selector-class">.weight</span><span class="hljs-selector-class">.data</span>, std=np<span class="hljs-selector-class">.sqrt</span>(<span class="hljs-number">2</span> / self.neuralnum))<br></code></pre></td></tr></table></figure><p>或者使用 PyTorch 提供的初始化方法：</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs stylus">nn<span class="hljs-selector-class">.init</span><span class="hljs-selector-class">.kaiming_normal</span>(m<span class="hljs-selector-class">.weight</span>.data)<br></code></pre></td></tr></table></figure><p>同时把激活函数改为 ReLU。</p><h3 id="常用初始化方法"><a href="#常用初始化方法" class="headerlink" title="常用初始化方法"></a>常用初始化方法</h3><p>PyTorch 中提供了 10 种初始化方法</p><ul><li>Xavier 均匀分布</li><li>Xavier 正态分布</li><li>Kaiming 均匀分布</li><li>Kaiming 正态分布</li><li>均匀分布</li><li>正态分布</li><li>常数分布</li><li>正交矩阵初始化</li><li>单位矩阵初始化</li><li>稀疏矩阵初始化</li></ul><p>每种初始化方法都有它自己使用的场景，原则是保持每一层输出的方差不能太大，也不能太小。</p><h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><p>损失函数是衡量模型输出与真实标签之间的差异。我们还经常听到代价函数和目标函数，它们之间差异如下：</p><ul><li><p>损失函数(Loss Function)是计算<strong>一个</strong>样本的模型输出与真实标签的差异</p><p>Loss $&#x3D;f\left(y^{\wedge}, y\right)$</p></li><li><p>代价函数(Cost Function)是计算整个样本集的模型输出与真实标签的差异，是所有样本损失函数的平均值</p><p>$\cos t&#x3D;\frac{1}{N} \sum_{i}^{N} f\left(y_{i}^{\wedge}, y_{i}\right)$</p></li><li><p>目标函数(Objective Function)就是代价函数加上正则项</p></li></ul><p>在 PyTorch 中的损失函数也是继承于<code>nn.Module</code>，所以损失函数也可以看作网络层。</p><p>在逻辑回归的实验中，我使用了交叉熵损失函数<code>loss_fn = nn.BCELoss()</code>，$BCELoss$的继承关系：<code>nn.BCELoss() -&gt; _WeightedLoss -&gt; _Loss -&gt; Module</code>。在计算具体的损失时<code>loss = loss_fn(y_pred.squeeze(), train_y)</code>，这里实际上在 Loss 中进行一次前向传播，最终调用<code>BCELoss()</code>的<code>forward()</code>函数<code>F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)</code>。</p><p>下面介绍 PyTorch 提供的损失函数。注意在所有的损失函数中，<code>size_average</code>和<code>reduce</code>参数都不再使用。</p><h3 id="nn-CrossEntropyLoss"><a href="#nn-CrossEntropyLoss" class="headerlink" title="nn.CrossEntropyLoss"></a>nn.CrossEntropyLoss</h3><p><code>nn.CrossEntropyLoss(weight=None, size_average=None, ignore_index=-100, reduce=None, reduction=&#39;mean&#39;)</code></p><p>功能：把<code>nn.LogSoftmax()</code>和<code>nn.NLLLoss()</code>结合，计算交叉熵。<code>nn.LogSoftmax()</code>的作用是把输出值归一化到了 [0,1] 之间。</p><p>主要参数：</p><ul><li>weight：各类别的 loss 设置权值</li><li>ignore_index：忽略某个类别的 loss 计算</li><li>reduction：计算模式，可以为 none(逐个元素计算)，sum(所有元素求和，返回标量)，mean(加权平均，返回标量)</li></ul><p>下面介绍熵的一些基本概念</p><ul><li>自信息：$\mathrm{I}(x)&#x3D;-\log [p(x)]$</li><li>信息熵就是求自信息的期望：$\mathrm{H}(\mathrm{P})&#x3D;E_{x \sim p}[I(x)]&#x3D;-\sum_{i}^{N} P\left(x_{i}\right) \log P\left(x_{i}\right)$</li><li>相对熵，也被称为 KL 散度，用于衡量两个分布的相似性(距离)：<img src="/img/pytorchtrain4/3.png">其中 P(X)是真实分布， Q(X)是拟合的分布</li><li>交叉熵：$\mathrm{H}(\boldsymbol{P}, \boldsymbol{Q})&#x3D;-\sum_{i&#x3D;1}^{N} \boldsymbol{P}\left(\boldsymbol{x}_{i}\right) \log \boldsymbol{Q}\left(\boldsymbol{x}_{i}\right)$</li></ul><p>相对熵展开可得：<br><img src="/img/pytorchtrain4/4.png"></p><p>所以交叉熵 &#x3D; 信息熵 + 相对熵，即$\mathrm{H}(\boldsymbol{P}, \boldsymbol{Q})&#x3D;\boldsymbol{D}{K \boldsymbol{L}}(\boldsymbol{P}, \boldsymbol{Q})+\mathrm{H}(\boldsymbol{P})$，又由于信息熵$H(P)$是固定的，因此优化交叉熵$H(P,Q)$等价于优化相对熵$D{KL}(P,Q)$。</p><p>所以对于<strong>每一个样本</strong>的 Loss 计算公式为：<br><img src="/img/pytorchtrain4/5.png"></p><p>所以$\operatorname{loss}(x, \text { class })&#x3D;-\log \left(\frac{\exp (x[\text { class }])}{\sum_{j} \exp (x[j])}\right)&#x3D;-x[\text { class }]+\log \left(\sum_{j} \exp (x[j])\right)$。</p><p>如果了解类别的权重，则$\operatorname{loss}(x, \text { class })&#x3D;\operatorname{weight}[\text { class }]\left(-x[\text { class }]+\log \left(\sum_{j} \exp (x[j])\right)\right)$。<br>下面设有 3 个样本做 2 分类。inputs 的形状为 $3 \times 2$，表示每个样本有两个神经元输出两个分类。target 的形状为 $3 \times 1$，注意类别从 0 开始，类型为<code>torch.long</code>。</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs routeros">import torch<br>import torch.nn as nn<br>import torch.nn.functional as F<br>import numpy as np<br><br><span class="hljs-comment"># fake data</span><br>inputs = torch.tensor([[1, 2], [1, 3], [1, 3]], <span class="hljs-attribute">dtype</span>=torch.float)<br>target = torch.tensor([0, 1, 1], <span class="hljs-attribute">dtype</span>=torch.long)<br><br><span class="hljs-comment"># def loss function</span><br>loss_f_none = nn.CrossEntropyLoss(<span class="hljs-attribute">weight</span>=None, <span class="hljs-attribute">reduction</span>=<span class="hljs-string">&#x27;none&#x27;</span>)<br>loss_f_sum = nn.CrossEntropyLoss(<span class="hljs-attribute">weight</span>=None, <span class="hljs-attribute">reduction</span>=<span class="hljs-string">&#x27;sum&#x27;</span>)<br>loss_f_mean = nn.CrossEntropyLoss(<span class="hljs-attribute">weight</span>=None, <span class="hljs-attribute">reduction</span>=<span class="hljs-string">&#x27;mean&#x27;</span>)<br><br><span class="hljs-comment"># forward</span><br>loss_none = loss_f_none(inputs, target)<br>loss_sum = loss_f_sum(inputs, target)<br>loss_mean = loss_f_mean(inputs, target)<br><br><span class="hljs-comment"># view</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Cross Entropy Loss:\n &quot;</span>, loss_none, loss_sum, loss_mean)<br></code></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">Cross</span> Entropy Loss:<br>  <span class="hljs-attribute">tensor</span>([<span class="hljs-number">1</span>.<span class="hljs-number">3133</span>, <span class="hljs-number">0</span>.<span class="hljs-number">1269</span>, <span class="hljs-number">0</span>.<span class="hljs-number">1269</span>]) tensor(<span class="hljs-number">1</span>.<span class="hljs-number">5671</span>) tensor(<span class="hljs-number">0</span>.<span class="hljs-number">5224</span>)<br></code></pre></td></tr></table></figure><p>我们根据单个样本的 loss 计算公式$\operatorname{loss}(x, \text { class })&#x3D;-\log \left(\frac{\exp (x[\text { class }])}{\sum_{j} \exp (x[j])}\right)&#x3D;-x[\text { class }]+\log \left(\sum_{j} \exp (x[j])\right)$，可以使用以下代码来手动计算第一个样本的损失</p><figure class="highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs makefile">idx = 0<br><br>input_1 = inputs.detach().numpy()[idx]      <span class="hljs-comment"># [1, 2]</span><br>target_1 = target.numpy()[idx]              <span class="hljs-comment"># [0]</span><br><br><span class="hljs-comment"># 第一项</span><br>x_class = input_1[target_1]<br><br><span class="hljs-comment"># 第二项</span><br>sigma_exp_x = np.sum(list(map(np.exp, input_1)))<br>log_sigma_exp_x = np.log(sigma_exp_x)<br><br><span class="hljs-comment"># 输出loss</span><br>loss_1 = -x_class + log_sigma_exp_x<br><br><span class="hljs-section">print(&quot;第一个样本loss为: &quot;, loss_1)</span><br></code></pre></td></tr></table></figure><p>结果为：1.3132617</p><p>下面继续看带有类别权重的损失计算，首先设置类别的权重向量<code>weights = torch.tensor([1, 2], dtype=torch.float)</code>，向量的元素个数等于类别的数量，然后在定义损失函数时把<code>weight</code>参数传进去。</p><p>输出为：</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">weights</span>:  tensor([<span class="hljs-number">1</span>., <span class="hljs-number">2</span>.])<br><span class="hljs-attribute">tensor</span>([<span class="hljs-number">1</span>.<span class="hljs-number">3133</span>, <span class="hljs-number">0</span>.<span class="hljs-number">2539</span>, <span class="hljs-number">0</span>.<span class="hljs-number">2539</span>]) tensor(<span class="hljs-number">1</span>.<span class="hljs-number">8210</span>) tensor(<span class="hljs-number">0</span>.<span class="hljs-number">3642</span>)<br></code></pre></td></tr></table></figure><p>权值总和为：$1+2+2&#x3D;5$，所以加权平均的 loss 为：$1.8210\div5&#x3D;0.3642$，通过手动计算的方式代码如下：</p><figure class="highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs makefile">weights = torch.tensor([1, 2], dtype=torch.float)<br>weights_all = np.sum(list(map(lambda x: weights.numpy()[x], target.numpy())))  <span class="hljs-comment"># [0, 1, 1]  # [1 2 2]</span><br>mean = 0<br>loss_f_none = nn.CrossEntropyLoss(reduction=&#x27;none&#x27;)<br>loss_none = loss_f_none(inputs, target)<br>loss_sep = loss_none.detach().numpy()<br>for i in range(target.shape[0]):<br><br>x_class = target.numpy()[i]<br>tmp = loss_sep[i] * (weights.numpy()[x_class] / weights_all)<br>mean += tmp<br><br>print(mean)<br></code></pre></td></tr></table></figure><p>结果为 0.3641947731375694</p><h3 id="nn-NLLLoss"><a href="#nn-NLLLoss" class="headerlink" title="nn.NLLLoss"></a>nn.NLLLoss</h3><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs routeros">nn.NLLLoss(<span class="hljs-attribute">weight</span>=None, <span class="hljs-attribute">size_average</span>=None, <span class="hljs-attribute">ignore_index</span>=-100, <span class="hljs-attribute">reduce</span>=None, <span class="hljs-attribute">reduction</span>=<span class="hljs-string">&#x27;mean&#x27;</span>)<br></code></pre></td></tr></table></figure><p>功能：实现负对数似然函数中的符号功能</p><p>主要参数：</p><ul><li>weight：各类别的 loss 权值设置</li><li>ignore_index：忽略某个类别</li><li>reduction：计算模式，可以为 none(逐个元素计算)，sum(所有元素求和，返回标量)，mean(加权平均，返回标量)</li></ul><p>每个样本的 loss 公式为：$l_{n}&#x3D;-w_{y_{n}} x_{n, y_{n}}$。还是使用上面的例子，第一个样本的输出为 [1,2]，类别为 0，则第一个样本的 loss 为 -1；第一个样本的输出为 [1,3]，类别为 1，则第一个样本的 loss 为 -3。</p><p>代码如下：</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs routeros">weights = torch.tensor([1, 1], <span class="hljs-attribute">dtype</span>=torch.float)<br><br>loss_f_none_w = nn.NLLLoss(<span class="hljs-attribute">weight</span>=weights, <span class="hljs-attribute">reduction</span>=<span class="hljs-string">&#x27;none&#x27;</span>)<br>loss_f_sum = nn.NLLLoss(<span class="hljs-attribute">weight</span>=weights, <span class="hljs-attribute">reduction</span>=<span class="hljs-string">&#x27;sum&#x27;</span>)<br>loss_f_mean = nn.NLLLoss(<span class="hljs-attribute">weight</span>=weights, <span class="hljs-attribute">reduction</span>=<span class="hljs-string">&#x27;mean&#x27;</span>)<br><br><span class="hljs-comment"># forward</span><br>loss_none_w = loss_f_none_w(inputs, target)<br>loss_sum = loss_f_sum(inputs, target)<br>loss_mean = loss_f_mean(inputs, target)<br><br><span class="hljs-comment"># view</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;\nweights: &quot;</span>, weights)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;NLL Loss&quot;</span>, loss_none_w, loss_sum, loss_mean)<br></code></pre></td></tr></table></figure><p>输出如下：</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">weights</span>:  tensor([<span class="hljs-number">1</span>., <span class="hljs-number">1</span>.])<br><span class="hljs-attribute">NLL</span> Loss tensor([-<span class="hljs-number">1</span>., -<span class="hljs-number">3</span>., -<span class="hljs-number">3</span>.]) tensor(-<span class="hljs-number">7</span>.) tensor(-<span class="hljs-number">2</span>.<span class="hljs-number">3333</span>)<br></code></pre></td></tr></table></figure><h3 id="nn-BCELoss"><a href="#nn-BCELoss" class="headerlink" title="nn.BCELoss"></a>nn.BCELoss</h3><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs routeros">nn.BCELoss(<span class="hljs-attribute">weight</span>=None, <span class="hljs-attribute">size_average</span>=None, <span class="hljs-attribute">reduce</span>=None, <span class="hljs-attribute">reduction</span>=<span class="hljs-string">&#x27;mean&#x27;</span>)<br></code></pre></td></tr></table></figure><p>功能：计算二分类的交叉熵。需要注意的是：输出值区间为 [0,1]。</p><p>主要参数：</p><ul><li>weight：各类别的 loss 权值设置</li><li>ignore_index：忽略某个类别</li><li>reduction：计算模式，可以为 none(逐个元素计算)，sum(所有元素求和，返回标量)，mean(加权平均，返回标量)</li></ul><p>计算公式为：$l_{n}&#x3D;-w_{n}\left[y_{n} \cdot \log x_{n}+\left(1-y_{n}\right) \cdot \log \left(1-x_{n}\right)\right]$</p><p>使用这个函数有两个不同的地方：</p><ul><li>预测的标签需要经过 sigmoid 变换到 [0,1] 之间。</li><li>真实的标签需要转换为 one hot 向量，类型为<code>torch.float</code>。</li></ul><p>代码如下：</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs routeros">inputs = torch.tensor([[1, 2], [2, 2], [3, 4], [4, 5]], <span class="hljs-attribute">dtype</span>=torch.float)<br>target = torch.tensor([[1, 0], [1, 0], [0, 1], [0, 1]], <span class="hljs-attribute">dtype</span>=torch.float)<br><br>target_bce = target<br><br><span class="hljs-comment"># itarget</span><br>inputs = torch.sigmoid(inputs)<br><br>weights = torch.tensor([1, 1], <span class="hljs-attribute">dtype</span>=torch.float)<br><br>loss_f_none_w = nn.BCELoss(<span class="hljs-attribute">weight</span>=weights, <span class="hljs-attribute">reduction</span>=<span class="hljs-string">&#x27;none&#x27;</span>)<br>loss_f_sum = nn.BCELoss(<span class="hljs-attribute">weight</span>=weights, <span class="hljs-attribute">reduction</span>=<span class="hljs-string">&#x27;sum&#x27;</span>)<br>loss_f_mean = nn.BCELoss(<span class="hljs-attribute">weight</span>=weights, <span class="hljs-attribute">reduction</span>=<span class="hljs-string">&#x27;mean&#x27;</span>)<br><br><span class="hljs-comment"># forward</span><br>loss_none_w = loss_f_none_w(inputs, target_bce)<br>loss_sum = loss_f_sum(inputs, target_bce)<br>loss_mean = loss_f_mean(inputs, target_bce)<br><br><span class="hljs-comment"># view</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;\nweights: &quot;</span>, weights)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;BCE Loss&quot;</span>, loss_none_w, loss_sum, loss_mean)<br></code></pre></td></tr></table></figure><p>结果为：</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs stylus">BCE Loss <span class="hljs-built_in">tensor</span>(<span class="hljs-selector-attr">[[0.3133, 2.1269]</span>,<br>        <span class="hljs-selector-attr">[0.1269, 2.1269]</span>,<br>        <span class="hljs-selector-attr">[3.0486, 0.0181]</span>,<br>        <span class="hljs-selector-attr">[4.0181, 0.0067]</span>]) <span class="hljs-built_in">tensor</span>(<span class="hljs-number">11.7856</span>) <span class="hljs-built_in">tensor</span>(<span class="hljs-number">1.4732</span>)<br></code></pre></td></tr></table></figure><p>第一个 loss 为 0，3133，手动计算的代码如下：</p><figure class="highlight maxima"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs maxima">x_i = inputs.detach().numpy()[idx, idx]<br>y_i = target.numpy()[idx, idx]              #<br><br># loss<br># l_i = -[ y_i * <span class="hljs-built_in">np</span>.<span class="hljs-built_in">log</span>(x_i) + (<span class="hljs-number">1</span>-y_i) * <span class="hljs-built_in">np</span>.<span class="hljs-built_in">log</span>(<span class="hljs-number">1</span>-y_i) ]      # <span class="hljs-built_in">np</span>.<span class="hljs-built_in">log</span>(<span class="hljs-number">0</span>) = nan<br>l_i = -y_i * <span class="hljs-built_in">np</span>.<span class="hljs-built_in">log</span>(x_i) <span class="hljs-keyword">if</span> y_i <span class="hljs-keyword">else</span> -(<span class="hljs-number">1</span>-y_i) * <span class="hljs-built_in">np</span>.<span class="hljs-built_in">log</span>(<span class="hljs-number">1</span>-x_i)<br></code></pre></td></tr></table></figure><h3 id="nn-BCEWithLogitsLoss"><a href="#nn-BCEWithLogitsLoss" class="headerlink" title="nn.BCEWithLogitsLoss"></a>nn.BCEWithLogitsLoss</h3><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs routeros">nn.BCEWithLogitsLoss(<span class="hljs-attribute">weight</span>=None, <span class="hljs-attribute">size_average</span>=None, <span class="hljs-attribute">reduce</span>=None, <span class="hljs-attribute">reduction</span>=<span class="hljs-string">&#x27;mean&#x27;</span>, <span class="hljs-attribute">pos_weight</span>=None)<br></code></pre></td></tr></table></figure><p>功能：结合 sigmoid 与二分类交叉熵。需要注意的是，网络最后的输出不用经过 sigmoid 函数。这个 loss 出现的原因是有时网络模型最后一层输出不希望是归一化到 [0,1] 之间，但是在计算 loss 时又需要归一化到 [0,1] 之间。</p><p>主要参数：</p><ul><li>weight：各类别的 loss 权值设置</li><li>pos_weight：<strong>设置样本类别对应的神经元的输出的 loss 权值</strong></li><li>ignore_index：忽略某个类别</li><li>reduction：计算模式，可以为 none(逐个元素计算)，sum(所有元素求和，返回标量)，mean(加权平均，返回标量)</li></ul><p>代码如下：</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs routeros">inputs = torch.tensor([[1, 2], [2, 2], [3, 4], [4, 5]], <span class="hljs-attribute">dtype</span>=torch.float)<br>target = torch.tensor([[1, 0], [1, 0], [0, 1], [0, 1]], <span class="hljs-attribute">dtype</span>=torch.float)<br><br>target_bce = target<br><br><span class="hljs-comment"># itarget</span><br><span class="hljs-comment"># inputs = torch.sigmoid(inputs)</span><br><br>weights = torch.tensor([1], <span class="hljs-attribute">dtype</span>=torch.float)<br>pos_w = torch.tensor([3], <span class="hljs-attribute">dtype</span>=torch.float)        # 3<br><br>loss_f_none_w = nn.BCEWithLogitsLoss(<span class="hljs-attribute">weight</span>=weights, <span class="hljs-attribute">reduction</span>=<span class="hljs-string">&#x27;none&#x27;</span>, <span class="hljs-attribute">pos_weight</span>=pos_w)<br>loss_f_sum = nn.BCEWithLogitsLoss(<span class="hljs-attribute">weight</span>=weights, <span class="hljs-attribute">reduction</span>=<span class="hljs-string">&#x27;sum&#x27;</span>, <span class="hljs-attribute">pos_weight</span>=pos_w)<br>loss_f_mean = nn.BCEWithLogitsLoss(<span class="hljs-attribute">weight</span>=weights, <span class="hljs-attribute">reduction</span>=<span class="hljs-string">&#x27;mean&#x27;</span>, <span class="hljs-attribute">pos_weight</span>=pos_w)<br><br><span class="hljs-comment"># forward</span><br>loss_none_w = loss_f_none_w(inputs, target_bce)<br>loss_sum = loss_f_sum(inputs, target_bce)<br>loss_mean = loss_f_mean(inputs, target_bce)<br><br><span class="hljs-comment"># view</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;\npos_weights: &quot;</span>, pos_w)<br><span class="hljs-built_in">print</span>(loss_none_w, loss_sum, loss_mean)<br></code></pre></td></tr></table></figure><p>输出为</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs stylus">pos_weights:  <span class="hljs-built_in">tensor</span>(<span class="hljs-selector-attr">[3.]</span>)<br><span class="hljs-built_in">tensor</span>(<span class="hljs-selector-attr">[[0.9398, 2.1269]</span>,<br>        <span class="hljs-selector-attr">[0.3808, 2.1269]</span>,<br>        <span class="hljs-selector-attr">[3.0486, 0.0544]</span>,<br>        <span class="hljs-selector-attr">[4.0181, 0.0201]</span>]) <span class="hljs-built_in">tensor</span>(<span class="hljs-number">12.7158</span>) <span class="hljs-built_in">tensor</span>(<span class="hljs-number">1.5895</span>)<br></code></pre></td></tr></table></figure><p>与 BCELoss 进行对比</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs stylus">BCE Loss <span class="hljs-built_in">tensor</span>(<span class="hljs-selector-attr">[[0.3133, 2.1269]</span>,<br>        <span class="hljs-selector-attr">[0.1269, 2.1269]</span>,<br>        <span class="hljs-selector-attr">[3.0486, 0.0181]</span>,<br>        <span class="hljs-selector-attr">[4.0181, 0.0067]</span>]) <span class="hljs-built_in">tensor</span>(<span class="hljs-number">11.7856</span>) <span class="hljs-built_in">tensor</span>(<span class="hljs-number">1.4732</span>)<br></code></pre></td></tr></table></figure><p>可以看到，样本类别对应的神经元的输出的 loss 都增加了 3 倍。</p><h3 id="nn-L1Loss"><a href="#nn-L1Loss" class="headerlink" title="nn.L1Loss"></a>nn.L1Loss</h3><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs routeros">nn.L1Loss(<span class="hljs-attribute">size_average</span>=None, <span class="hljs-attribute">reduce</span>=None, <span class="hljs-attribute">reduction</span>=<span class="hljs-string">&#x27;mean&#x27;</span>)<br></code></pre></td></tr></table></figure><p>功能：计算 inputs 与 target 之差的绝对值</p><p>主要参数：</p><ul><li>reduction：计算模式，可以为 none(逐个元素计算)，sum(所有元素求和，返回标量)，mean(加权平均，返回标量)</li></ul><p>公式：$l_{n}&#x3D;\left|x_{n}-y_{n}\right|$</p><h3 id="nn-MSELoss"><a href="#nn-MSELoss" class="headerlink" title="nn.MSELoss"></a>nn.MSELoss</h3><p>功能：计算 inputs 与 target 之差的平方</p><p>公式：$l_{n}&#x3D;\left(x_{n}-y_{n}\right)^{2}$</p><p>主要参数：</p><ul><li>reduction：计算模式，可以为 none(逐个元素计算)，sum(所有元素求和，返回标量)，mean(加权平均，返回标量)</li></ul><p>代码如下：</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs routeros">inputs = torch.ones((2, 2))<br>target = torch.ones((2, 2)) * 3<br><br>loss_f = nn.L1Loss(<span class="hljs-attribute">reduction</span>=<span class="hljs-string">&#x27;none&#x27;</span>)<br>loss = loss_f(inputs, target)<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;input:&#123;&#125;\ntarget:&#123;&#125;\nL1 loss:&#123;&#125;&quot;</span>.format(inputs, target, loss))<br><br><span class="hljs-comment"># ------------------------------------------------- 6 MSE loss ----------------------------------------------</span><br><br>loss_f_mse = nn.MSELoss(<span class="hljs-attribute">reduction</span>=<span class="hljs-string">&#x27;none&#x27;</span>)<br>loss_mse = loss_f_mse(inputs, target)<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;MSE loss:&#123;&#125;&quot;</span>.format(loss_mse))<br></code></pre></td></tr></table></figure><p>输出如下：</p><figure class="highlight lua"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs lua"><span class="hljs-built_in">input</span>:tensor(<span class="hljs-string">[[1., 1.],</span><br><span class="hljs-string">        [1., 1.]]</span>)<br>target:tensor(<span class="hljs-string">[[3., 3.],</span><br><span class="hljs-string">        [3., 3.]]</span>)<br>L1 loss:tensor(<span class="hljs-string">[[2., 2.],</span><br><span class="hljs-string">        [2., 2.]]</span>)<br>MSE loss:tensor(<span class="hljs-string">[[4., 4.],</span><br><span class="hljs-string">        [4., 4.]]</span>)<br></code></pre></td></tr></table></figure><h3 id="nn-SmoothL1Loss"><a href="#nn-SmoothL1Loss" class="headerlink" title="nn.SmoothL1Loss"></a>nn.SmoothL1Loss</h3><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs routeros">nn.SmoothL1Loss(<span class="hljs-attribute">size_average</span>=None, <span class="hljs-attribute">reduce</span>=None, <span class="hljs-attribute">reduction</span>=<span class="hljs-string">&#x27;mean&#x27;</span>)<br></code></pre></td></tr></table></figure><p>功能：平滑的 L1Loss</p><p>公式：<br><img src="/img/pytorchtrain4/6.png"></p><p>下图中橙色曲线是 L1Loss，蓝色曲线是 Smooth L1Loss</p><p><img src="/img/pytorchtrain4/7.png"><br> 主要参数：</p><ul><li>reduction：计算模式，可以为 none(逐个元素计算)，sum(所有元素求和，返回标量)，mean(加权平均，返回标量)</li></ul><h3 id="nn-PoissonNLLLoss"><a href="#nn-PoissonNLLLoss" class="headerlink" title="nn.PoissonNLLLoss"></a>nn.PoissonNLLLoss</h3><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs routeros">nn.PoissonNLLLoss(<span class="hljs-attribute">log_input</span>=<span class="hljs-literal">True</span>, <span class="hljs-attribute">full</span>=<span class="hljs-literal">False</span>, <span class="hljs-attribute">size_average</span>=None, <span class="hljs-attribute">eps</span>=1e-08, <span class="hljs-attribute">reduce</span>=None, <span class="hljs-attribute">reduction</span>=<span class="hljs-string">&#x27;mean&#x27;</span>)<br></code></pre></td></tr></table></figure><p>功能：泊松分布的负对数似然损失函数</p><p>主要参数：</p><ul><li>log_input：输入是否为对数形式，决定计算公式<ul><li>当 log_input &#x3D;  True，表示输入数据已经是经过对数运算之后的，loss(input, target) &#x3D; exp(input) - target * input</li><li>当 log_input &#x3D;  False，，表示输入数据还没有取对数，loss(input, target) &#x3D; input - target * log(input+eps)</li></ul></li><li>full：计算所有 loss，默认为 loss</li><li>eps：修正项，避免 log(input) 为 nan</li></ul><p>代码如下：</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs routeros">inputs = torch.randn((2, 2))<br>target = torch.randn((2, 2))<br><br>loss_f = nn.PoissonNLLLoss(<span class="hljs-attribute">log_input</span>=<span class="hljs-literal">True</span>, <span class="hljs-attribute">full</span>=<span class="hljs-literal">False</span>, <span class="hljs-attribute">reduction</span>=<span class="hljs-string">&#x27;none&#x27;</span>)<br>loss = loss_f(inputs, target)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;input:&#123;&#125;\ntarget:&#123;&#125;\nPoisson NLL loss:&#123;&#125;&quot;</span>.format(inputs, target, loss))<br></code></pre></td></tr></table></figure><p>输出如下：</p><figure class="highlight lua"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs lua"><span class="hljs-built_in">input</span>:tensor(<span class="hljs-string">[[0.6614, 0.2669],</span><br><span class="hljs-string">        [0.0617, 0.6213]]</span>)<br>target:tensor(<span class="hljs-string">[[-0.4519, -0.1661],</span><br><span class="hljs-string">        [-1.5228,  0.3817]]</span>)<br>Poisson NLL loss:tensor(<span class="hljs-string">[[2.2363, 1.3503],</span><br><span class="hljs-string">        [1.1575, 1.6242]]</span>)<br></code></pre></td></tr></table></figure><p>手动计算第一个 loss 的代码如下：</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs stylus">idx = <span class="hljs-number">0</span><br><br>loss_1 = torch<span class="hljs-selector-class">.exp</span>(inputs<span class="hljs-selector-attr">[idx, idx]</span>) - target<span class="hljs-selector-attr">[idx, idx]</span>*inputs<span class="hljs-selector-attr">[idx, idx]</span><br><br><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(<span class="hljs-string">&quot;第一个元素loss:&quot;</span>, loss_1)</span></span><br></code></pre></td></tr></table></figure><p>结果为：2.2363</p><h3 id="nn-KLDivLoss"><a href="#nn-KLDivLoss" class="headerlink" title="nn.KLDivLoss"></a>nn.KLDivLoss</h3><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs routeros">nn.KLDivLoss(<span class="hljs-attribute">size_average</span>=None, <span class="hljs-attribute">reduce</span>=None, <span class="hljs-attribute">reduction</span>=<span class="hljs-string">&#x27;mean&#x27;</span>)<br></code></pre></td></tr></table></figure><p>功能：计算 KLD(divergence)，KL 散度，相对熵</p><p>注意事项：需要提前将输入计算 log-probabilities，如通过<code>nn.logsoftmax()</code></p><p>主要参数：</p><ul><li>reduction：计算模式，可以为 none(逐个元素计算)，sum(所有元素求和，返回标量)，mean(加权平均，返回标量)，batchmean(batchsize 维度求平均值)</li></ul><p>公式：$\begin{aligned} D_{K L}(P | Q)&#x3D;E_{x-p}\left[\log \frac{P(x)}{Q(x)}\right] &amp;&#x3D;E_{x-p}[\log P(x)-\log Q(x)] &#x3D;\sum_{i&#x3D;1}^{N} P\left(x_{i}\right)\left(\log P\left(x_{i}\right)-\log Q\left(x_{i}\right)\right) \end{aligned}$</p><p>对于每个样本来说，计算公式如下，其中$y_{n}$是真实值$P(x)$，$x_{n}$是经过对数运算之后的预测值$logQ(x)$。</p><p>$l_{n}&#x3D;y_{n} \cdot\left(\log y_{n}-x_{n}\right)$</p><p>代码如下：</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs stylus">inputs = torch<span class="hljs-selector-class">.tensor</span>(<span class="hljs-selector-attr">[[0.5, 0.3, 0.2]</span>, <span class="hljs-selector-attr">[0.2, 0.3, 0.5]</span>])<br>inputs_log = torch<span class="hljs-selector-class">.log</span>(inputs)<br>target = torch<span class="hljs-selector-class">.tensor</span>(<span class="hljs-selector-attr">[[0.9, 0.05, 0.05]</span>, <span class="hljs-selector-attr">[0.1, 0.7, 0.2]</span>], dtype=torch.<span class="hljs-attribute">float</span>)<br><br>loss_f_none = nn<span class="hljs-selector-class">.KLDivLoss</span>(reduction=<span class="hljs-string">&#x27;none&#x27;</span>)<br>loss_f_mean = nn<span class="hljs-selector-class">.KLDivLoss</span>(reduction=<span class="hljs-string">&#x27;mean&#x27;</span>)<br>loss_f_bs_mean = nn<span class="hljs-selector-class">.KLDivLoss</span>(reduction=<span class="hljs-string">&#x27;batchmean&#x27;</span>)<br><br>loss_none = <span class="hljs-built_in">loss_f_none</span>(inputs, target)<br>loss_mean = <span class="hljs-built_in">loss_f_mean</span>(inputs, target)<br>loss_bs_mean = <span class="hljs-built_in">loss_f_bs_mean</span>(inputs, target)<br><br><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(<span class="hljs-string">&quot;loss_none:\n&#123;&#125;\nloss_mean:\n&#123;&#125;\nloss_bs_mean:\n&#123;&#125;&quot;</span>.format(loss_none, loss_mean, loss_bs_mean)</span></span>)<br></code></pre></td></tr></table></figure><p>输出如下：</p><figure class="highlight subunit"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs subunit">loss_none:<br>tensor([[<span class="hljs-string">-0</span>.5448, <span class="hljs-string">-0</span>.1648, <span class="hljs-string">-0</span>.1598],<br>        [<span class="hljs-string">-0</span>.2503, <span class="hljs-string">-0</span>.4597, <span class="hljs-string">-0</span>.4219]])<br>loss_mean:<br><span class="hljs-string">-0</span>.3335360586643219<br>loss_bs_mean:<br><span class="hljs-string">-1</span>.000608205795288<br></code></pre></td></tr></table></figure><p>手动计算第一个 loss 的代码为：</p><figure class="highlight stan"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs stan">idx = <span class="hljs-number">0</span><br>loss_1 = <span class="hljs-built_in">target</span>[idx, idx] * (torch.<span class="hljs-built_in">log</span>(<span class="hljs-built_in">target</span>[idx, idx]) - inputs[idx, idx])<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;第一个元素loss:&quot;</span>, loss_1)<br></code></pre></td></tr></table></figure><p>结果为：-0.5448。</p><h3 id="nn-MarginRankingLoss"><a href="#nn-MarginRankingLoss" class="headerlink" title="nn.MarginRankingLoss"></a>nn.MarginRankingLoss</h3><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs routeros">nn.MarginRankingLoss(<span class="hljs-attribute">margin</span>=0.0, <span class="hljs-attribute">size_average</span>=None, <span class="hljs-attribute">reduce</span>=None, <span class="hljs-attribute">reduction</span>=<span class="hljs-string">&#x27;mean&#x27;</span>)<br></code></pre></td></tr></table></figure><p>功能：计算两个向量之间的相似度，用于排序任务</p><p>特别说明：该方法计算 两组数据之间的差异，返回一个$n \times n$ 的 loss 矩阵</p><p>主要参数：</p><ul><li>margin：边界值，$x_{1}$与$x_{2}$之间的差异值</li><li>reduction：计算模式，可以为 none(逐个元素计算)，sum(所有元素求和，返回标量)，mean(加权平均，返回标量)</li></ul><p>计算公式：$\operatorname{loss}(x, y)&#x3D;\max (0,-y *(x 1-x 2)+\operatorname{margin})$，$y$的取值有 +1 和 -1。</p><ul><li>当 $y&#x3D;1$时，希望$x_{1} &gt; x_{2}$，当$x_{1} &gt; x_{2}$，不产生 loss</li><li>当 $y&#x3D;-1$时，希望$x_{1} &lt; x_{2}$，当$x_{1} &lt; x_{2}$，不产生 loss</li></ul><p>代码如下：</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs stylus">x1 = torch<span class="hljs-selector-class">.tensor</span>(<span class="hljs-selector-attr">[[1]</span>, <span class="hljs-selector-attr">[2]</span>, <span class="hljs-selector-attr">[3]</span>], dtype=torch.<span class="hljs-attribute">float</span>)<br>x2 = torch<span class="hljs-selector-class">.tensor</span>(<span class="hljs-selector-attr">[[2]</span>, <span class="hljs-selector-attr">[2]</span>, <span class="hljs-selector-attr">[2]</span>], dtype=torch.<span class="hljs-attribute">float</span>)<br><br>target = torch<span class="hljs-selector-class">.tensor</span>(<span class="hljs-selector-attr">[1, 1, -1]</span>, dtype=torch.<span class="hljs-attribute">float</span>)<br><br>loss_f_none = nn<span class="hljs-selector-class">.MarginRankingLoss</span>(<span class="hljs-attribute">margin</span>=<span class="hljs-number">0</span>, reduction=<span class="hljs-string">&#x27;none&#x27;</span>)<br><br>loss = <span class="hljs-built_in">loss_f_none</span>(x1, x2, target)<br><br><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(loss)</span></span><br></code></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight lua"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs lua">tensor(<span class="hljs-string">[[1., 1., 0.],</span><br><span class="hljs-string">        [0., 0., 0.],</span><br><span class="hljs-string">        [0., 0., 1.]]</span>)<br></code></pre></td></tr></table></figure><p>第一行表示$x_{1}$中的第一个元素分别与$x_{2}$中的 3 个元素计算 loss，以此类推。</p><h3 id="nn-MultiLabelMarginLoss"><a href="#nn-MultiLabelMarginLoss" class="headerlink" title="nn.MultiLabelMarginLoss"></a>nn.MultiLabelMarginLoss</h3><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs routeros">nn.MultiLabelMarginLoss(<span class="hljs-attribute">size_average</span>=None, <span class="hljs-attribute">reduce</span>=None, <span class="hljs-attribute">reduction</span>=<span class="hljs-string">&#x27;mean&#x27;</span>)<br></code></pre></td></tr></table></figure><p>功能：多标签边界损失函数</p><p>举例：4 分类任务，样本 x 属于 0 类和 3 类，那么标签为 [0, 3, -1, -1]，</p><p>主要参数：</p><ul><li>reduction：计算模式，可以为 none(逐个元素计算)，sum(所有元素求和，返回标量)，mean(加权平均，返回标量)</li></ul><p>计算公式：$\operatorname{loss}(x, y)&#x3D;\sum_{i j} \frac{\max (0,1-(x[y[j]]-x[i]))}{x \cdot \operatorname{size}(0)}$，表示每个真实类别的神经元输出减去其他神经元的输出。</p><p>代码如下：</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">x</span> = torch.tensor([[<span class="hljs-number">0</span>.<span class="hljs-number">1</span>, <span class="hljs-number">0</span>.<span class="hljs-number">2</span>, <span class="hljs-number">0</span>.<span class="hljs-number">4</span>, <span class="hljs-number">0</span>.<span class="hljs-number">8</span>]])<br><span class="hljs-attribute">y</span> = torch.tensor([[<span class="hljs-number">0</span>, <span class="hljs-number">3</span>, -<span class="hljs-number">1</span>, -<span class="hljs-number">1</span>]], dtype=torch.long)<br><br><span class="hljs-attribute">loss_f</span> = nn.MultiLabelMarginLoss(reduction=&#x27;none&#x27;)<br><br><span class="hljs-attribute">loss</span> = loss_f(x, y)<br><br><span class="hljs-attribute">print</span>(loss)<br></code></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">0</span>.<span class="hljs-number">8500</span><br></code></pre></td></tr></table></figure><p>手动计算如下：</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">x</span> = x[<span class="hljs-number">0</span>]<br><span class="hljs-attribute">item_1</span> = (<span class="hljs-number">1</span>-(x[<span class="hljs-number">0</span>] - x[<span class="hljs-number">1</span>])) + (<span class="hljs-number">1</span> - (x[<span class="hljs-number">0</span>] - x[<span class="hljs-number">2</span>]))    #<span class="hljs-meta"> [0]</span><br><span class="hljs-attribute">item_2</span> = (<span class="hljs-number">1</span>-(x[<span class="hljs-number">3</span>] - x[<span class="hljs-number">1</span>])) + (<span class="hljs-number">1</span> - (x[<span class="hljs-number">3</span>] - x[<span class="hljs-number">2</span>]))    #<span class="hljs-meta"> [3]</span><br><br><span class="hljs-attribute">loss_h</span> = (item_1 + item_2) / x.shape[<span class="hljs-number">0</span>]<br><br><span class="hljs-attribute">print</span>(loss_h)<br></code></pre></td></tr></table></figure><h3 id="nn-SoftMarginLoss"><a href="#nn-SoftMarginLoss" class="headerlink" title="nn.SoftMarginLoss"></a>nn.SoftMarginLoss</h3><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs routeros">nn.SoftMarginLoss(<span class="hljs-attribute">size_average</span>=None, <span class="hljs-attribute">reduce</span>=None, <span class="hljs-attribute">reduction</span>=<span class="hljs-string">&#x27;mean&#x27;</span>)<br></code></pre></td></tr></table></figure><p>功能：计算二分类的 logistic 损失</p><p>主要参数：</p><ul><li>reduction：计算模式，可以为 none(逐个元素计算)，sum(所有元素求和，返回标量)，mean(加权平均，返回标量)</li></ul><p>计算公式：$\operatorname{loss}(x, y)&#x3D;\sum_{i} \frac{\log (1+\exp (-y[i] * x[i]))}{\text { x.nelement } 0}$</p><p>代码如下：</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs stylus">inputs = torch<span class="hljs-selector-class">.tensor</span>(<span class="hljs-selector-attr">[[0.3, 0.7]</span>, <span class="hljs-selector-attr">[0.5, 0.5]</span>])<br>target = torch<span class="hljs-selector-class">.tensor</span>(<span class="hljs-selector-attr">[[-1, 1]</span>, <span class="hljs-selector-attr">[1, -1]</span>], dtype=torch.<span class="hljs-attribute">float</span>)<br><br>loss_f = nn<span class="hljs-selector-class">.SoftMarginLoss</span>(reduction=<span class="hljs-string">&#x27;none&#x27;</span>)<br><br>loss = <span class="hljs-built_in">loss_f</span>(inputs, target)<br><br><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(<span class="hljs-string">&quot;SoftMargin: &quot;</span>, loss)</span></span><br></code></pre></td></tr></table></figure><p>输出如下：</p><figure class="highlight lua"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs lua">SoftMargin:  tensor(<span class="hljs-string">[[0.8544, 0.4032],</span><br><span class="hljs-string">        [0.4741, 0.9741]]</span>)<br></code></pre></td></tr></table></figure><p>手动计算第一个 loss 的代码如下：</p><figure class="highlight maxima"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs maxima">idx = <span class="hljs-number">0</span><br><br>inputs_i = inputs[idx, idx]<br>target_i = target[idx, idx]<br><br>loss_h = <span class="hljs-built_in">np</span>.<span class="hljs-built_in">log</span>(<span class="hljs-number">1</span> + <span class="hljs-built_in">np</span>.<span class="hljs-built_in">exp</span>(-target_i * inputs_i))<br><br><span class="hljs-built_in">print</span>(loss_h)<br></code></pre></td></tr></table></figure><p>结果为：0.8544</p><h3 id="nn-MultiLabelSoftMarginLoss"><a href="#nn-MultiLabelSoftMarginLoss" class="headerlink" title="nn.MultiLabelSoftMarginLoss"></a>nn.MultiLabelSoftMarginLoss</h3><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs routeros">nn.MultiLabelSoftMarginLoss(<span class="hljs-attribute">weight</span>=None, <span class="hljs-attribute">size_average</span>=None, <span class="hljs-attribute">reduce</span>=None, <span class="hljs-attribute">reduction</span>=<span class="hljs-string">&#x27;mean&#x27;</span>)<br></code></pre></td></tr></table></figure><p>功能：SoftMarginLoss 的多标签版本</p><p>主要参数：</p><ul><li>weight：各类别的 loss 权值设置</li><li>reduction：计算模式，可以为 none(逐个元素计算)，sum(所有元素求和，返回标量)，mean(加权平均，返回标量)</li></ul><p>计算公式：$\operatorname{loss}(x, y)&#x3D;-\frac{1}{C}  <em>\sum_{i} y[i]</em>  \log \left((1+\exp (-x[i]))^{-1}\right)+(1-y[i]) * \log \left(\frac{\exp (-x[i])}{(1+\exp (-x[i]))}\right)$</p><p>代码如下</p><figure class="highlight lua"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs lua">inputs = torch.tensor(<span class="hljs-string">[[0.3, 0.7, 0.8]]</span>)<br>target = torch.tensor(<span class="hljs-string">[[0, 1, 1]]</span>, dtype=torch.float)<br><br>loss_f = nn.MultiLabelSoftMarginLoss(reduction=<span class="hljs-string">&#x27;none&#x27;</span>)<br><br>loss = loss_f(inputs, target)<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;MultiLabel SoftMargin: &quot;</span>, loss)<br></code></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">MultiLabel</span> SoftMargin:  tensor([<span class="hljs-number">0</span>.<span class="hljs-number">5429</span>])<br></code></pre></td></tr></table></figure><p>手动计算的代码如下：</p><figure class="highlight lua"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs lua">x = torch.tensor(<span class="hljs-string">[[0.1, 0.2, 0.7], [0.2, 0.5, 0.3]]</span>)<br>y = torch.tensor([<span class="hljs-number">1</span>, <span class="hljs-number">2</span>], dtype=torch.long)<br><br>loss_f = nn.MultiMarginLoss(reduction=<span class="hljs-string">&#x27;none&#x27;</span>)<br><br>loss = loss_f(x, y)<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Multi Margin Loss: &quot;</span>, loss)<br></code></pre></td></tr></table></figure><h3 id="nn-MultiMarginLoss"><a href="#nn-MultiMarginLoss" class="headerlink" title="nn.MultiMarginLoss"></a>nn.MultiMarginLoss</h3><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs routeros">nn.MultiMarginLoss(<span class="hljs-attribute">p</span>=1, <span class="hljs-attribute">margin</span>=1.0, <span class="hljs-attribute">weight</span>=None, <span class="hljs-attribute">size_average</span>=None, <span class="hljs-attribute">reduce</span>=None, <span class="hljs-attribute">reduction</span>=<span class="hljs-string">&#x27;mean&#x27;</span>)<br></code></pre></td></tr></table></figure><p>功能：计算多分类的折页损失</p><p>主要参数：</p><ul><li>p：可以选择 1 或 2</li><li>weight：各类别的 loss 权值设置</li><li>margin：边界值</li><li>reduction：计算模式，可以为 none(逐个元素计算)，sum(所有元素求和，返回标量)，mean(加权平均，返回标量)</li></ul><p>计算公式：$\operatorname{loss}(x, y)&#x3D;\frac{\left.\sum_{i} \max (0, \operatorname{margin}-x[y]+x[i])\right)^{p}}{\quad \text { x.size }(0)}$，其中 y 表示真实标签对应的神经元输出，x 表示其他神经元的输出。</p><p>代码如下：</p><figure class="highlight lua"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs lua">x = torch.tensor(<span class="hljs-string">[[0.1, 0.2, 0.7], [0.2, 0.5, 0.3]]</span>)<br>y = torch.tensor([<span class="hljs-number">1</span>, <span class="hljs-number">2</span>], dtype=torch.long)<br><br>loss_f = nn.MultiMarginLoss(reduction=<span class="hljs-string">&#x27;none&#x27;</span>)<br><br>loss = loss_f(x, y)<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Multi Margin Loss: &quot;</span>, loss)<br></code></pre></td></tr></table></figure><p>输出如下：</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">Multi</span> Margin Loss:  tensor([<span class="hljs-number">0</span>.<span class="hljs-number">8000</span>, <span class="hljs-number">0</span>.<span class="hljs-number">7000</span>])<br></code></pre></td></tr></table></figure><p>手动计算第一个 loss 的代码如下：</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">x</span> = x[<span class="hljs-number">0</span>]<br><span class="hljs-attribute">margin</span> = <span class="hljs-number">1</span><br><br><span class="hljs-attribute">i_0</span> = margin - (x[<span class="hljs-number">1</span>] - x[<span class="hljs-number">0</span>])<br><span class="hljs-comment"># i_1 = margin - (x[1] - x[1])</span><br><span class="hljs-attribute">i_2</span> = margin - (x[<span class="hljs-number">1</span>] - x[<span class="hljs-number">2</span>])<br><br><span class="hljs-attribute">loss_h</span> = (i_0 + i_2) / x.shape[<span class="hljs-number">0</span>]<br><br><span class="hljs-attribute">print</span>(loss_h)<br></code></pre></td></tr></table></figure><p>输出为：0.8000</p><h3 id="nn-TripletMarginLoss"><a href="#nn-TripletMarginLoss" class="headerlink" title="nn.TripletMarginLoss"></a>nn.TripletMarginLoss</h3><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs routeros">nn.TripletMarginLoss(<span class="hljs-attribute">margin</span>=1.0, <span class="hljs-attribute">p</span>=2.0, <span class="hljs-attribute">eps</span>=1e-06, <span class="hljs-attribute">swap</span>=<span class="hljs-literal">False</span>, <span class="hljs-attribute">size_average</span>=None, <span class="hljs-attribute">reduce</span>=None, <span class="hljs-attribute">reduction</span>=<span class="hljs-string">&#x27;mean&#x27;</span>)<br></code></pre></td></tr></table></figure><p>功能：计算三元组损失，人脸验证中常用</p><p>主要参数：</p><ul><li>p：范数的阶，默认为 2</li><li>margin：边界值</li><li>reduction：计算模式，可以为 none(逐个元素计算)，sum(所有元素求和，返回标量)，mean(加权平均，返回标量)</li></ul><p>计算公式：<img src="/img/pytorchtrain4/8.png">，$d\left(x{i}, y{i}\right)&#x3D;\left|\mathbf{x}{i}-\mathbf{y}{i}\right|{p}$，其中$d(a_{i}, p_{i})$表示正样本对之间的距离(距离计算公式与 p 有关)，$d(a_{i}, n_{i})$表示负样本对之间的距离。表示正样本对之间的距离比负样本对之间的距离小 margin，就没有了 loss。</p><p>代码如下：</p><figure class="highlight lua"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs lua">anchor = torch.tensor(<span class="hljs-string">[[1.]]</span>)<br>pos = torch.tensor(<span class="hljs-string">[[2.]]</span>)<br>neg = torch.tensor(<span class="hljs-string">[[0.5]]</span>)<br><br>loss_f = nn.TripletMarginLoss(margin=<span class="hljs-number">1.0</span>, p=<span class="hljs-number">1</span>)<br><br>loss = loss_f(anchor, pos, neg)<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Triplet Margin Loss&quot;</span>, loss)<br></code></pre></td></tr></table></figure><p>输出如下：</p><figure class="highlight scss"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs scss">Triplet <span class="hljs-attribute">Margin</span> Loss <span class="hljs-built_in">tensor</span>(<span class="hljs-number">1.5000</span>)<br></code></pre></td></tr></table></figure><p>手动计算的代码如下：</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs stylus"><span class="hljs-attribute">margin</span> = <span class="hljs-number">1</span><br><span class="hljs-selector-tag">a</span>, <span class="hljs-selector-tag">p</span>, n = anchor<span class="hljs-selector-attr">[0]</span>, pos<span class="hljs-selector-attr">[0]</span>, neg<span class="hljs-selector-attr">[0]</span><br><br>d_ap = torch<span class="hljs-selector-class">.abs</span>(a-p)<br>d_an = torch<span class="hljs-selector-class">.abs</span>(a-n)<br><br>loss = d_ap - d_an + <span class="hljs-attribute">margin</span><br><br><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(loss)</span></span><br></code></pre></td></tr></table></figure><h3 id="nn-HingeEmbeddingLoss"><a href="#nn-HingeEmbeddingLoss" class="headerlink" title="nn.HingeEmbeddingLoss"></a>nn.HingeEmbeddingLoss</h3><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs routeros">nn.HingeEmbeddingLoss(<span class="hljs-attribute">margin</span>=1.0, <span class="hljs-attribute">size_average</span>=None, <span class="hljs-attribute">reduce</span>=None, <span class="hljs-attribute">reduction</span>=<span class="hljs-string">&#x27;mean&#x27;</span>)<br></code></pre></td></tr></table></figure><p>功能：计算两个输入的相似性，常用于非线性 embedding 和半监督学习</p><p>特别注意：输入 x 应该为两个输入之差的绝对值</p><p>主要参数：</p><ul><li>margin：边界值</li><li>reduction：计算模式，可以为 none(逐个元素计算)，sum(所有元素求和，返回标量)，mean(加权平均，返回标量)</li></ul><p>计算公式：</p><p>!<img src="/img/pytorchtrain4/9.png"></p><p>代码如下：</p><figure class="highlight lua"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs lua">inputs = torch.tensor(<span class="hljs-string">[[1., 0.8, 0.5]]</span>)<br>target = torch.tensor(<span class="hljs-string">[[1, 1, -1]]</span>)<br><br>loss_f = nn.HingeEmbeddingLoss(margin=<span class="hljs-number">1</span>, reduction=<span class="hljs-string">&#x27;none&#x27;</span>)<br><br>loss = loss_f(inputs, target)<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Hinge Embedding Loss&quot;</span>, loss)<br></code></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">Hinge</span> Embedding Loss tensor([[<span class="hljs-number">1</span>.<span class="hljs-number">0000</span>, <span class="hljs-number">0</span>.<span class="hljs-number">8000</span>, <span class="hljs-number">0</span>.<span class="hljs-number">5000</span>]])<br></code></pre></td></tr></table></figure><p>手动计算第三个 loss 的代码如下：</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">margin</span> = <span class="hljs-number">1</span>.<br><span class="hljs-attribute">loss</span> = max(<span class="hljs-number">0</span>, margin - inputs.numpy()[<span class="hljs-number">0</span>, <span class="hljs-number">2</span>])<br><br><span class="hljs-attribute">print</span>(loss)<br></code></pre></td></tr></table></figure><p>结果为 0.5</p><h3 id="nn-CosineEmbeddingLoss"><a href="#nn-CosineEmbeddingLoss" class="headerlink" title="nn.CosineEmbeddingLoss"></a>nn.CosineEmbeddingLoss</h3><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs routeros">torch.nn.CosineEmbeddingLoss(<span class="hljs-attribute">margin</span>=0.0, <span class="hljs-attribute">size_average</span>=None, <span class="hljs-attribute">reduce</span>=None, <span class="hljs-attribute">reduction</span>=<span class="hljs-string">&#x27;mean&#x27;</span>)<br></code></pre></td></tr></table></figure><p>功能：采用余弦相似度计算两个输入的相似性</p><p>主要参数：</p><ul><li>margin：边界值，可取值 [-1, 1]，推荐为 [0, 0.5]</li><li>reduction：计算模式，可以为 none(逐个元素计算)，sum(所有元素求和，返回标量)，mean(加权平均，返回标量)</li></ul><p>计算公式：</p><p><img src="/img/pytorchtrain4/10.png"></p><p>其中$\cos (\theta)&#x3D;\frac{A \cdot B}{|A||B|}&#x3D;\frac{\sum_{i&#x3D;1}^{n} A_{i} \times B_{i}}{\sqrt{\sum_{i&#x3D;1}^{n}\left(A_{i}\right)^{2}} \times \sqrt{\sum_{i&#x3D;1}^{n}\left(B_{i}\right)^{2}}}$</p><p>代码如下：</p><figure class="highlight lua"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs lua">x1 = torch.tensor(<span class="hljs-string">[[0.3, 0.5, 0.7], [0.3, 0.5, 0.7]]</span>)<br>x2 = torch.tensor(<span class="hljs-string">[[0.1, 0.3, 0.5], [0.1, 0.3, 0.5]]</span>)<br><br>target = torch.tensor(<span class="hljs-string">[[1, -1]]</span>, dtype=torch.float)<br><br>loss_f = nn.CosineEmbeddingLoss(margin=<span class="hljs-number">0.</span>, reduction=<span class="hljs-string">&#x27;none&#x27;</span>)<br><br>loss = loss_f(x1, x2, target)<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Cosine Embedding Loss&quot;</span>, loss)<br></code></pre></td></tr></table></figure><p>输出如下：</p><figure class="highlight lua"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs lua">Cosine Embedding Loss tensor(<span class="hljs-string">[[0.0167, 0.9833]]</span>)<br></code></pre></td></tr></table></figure><p>手动计算第一个样本的 loss 的代码为：</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">margin</span> = <span class="hljs-number">0</span>.<br><br><span class="hljs-attribute">def</span> cosine(a, b):<br><span class="hljs-attribute">numerator</span> = torch.dot(a, b)<br><span class="hljs-attribute">denominator</span> = torch.norm(a, <span class="hljs-number">2</span>) * torch.norm(b, <span class="hljs-number">2</span>)<br><span class="hljs-attribute">return</span> float(numerator/denominator)<br><br><span class="hljs-attribute">l_1</span> = <span class="hljs-number">1</span> - (cosine(x1[<span class="hljs-number">0</span>], x2[<span class="hljs-number">0</span>]))<br><br><span class="hljs-attribute">l_2</span> = max(<span class="hljs-number">0</span>, cosine(x1[<span class="hljs-number">0</span>], x2[<span class="hljs-number">0</span>]))<br><br><span class="hljs-attribute">print</span>(l_1, l_2)<br></code></pre></td></tr></table></figure><p>结果为：0.016662120819091797 0.9833378791809082</p><h3 id="nn-CTCLoss"><a href="#nn-CTCLoss" class="headerlink" title="nn.CTCLoss"></a>nn.CTCLoss</h3><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs routeros">nn.CTCLoss(<span class="hljs-attribute">blank</span>=0, <span class="hljs-attribute">reduction</span>=<span class="hljs-string">&#x27;mean&#x27;</span>, <span class="hljs-attribute">zero_infinity</span>=<span class="hljs-literal">False</span>)<br></code></pre></td></tr></table></figure><p>功能：计算 CTC 损失，解决时序类数据的分类，全称为 Connectionist Temporal Classification</p><p>主要参数：</p><ul><li>blank：blank label</li><li>zero_infinity：无穷大的值或梯度置 0</li><li>reduction：计算模式，可以为 none(逐个元素计算)，sum(所有元素求和，返回标量)，mean(加权平均，返回标量)</li></ul><p>对时序方面研究比较少，不展开讲了。</p><h2 id="优化器"><a href="#优化器" class="headerlink" title="优化器"></a>优化器</h2><p>PyTorch 中的优化器是用于管理并更新模型中可学习参数的值，使得模型输出更加接近真实标签。</p><h3 id="optimizer-的属性"><a href="#optimizer-的属性" class="headerlink" title="optimizer 的属性"></a>optimizer 的属性</h3><p>PyTorch 中提供了 Optimizer 类，定义如下：</p><figure class="highlight pf"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs pf">class Optimizer(object):<br>    def __init__(<span class="hljs-literal">self</span>, params, defaults):        <br>        <span class="hljs-literal">self</span>.defaults = defaults<br>        <span class="hljs-literal">self</span>.<span class="hljs-keyword">state</span> = defaultdict(dict)<br>        <span class="hljs-literal">self</span>.param_groups = []<br></code></pre></td></tr></table></figure><p>主要有 3 个属性</p><ul><li>defaults：优化器的超参数，如 weight_decay，momentum</li><li>state：参数的缓存，如 momentum 中需要用到前几次的梯度，就缓存在这个变量中</li><li>param_groups：管理的参数组，是一个 list，其中每个元素是字典，包括 momentum、lr、weight_decay、params 等。</li><li>_step_count：记录更新 次数，在学习率调整中使用</li></ul><h3 id="optimizer-的方法"><a href="#optimizer-的方法" class="headerlink" title="optimizer 的方法"></a>optimizer 的方法</h3><ul><li><p>zero_grad()：清空所管理参数的梯度。由于 PyTorch 的特性是张量的梯度不自动清零，因此每次反向传播之后都需要清空梯度。代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">zero_grad</span>(<span class="hljs-params">self</span>):<br>    <span class="hljs-string">r&quot;&quot;&quot;Clears the gradients of all optimized :class:`torch.Tensor` s.&quot;&quot;&quot;</span><br>    <span class="hljs-keyword">for</span> group <span class="hljs-keyword">in</span> <span class="hljs-variable language_">self</span>.param_groups:<br>        <span class="hljs-keyword">for</span> p <span class="hljs-keyword">in</span> group[<span class="hljs-string">&#x27;params&#x27;</span>]:<br>            <span class="hljs-keyword">if</span> p.grad <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>                p.grad.detach_()<br>                p.grad.zero_()<br></code></pre></td></tr></table></figure></li><li><p>step()：执行一步梯度更新</p></li><li><p>add_param_group()：添加参数组，主要代码如下：</p><figure class="highlight csharp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs csharp"><span class="hljs-function">def <span class="hljs-title">add_param_group</span>(<span class="hljs-params">self, param_group</span>):</span><br><span class="hljs-function">    <span class="hljs-keyword">params</span></span> = param_group[<span class="hljs-string">&#x27;params&#x27;</span>]<br>    <span class="hljs-function"><span class="hljs-keyword">if</span> <span class="hljs-title">isinstance</span>(<span class="hljs-params"><span class="hljs-keyword">params</span>, torch.Tensor</span>):</span><br><span class="hljs-function">        param_group[&#x27;<span class="hljs-keyword">params</span>&#x27;]</span> = [<span class="hljs-keyword">params</span>]<br>    ...<br>    self.param_groups.append(param_group)<br></code></pre></td></tr></table></figure></li><li><p>state_dict()：获取优化器当前状态信息字典</p></li><li><p>load_state_dict()：加载状态信息字典，包括 state 、momentum_buffer 和 param_groups。主要用于模型的断点续训练。我们可以在每隔 50 个 epoch 就保存模型的 state_dict 到硬盘，在意外终止训练时，可以继续加载上次保存的状态，继续训练。代码如下：</p><figure class="highlight csharp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs csharp"><span class="hljs-function">def <span class="hljs-title">state_dict</span>(<span class="hljs-params">self</span>):</span><br><span class="hljs-function">    r&quot;&quot;&quot;Returns the state of the optimizer <span class="hljs-keyword">as</span> a :<span class="hljs-keyword">class</span>:`dict`.</span><br><span class="hljs-function">    ...</span><br><span class="hljs-function">    <span class="hljs-keyword">return</span></span> &#123;<br>    <span class="hljs-string">&#x27;state&#x27;</span>: packed_state,<br>    <span class="hljs-string">&#x27;param_groups&#x27;</span>: param_groups,<br>    &#125;<br></code></pre></td></tr></table></figure></li></ul><p>下面是代码示例：</p><h4 id="step"><a href="#step" class="headerlink" title="step()"></a>step()</h4><p>张量 weight 的形状为$2 \times 2$，并设置梯度为 1，把 weight 传进优化器，学习率设置为 1，执行<code>optimizer.step()</code>更新梯度，也就是所有的张量都减去 1。</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs routeros">weight = torch.randn((2, 2), <span class="hljs-attribute">requires_grad</span>=<span class="hljs-literal">True</span>)<br>weight.grad = torch.ones((2, 2))<br><br>optimizer = optim.SGD([weight], <span class="hljs-attribute">lr</span>=1)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;weight before step:&#123;&#125;&quot;</span>.format(weight.data))<br>optimizer.<span class="hljs-keyword">step</span>()        # 修改<span class="hljs-attribute">lr</span>=1, 0.1观察结果<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;weight after step:&#123;&#125;&quot;</span>.format(weight.data))<br></code></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight lua"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs lua">weight before step:tensor(<span class="hljs-string">[[0.6614, 0.2669],</span><br><span class="hljs-string">        [0.0617, 0.6213]]</span>)<br>weight after step:tensor(<span class="hljs-string">[[-0.3386, -0.7331],</span><br><span class="hljs-string">        [-0.9383, -0.3787]]</span>)<br></code></pre></td></tr></table></figure><h4 id="zero-grad"><a href="#zero-grad" class="headerlink" title="zero_grad()"></a>zero_grad()</h4><p>代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;weight before step:&#123;&#125;&quot;</span>.<span class="hljs-built_in">format</span>(weight.data))<br>optimizer.step()        <span class="hljs-comment"># 修改lr=1 0.1观察结果</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;weight after step:&#123;&#125;&quot;</span>.<span class="hljs-built_in">format</span>(weight.data))<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;weight in optimizer:&#123;&#125;\nweight in weight:&#123;&#125;\n&quot;</span>.<span class="hljs-built_in">format</span>(<span class="hljs-built_in">id</span>(optimizer.param_groups[<span class="hljs-number">0</span>][<span class="hljs-string">&#x27;params&#x27;</span>][<span class="hljs-number">0</span>]), <span class="hljs-built_in">id</span>(weight)))<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;weight.grad is &#123;&#125;\n&quot;</span>.<span class="hljs-built_in">format</span>(weight.grad))<br>optimizer.zero_grad()<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;after optimizer.zero_grad(), weight.grad is\n&#123;&#125;&quot;</span>.<span class="hljs-built_in">format</span>(weight.grad))<br></code></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight lua"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs lua">weight before step:tensor(<span class="hljs-string">[[0.6614, 0.2669],</span><br><span class="hljs-string">        [0.0617, 0.6213]]</span>)<br>weight after step:tensor(<span class="hljs-string">[[-0.3386, -0.7331],</span><br><span class="hljs-string">        [-0.9383, -0.3787]]</span>)<br>weight <span class="hljs-keyword">in</span> optimizer:<span class="hljs-number">1932450477472</span><br>weight <span class="hljs-keyword">in</span> weight:<span class="hljs-number">1932450477472</span><br>weight.grad is tensor(<span class="hljs-string">[[1., 1.],</span><br><span class="hljs-string">        [1., 1.]]</span>)<br>after optimizer.zero_grad(), weight.grad is<br>tensor(<span class="hljs-string">[[0., 0.],</span><br><span class="hljs-string">        [0., 0.]]</span>)<br></code></pre></td></tr></table></figure><p>可以看到优化器的 param_groups 中存储的参数和 weight 的内存地址是一样的，所以优化器中保存的是参数的地址，而不是把参数复制到优化器中。</p><h4 id="add-param-group"><a href="#add-param-group" class="headerlink" title="add_param_group()"></a>add_param_group()</h4><p>向优化器中添加一组参数，代码如下：</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs stylus"><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(<span class="hljs-string">&quot;optimizer.param_groups is\n&#123;&#125;&quot;</span>.format(optimizer.param_groups)</span></span>)<br>w2 = torch<span class="hljs-selector-class">.randn</span>((<span class="hljs-number">3</span>, <span class="hljs-number">3</span>), requires_grad=True)<br>optimizer<span class="hljs-selector-class">.add_param_group</span>(&#123;<span class="hljs-string">&quot;params&quot;</span>: w2, <span class="hljs-string">&#x27;lr&#x27;</span>: <span class="hljs-number">0.0001</span>&#125;)<br><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(<span class="hljs-string">&quot;optimizer.param_groups is\n&#123;&#125;&quot;</span>.format(optimizer.param_groups)</span></span>)<br></code></pre></td></tr></table></figure><p>输出如下：</p><figure class="highlight lua"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs lua">optimizer.param_groups is<br>[&#123;<span class="hljs-string">&#x27;params&#x27;</span>: [tensor(<span class="hljs-string">[[0.6614, 0.2669],</span><br><span class="hljs-string">        [0.0617, 0.6213]]</span>, requires_grad=True)], <span class="hljs-string">&#x27;lr&#x27;</span>: <span class="hljs-number">1</span>, <span class="hljs-string">&#x27;momentum&#x27;</span>: <span class="hljs-number">0</span>, <span class="hljs-string">&#x27;dampening&#x27;</span>: <span class="hljs-number">0</span>, <span class="hljs-string">&#x27;weight_decay&#x27;</span>: <span class="hljs-number">0</span>, <span class="hljs-string">&#x27;nesterov&#x27;</span>: False&#125;]<br>optimizer.param_groups is<br>[&#123;<span class="hljs-string">&#x27;params&#x27;</span>: [tensor(<span class="hljs-string">[[0.6614, 0.2669],</span><br><span class="hljs-string">        [0.0617, 0.6213]]</span>, requires_grad=True)], <span class="hljs-string">&#x27;lr&#x27;</span>: <span class="hljs-number">1</span>, <span class="hljs-string">&#x27;momentum&#x27;</span>: <span class="hljs-number">0</span>, <span class="hljs-string">&#x27;dampening&#x27;</span>: <span class="hljs-number">0</span>, <span class="hljs-string">&#x27;weight_decay&#x27;</span>: <span class="hljs-number">0</span>, <span class="hljs-string">&#x27;nesterov&#x27;</span>: False&#125;, &#123;<span class="hljs-string">&#x27;params&#x27;</span>: [tensor(<span class="hljs-string">[[-0.4519, -0.1661, -1.5228],</span><br><span class="hljs-string">        [ 0.3817, -1.0276, -0.5631],</span><br><span class="hljs-string">        [-0.8923, -0.0583, -0.1955]]</span>, requires_grad=True)], <span class="hljs-string">&#x27;lr&#x27;</span>: <span class="hljs-number">0.0001</span>, <span class="hljs-string">&#x27;momentum&#x27;</span>: <span class="hljs-number">0</span>, <span class="hljs-string">&#x27;dampening&#x27;</span>: <span class="hljs-number">0</span>, <span class="hljs-string">&#x27;weight_decay&#x27;</span>: <span class="hljs-number">0</span>, <span class="hljs-string">&#x27;nesterov&#x27;</span>: False&#125;]<br></code></pre></td></tr></table></figure><h4 id="state-dict"><a href="#state-dict" class="headerlink" title="state_dict()"></a>state_dict()</h4><p>首先进行 10 次反向传播更新，然后对比 state_dict 的变化。可以使用<code>torch.save()</code>把 state_dict 保存到 pkl 文件中。</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs stylus">optimizer = optim<span class="hljs-selector-class">.SGD</span>(<span class="hljs-selector-attr">[weight]</span>, lr=<span class="hljs-number">0.1</span>, momentum=<span class="hljs-number">0.9</span>)<br>opt_state_dict = optimizer<span class="hljs-selector-class">.state_dict</span>()<br><br><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(<span class="hljs-string">&quot;state_dict before step:\n&quot;</span>, opt_state_dict)</span></span><br><br><span class="hljs-keyword">for</span> <span class="hljs-selector-tag">i</span> <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">10</span>):<br>optimizer<span class="hljs-selector-class">.step</span>()<br><br><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(<span class="hljs-string">&quot;state_dict after step:\n&quot;</span>, optimizer.state_dict()</span></span>)<br><br>torch<span class="hljs-selector-class">.save</span>(optimizer<span class="hljs-selector-class">.state_dict</span>(), os<span class="hljs-selector-class">.path</span><span class="hljs-selector-class">.join</span>(BASE_DIR, <span class="hljs-string">&quot;optimizer_state_dict.pkl&quot;</span>))<br></code></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight prolog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs prolog">state_dict before step:<br> &#123;<span class="hljs-string">&#x27;state&#x27;</span>: &#123;&#125;, <span class="hljs-string">&#x27;param_groups&#x27;</span>: [&#123;<span class="hljs-string">&#x27;lr&#x27;</span>: <span class="hljs-number">0.1</span>, <span class="hljs-string">&#x27;momentum&#x27;</span>: <span class="hljs-number">0.9</span>, <span class="hljs-string">&#x27;dampening&#x27;</span>: <span class="hljs-number">0</span>, <span class="hljs-string">&#x27;weight_decay&#x27;</span>: <span class="hljs-number">0</span>, <span class="hljs-string">&#x27;nesterov&#x27;</span>: <span class="hljs-symbol">False</span>, <span class="hljs-string">&#x27;params&#x27;</span>: [<span class="hljs-number">1976501036448</span>]&#125;]&#125;<br>state_dict after step:<br> &#123;<span class="hljs-string">&#x27;state&#x27;</span>: &#123;<span class="hljs-number">1976501036448</span>: &#123;<span class="hljs-string">&#x27;momentum_buffer&#x27;</span>: tensor([[<span class="hljs-number">6.5132</span>, <span class="hljs-number">6.5132</span>],<br>        [<span class="hljs-number">6.5132</span>, <span class="hljs-number">6.5132</span>]])&#125;&#125;, <span class="hljs-string">&#x27;param_groups&#x27;</span>: [&#123;<span class="hljs-string">&#x27;lr&#x27;</span>: <span class="hljs-number">0.1</span>, <span class="hljs-string">&#x27;momentum&#x27;</span>: <span class="hljs-number">0.9</span>, <span class="hljs-string">&#x27;dampening&#x27;</span>: <span class="hljs-number">0</span>, <span class="hljs-string">&#x27;weight_decay&#x27;</span>: <span class="hljs-number">0</span>, <span class="hljs-string">&#x27;nesterov&#x27;</span>: <span class="hljs-symbol">False</span>, <span class="hljs-string">&#x27;params&#x27;</span>: [<span class="hljs-number">1976501036448</span>]&#125;]&#125;<br></code></pre></td></tr></table></figure><p>经过反向传播后，state_dict 中的字典保存了<code>1976501036448</code>作为 key，这个 key 就是参数的内存地址。</p><h4 id="load-state-dict"><a href="#load-state-dict" class="headerlink" title="load_state_dict()"></a>load_state_dict()</h4><p>上面保存了 state_dict 之后，可以先使用<code>torch.load()</code>把加载到内存中，然后再使用<code>load_state_dict()</code>加载到模型中，继续训练。代码如下：</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs stylus">optimizer = optim<span class="hljs-selector-class">.SGD</span>(<span class="hljs-selector-attr">[weight]</span>, lr=<span class="hljs-number">0.1</span>, momentum=<span class="hljs-number">0.9</span>)<br>state_dict = torch<span class="hljs-selector-class">.load</span>(os<span class="hljs-selector-class">.path</span><span class="hljs-selector-class">.join</span>(BASE_DIR, <span class="hljs-string">&quot;optimizer_state_dict.pkl&quot;</span>))<br><br><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(<span class="hljs-string">&quot;state_dict before load state:\n&quot;</span>, optimizer.state_dict()</span></span>)<br>optimizer<span class="hljs-selector-class">.load_state_dict</span>(state_dict)<br><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(<span class="hljs-string">&quot;state_dict after load state:\n&quot;</span>, optimizer.state_dict()</span></span>)<br></code></pre></td></tr></table></figure><p>输出如下：</p><figure class="highlight lua"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs lua">state_dict before <span class="hljs-built_in">load</span> state:<br> &#123;<span class="hljs-string">&#x27;state&#x27;</span>: &#123;&#125;, <span class="hljs-string">&#x27;param_groups&#x27;</span>: [&#123;<span class="hljs-string">&#x27;lr&#x27;</span>: <span class="hljs-number">0.1</span>, <span class="hljs-string">&#x27;momentum&#x27;</span>: <span class="hljs-number">0.9</span>, <span class="hljs-string">&#x27;dampening&#x27;</span>: <span class="hljs-number">0</span>, <span class="hljs-string">&#x27;weight_decay&#x27;</span>: <span class="hljs-number">0</span>, <span class="hljs-string">&#x27;nesterov&#x27;</span>: False, <span class="hljs-string">&#x27;params&#x27;</span>: [<span class="hljs-number">2075286132128</span>]&#125;]&#125;<br>state_dict after <span class="hljs-built_in">load</span> state:<br> &#123;<span class="hljs-string">&#x27;state&#x27;</span>: &#123;<span class="hljs-number">2075286132128</span>: &#123;<span class="hljs-string">&#x27;momentum_buffer&#x27;</span>: tensor(<span class="hljs-string">[[6.5132, 6.5132],</span><br><span class="hljs-string">        [6.5132, 6.5132]]</span>)&#125;&#125;, <span class="hljs-string">&#x27;param_groups&#x27;</span>: [&#123;<span class="hljs-string">&#x27;lr&#x27;</span>: <span class="hljs-number">0.1</span>, <span class="hljs-string">&#x27;momentum&#x27;</span>: <span class="hljs-number">0.9</span>, <span class="hljs-string">&#x27;dampening&#x27;</span>: <span class="hljs-number">0</span>, <span class="hljs-string">&#x27;weight_decay&#x27;</span>: <span class="hljs-number">0</span>, <span class="hljs-string">&#x27;nesterov&#x27;</span>: False, <span class="hljs-string">&#x27;params&#x27;</span>: [<span class="hljs-number">2075286132128</span>]&#125;]&#125;<br></code></pre></td></tr></table></figure><h3 id="学习率"><a href="#学习率" class="headerlink" title="学习率"></a>学习率</h3><p>学习率是影响损失函数收敛的重要因素，控制了梯度下降更新的步伐。下面构造一个损失函数$y&#x3D;(2x)^{2}$，$x$的初始值为 2，学习率设置为 1。</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">iter_rec</span>, loss_rec, x_rec = list(), list(), list()<br><br><span class="hljs-attribute">lr</span> = <span class="hljs-number">0</span>.<span class="hljs-number">01</span>    # /<span class="hljs-number">1</span>. /.<span class="hljs-number">5</span> /.<span class="hljs-number">2</span> /.<span class="hljs-number">1</span> /.<span class="hljs-number">125</span><br><span class="hljs-attribute">max_iteration</span> = <span class="hljs-number">20</span>   # /<span class="hljs-number">1</span>. <span class="hljs-number">4</span>     /.<span class="hljs-number">5</span> <span class="hljs-number">4</span>   /.<span class="hljs-number">2</span> <span class="hljs-number">20</span> <span class="hljs-number">200</span><br><br><span class="hljs-attribute">for</span> i in range(max_iteration):<br><br><span class="hljs-attribute">y</span> = func(x)<br><span class="hljs-attribute">y</span>.backward()<br><br><span class="hljs-attribute">print</span>(<span class="hljs-string">&quot;Iter:&#123;&#125;, X:&#123;:8&#125;, X.grad:&#123;:8&#125;, loss:&#123;:10&#125;&quot;</span>.format(<br><span class="hljs-attribute">i</span>, x.detach().numpy()[<span class="hljs-number">0</span>], x.grad.detach().numpy()[<span class="hljs-number">0</span>], y.item()))<br><br><span class="hljs-attribute">x_rec</span>.append(x.item())<br><br><span class="hljs-attribute">x</span>.data.sub_(lr * x.grad)    # x -= x.grad  数学表达式意义:  x = x - x.grad    # <span class="hljs-number">0</span>.<span class="hljs-number">5</span> <span class="hljs-number">0</span>.<span class="hljs-number">2</span> <span class="hljs-number">0</span>.<span class="hljs-number">1</span> <span class="hljs-number">0</span>.<span class="hljs-number">125</span><br><span class="hljs-attribute">x</span>.grad.zero_()<br><br><span class="hljs-attribute">iter_rec</span>.append(i)<br><span class="hljs-attribute">loss_rec</span>.append(y)<br><br><span class="hljs-attribute">plt</span>.subplot(<span class="hljs-number">121</span>).plot(iter_rec, loss_rec, &#x27;-ro&#x27;)<br><span class="hljs-attribute">plt</span>.xlabel(<span class="hljs-string">&quot;Iteration&quot;</span>)<br><span class="hljs-attribute">plt</span>.ylabel(<span class="hljs-string">&quot;Loss value&quot;</span>)<br><br><span class="hljs-attribute">x_t</span> = torch.linspace(-<span class="hljs-number">3</span>, <span class="hljs-number">3</span>, <span class="hljs-number">100</span>)<br><span class="hljs-attribute">y</span> = func(x_t)<br><span class="hljs-attribute">plt</span>.subplot(<span class="hljs-number">122</span>).plot(x_t.numpy(), y.numpy(), label=<span class="hljs-string">&quot;y = 4*x^2&quot;</span>)<br><span class="hljs-attribute">plt</span>.grid()<br><span class="hljs-attribute">y_rec</span> =<span class="hljs-meta"> [func(torch.tensor(i)).item() for i in x_rec]</span><br><span class="hljs-attribute">plt</span>.subplot(<span class="hljs-number">122</span>).plot(x_rec, y_rec, &#x27;-ro&#x27;)<br><span class="hljs-attribute">plt</span>.legend()<br><span class="hljs-attribute">plt</span>.show()<br></code></pre></td></tr></table></figure><p>结果如下：</p><p><img src="/img/pytorchtrain4/11.png"><br> 损失函数没有减少，而是增大，这时因为学习率太大，无法收敛，把学习率设置为 0.01 后，结果如下；</p><p><img src="/img/pytorchtrain4/12.png"><br> 从上面可以看出，适当的学习率可以加快模型的收敛。</p><p>下面的代码是试验 10 个不同的学习率 ，[0.01, 0.5] 之间线性选择 10 个学习率，并比较损失函数的收敛情况</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><code class="hljs python">iteration = <span class="hljs-number">100</span><br>num_lr = <span class="hljs-number">10</span><br>lr_min, lr_max = <span class="hljs-number">0.01</span>, <span class="hljs-number">0.2</span>  <span class="hljs-comment"># .5 .3 .2</span><br><br>lr_list = np.linspace(lr_min, lr_max, num=num_lr).tolist()<br>loss_rec = [[] <span class="hljs-keyword">for</span> l <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(lr_list))]<br>iter_rec = <span class="hljs-built_in">list</span>()<br><br><span class="hljs-keyword">for</span> i, lr <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(lr_list):<br>    x = torch.tensor([<span class="hljs-number">2.</span>], requires_grad=<span class="hljs-literal">True</span>)<br>    <span class="hljs-keyword">for</span> <span class="hljs-built_in">iter</span> <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(iteration):<br><br>        y = func(x)<br>        y.backward()<br>        x.data.sub_(lr * x.grad)  <span class="hljs-comment"># x.data -= x.grad</span><br>        x.grad.zero_()<br><br>        loss_rec[i].append(y.item())<br><br><span class="hljs-keyword">for</span> i, loss_r <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(loss_rec):<br>    plt.plot(<span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(loss_r)), loss_r, label=<span class="hljs-string">&quot;LR: &#123;&#125;&quot;</span>.<span class="hljs-built_in">format</span>(lr_list[i]))<br>plt.legend()<br>plt.xlabel(<span class="hljs-string">&#x27;Iterations&#x27;</span>)<br>plt.ylabel(<span class="hljs-string">&#x27;Loss value&#x27;</span>)<br>plt.show()<br></code></pre></td></tr></table></figure><p>结果如下：</p><p><img src="/img/pytorchtrain4/13.png"><br> 上面的结果表示在学习率较大时，损失函数越来越大，模型不能收敛。把学习率区间改为 [0.01， 0.2] 之后，结果如下：</p><p><img src="/img/pytorchtrain4/14.png"><br> 这个损失函数在学习率为 0.125 时最快收敛，学习率为 0.01 收敛最慢。但是不同模型的最佳学习率不一样，无法事先知道，一般把学习率设置为比较小的数就可以了。</p><h3 id="momentum-动量"><a href="#momentum-动量" class="headerlink" title="momentum 动量"></a>momentum 动量</h3><p>momentum 动量的更新方法，不仅考虑当前的梯度，还会结合前面的梯度。</p><p>momentum 来源于指数加权平均：$\mathrm{v}{t}&#x3D;\boldsymbol{\beta} * \boldsymbol{v}{t-1}+(\mathbf{1}-\boldsymbol{\beta}) * \boldsymbol{\theta}{t}$，其中$v_{t-1}$是上一个时刻的指数加权平均，$\theta_{t}$表示当前时刻的值，$\beta$是系数，一般小于 1。指数加权平均常用于时间序列求平均值。假设现在求得是 100 个时刻的指数加权平均，那么<br><img src="/img/pytorchtrain4/15.png"> </p><p>从上式可以看到，由于$\beta$小于1，越前面时刻的$\theta$，$\beta$的次方就越大，系数就越小。</p><p>$\beta$ 可以理解为记忆周期，$\beta$越小，记忆周期越短，$\beta$越大，记忆周期越长。通常$\beta$设置为 0.9，那么 $\frac{1}{1-\beta}&#x3D;\frac{1}{1-0.9}&#x3D;10$，表示更关注最近 10 天的数据。</p><p>下面代码展示了$\beta&#x3D;0.9$的情况</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs stylus">weights = <span class="hljs-built_in">exp_w_func</span>(beta, time_list)<br><br>plt<span class="hljs-selector-class">.plot</span>(time_list, weights, <span class="hljs-string">&#x27;-ro&#x27;</span>, label=<span class="hljs-string">&quot;Beta: &#123;&#125;\ny = B^t * (1-B)&quot;</span><span class="hljs-selector-class">.format</span>(beta))<br>plt<span class="hljs-selector-class">.xlabel</span>(<span class="hljs-string">&quot;time&quot;</span>)<br>plt<span class="hljs-selector-class">.ylabel</span>(<span class="hljs-string">&quot;weight&quot;</span>)<br>plt<span class="hljs-selector-class">.legend</span>()<br>plt<span class="hljs-selector-class">.title</span>(<span class="hljs-string">&quot;exponentially weighted average&quot;</span>)<br>plt<span class="hljs-selector-class">.show</span>()<br><br><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(np.sum(weights)</span></span>)<br></code></pre></td></tr></table></figure><p>结果为：</p><p><img src="/img/pytorchtrain4/16.png"><br> 下面代码展示了不同的$\beta$取值情况</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs stylus">beta_list = <span class="hljs-selector-attr">[0.98, 0.95, 0.9, 0.8]</span><br>w_list = <span class="hljs-selector-attr">[exp_w_func(beta, time_list) for beta in beta_list]</span><br><span class="hljs-keyword">for</span> <span class="hljs-selector-tag">i</span>, w <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(w_list):<br>    plt<span class="hljs-selector-class">.plot</span>(time_list, w, label=<span class="hljs-string">&quot;Beta: &#123;&#125;&quot;</span><span class="hljs-selector-class">.format</span>(beta_list<span class="hljs-selector-attr">[i]</span>))<br>    plt<span class="hljs-selector-class">.xlabel</span>(<span class="hljs-string">&quot;time&quot;</span>)<br>    plt<span class="hljs-selector-class">.ylabel</span>(<span class="hljs-string">&quot;weight&quot;</span>)<br>plt<span class="hljs-selector-class">.legend</span>()<br>plt<span class="hljs-selector-class">.show</span>()<br></code></pre></td></tr></table></figure><p>结果为：</p><p><img src="/img/pytorchtrain4/17.png"><br> $\beta$的值越大，记忆周期越长，就会更多考虑前面时刻的数值，因此越平缓。</p><p>在 PyTroch 中，momentum 的更新公式是：</p><p>$v_{i}&#x3D;m * v_{i-1}+g\left(w_{i}\right)$ $w_{i+1}&#x3D;w_{i}-l r * v_{i}$</p><p>其中$w_{i+1}$表示第$i+1$次更新的参数，lr 表示学习率，$v_{i}$表示更新量，$m$表示 momentum 系数，$g(w_{i})$表示$w_{i}$的梯度。展开表示如下：</p><p><img src="/img/pytorchtrain4/18.png">  </p><p>下面的代码是构造一个损失函数$y&#x3D;(2x)^{2}$，$x$的初始值为 2，记录每一次梯度下降并画图，学习率使用 0.01 和 0.03，不适用 momentum。</p><figure class="highlight scss"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><code class="hljs scss">def <span class="hljs-built_in">func</span>(x):<br>    return torch.<span class="hljs-built_in">pow</span>(<span class="hljs-number">2</span>*x, <span class="hljs-number">2</span>)    # y = (<span class="hljs-number">2</span>x)^<span class="hljs-number">2</span> = <span class="hljs-number">4</span>*x^<span class="hljs-number">2</span>        dy/dx = <span class="hljs-number">8</span>x<br><br>iteration = <span class="hljs-number">100</span><br>m = <span class="hljs-number">0</span>     # .<span class="hljs-number">9</span> .<span class="hljs-number">63</span><br><br>lr_list = [<span class="hljs-number">0.01</span>, <span class="hljs-number">0.03</span>]<br><br>momentum_list = <span class="hljs-built_in">list</span>()<br>loss_rec = [[] for l in <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(lr_list))]<br>iter_rec = <span class="hljs-built_in">list</span>()<br><br>for i, lr in <span class="hljs-built_in">enumerate</span>(lr_list):<br>    x = torch.<span class="hljs-built_in">tensor</span>([<span class="hljs-number">2</span>.], requires_grad=True)<br><br>    momentum = <span class="hljs-number">0</span>. if lr == <span class="hljs-number">0.03</span> else m<br>    momentum_list.<span class="hljs-built_in">append</span>(momentum)<br><br>    optimizer = optim.<span class="hljs-built_in">SGD</span>([x], lr=lr, momentum=momentum)<br><br>    for iter in <span class="hljs-built_in">range</span>(iteration):<br><br>        y = <span class="hljs-built_in">func</span>(x)<br>        y.<span class="hljs-built_in">backward</span>()<br><br>        optimizer.<span class="hljs-built_in">step</span>()<br>        optimizer.<span class="hljs-built_in">zero_grad</span>()<br><br>        loss_rec[i].<span class="hljs-built_in">append</span>(y.<span class="hljs-built_in">item</span>())<br><br>for i, loss_r in <span class="hljs-built_in">enumerate</span>(loss_rec):<br>    plt.<span class="hljs-built_in">plot</span>(<span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(loss_r)), loss_r, label=<span class="hljs-string">&quot;LR: &#123;&#125; M:&#123;&#125;&quot;</span>.<span class="hljs-built_in">format</span>(lr_list[i], momentum_list[i]))<br>plt.<span class="hljs-built_in">legend</span>()<br>plt.<span class="hljs-built_in">xlabel</span>(<span class="hljs-string">&#x27;Iterations&#x27;</span>)<br>plt.<span class="hljs-built_in">ylabel</span>(<span class="hljs-string">&#x27;Loss value&#x27;</span>)<br>plt.<span class="hljs-built_in">show</span>()<br></code></pre></td></tr></table></figure><p>结果为：</p><p><img src="/img/pytorchtrain4/19.jpg"><br> 可以看到学习率为 0.3 时收敛更快。然后我们把学习率为 0.1 时，设置 momentum 为 0.9，结果<br>虽然设置了 momentum，但是震荡收敛，这是由于 momentum 的值太大，每一次都考虑上一次的比例太多，可以把 momentum 设置为 0.63 后，结果如下：</p><p><img src="/img/pytorchtrain4/20.png"><br>可以看到设置适当的 momentum 后，学习率 0.1 的情况下收敛更快了。</p><p>下面介绍 PyTroch 所提供的 10 种优化器。</p><h3 id="PyTroch-提供的-10-种优化器"><a href="#PyTroch-提供的-10-种优化器" class="headerlink" title="PyTroch 提供的 10 种优化器"></a>PyTroch 提供的 10 种优化器</h3><h4 id="optim-SGD"><a href="#optim-SGD" class="headerlink" title="optim.SGD"></a>optim.SGD</h4><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs routeros">optim.SGD(params, <span class="hljs-attribute">lr</span>=&lt;required parameter&gt;, <span class="hljs-attribute">momentum</span>=0, <span class="hljs-attribute">dampening</span>=0, <span class="hljs-attribute">weight_decay</span>=0, <span class="hljs-attribute">nesterov</span>=<span class="hljs-literal">False</span><br></code></pre></td></tr></table></figure><p>随机梯度下降法</p><p>主要参数：</p><ul><li>params：管理的参数组</li><li>lr：初始学习率</li><li>momentum：动量系数$\beta$</li><li>weight_decay：L2 正则化系数</li><li>nesterov：是否采用 NAG</li></ul><h4 id="optim-Adagrad"><a href="#optim-Adagrad" class="headerlink" title="optim.Adagrad"></a>optim.Adagrad</h4><p>自适应学习率梯度下降法</p><h4 id="optim-RMSprop"><a href="#optim-RMSprop" class="headerlink" title="optim.RMSprop"></a>optim.RMSprop</h4><p>Adagrad 的改进</p><h4 id="optim-Adadelta"><a href="#optim-Adadelta" class="headerlink" title="optim.Adadelta"></a>optim.Adadelta</h4><h4 id="optim-Adam"><a href="#optim-Adam" class="headerlink" title="optim.Adam"></a>optim.Adam</h4><p>RMSProp 集合 Momentum，这个是目前最常用的优化器，因为它可以使用较大的初始学习率。</p><h4 id="optim-Adamax"><a href="#optim-Adamax" class="headerlink" title="optim.Adamax"></a>optim.Adamax</h4><p>Adam 增加学习率上限</p><h4 id="optim-SparseAdam"><a href="#optim-SparseAdam" class="headerlink" title="optim.SparseAdam"></a>optim.SparseAdam</h4><p>稀疏版的 Adam</p><h4 id="optim-ASGD"><a href="#optim-ASGD" class="headerlink" title="optim.ASGD"></a>optim.ASGD</h4><p>随机平均梯度下降</p><h4 id="optim-Rprop"><a href="#optim-Rprop" class="headerlink" title="optim.Rprop"></a>optim.Rprop</h4><p>弹性反向传播，这种优化器通常是在所有样本都一起训练，也就是 batchsize 为全部样本时使用。</p><h4 id="optim-LBFGS"><a href="#optim-LBFGS" class="headerlink" title="optim.LBFGS"></a>optim.LBFGS</h4><p>BFGS 在内存上的改进</p>]]></content>
    
    
    <categories>
      
      <category>深度学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>pytorch</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>深度学习-10-pytorch-3-模型构建</title>
    <link href="/2021/08/11/pytorchtrain3/"/>
    <url>/2021/08/11/pytorchtrain3/</url>
    
    <content type="html"><![CDATA[<blockquote><p>创作声明：主要内容参考于张贤同学<a href="https://zhuanlan.zhihu.com/p/265394674">https://zhuanlan.zhihu.com/p/265394674</a></p></blockquote><p>这篇文章来看下 PyTorch 中网络模型的实现步骤。网络模型的内容如下，包括模型创建和权值初始化，这些内容都在nn.Module中有实现。</p><p><img src="/img/pytorchtrain3/1.png"></p><h2 id="模型创建"><a href="#模型创建" class="headerlink" title="模型创建"></a>模型创建</h2><p>创建模型有 2 个要素：构建子模块和拼接子模块。如 LeNet 里包含很多卷积层、池化层、全连接层，当我们构建好所有的子模块之后，按照一定的顺序拼接起来。<br><img src="/img/pytorchtrain3/2.png"><br>这里以上一篇文章中 <code>lenet.py</code>的 LeNet 为例，继承<code>nn.Module</code>，必须实现<code>__init__()</code> 方法和<code>forward()</code>方法。其中<code>__init__() </code>方法里创建子模块，在<code>forward()</code>方法里拼接子模块。</p><figure class="highlight scss"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs scss">class <span class="hljs-built_in">LeNet</span>(nn.Module):<br>    # 子模块创建<br>    def <span class="hljs-built_in">__init__</span>(self, classes):<br>        <span class="hljs-built_in">super</span>(LeNet, self).<span class="hljs-built_in">__init__</span>()<br>        self.conv1 = nn.<span class="hljs-built_in">Conv2d</span>(<span class="hljs-number">3</span>, <span class="hljs-number">6</span>, <span class="hljs-number">5</span>)<br>        self.conv2 = nn.<span class="hljs-built_in">Conv2d</span>(<span class="hljs-number">6</span>, <span class="hljs-number">16</span>, <span class="hljs-number">5</span>)<br>        self.fc1 = nn.<span class="hljs-built_in">Linear</span>(<span class="hljs-number">16</span>*<span class="hljs-number">5</span>*<span class="hljs-number">5</span>, <span class="hljs-number">120</span>)<br>        self.fc2 = nn.<span class="hljs-built_in">Linear</span>(<span class="hljs-number">120</span>, <span class="hljs-number">84</span>)<br>        self.fc3 = nn.<span class="hljs-built_in">Linear</span>(<span class="hljs-number">84</span>, classes)<br>    # 子模块拼接<br>    def <span class="hljs-built_in">forward</span>(self, x):<br>        out = F.<span class="hljs-built_in">relu</span>(self.<span class="hljs-built_in">conv1</span>(x))<br>        out = F.<span class="hljs-built_in">max_pool2d</span>(out, <span class="hljs-number">2</span>)<br>        out = F.<span class="hljs-built_in">relu</span>(self.<span class="hljs-built_in">conv2</span>(out))<br>        out = F.<span class="hljs-built_in">max_pool2d</span>(out, <span class="hljs-number">2</span>)<br>        out = out.<span class="hljs-built_in">view</span>(out.<span class="hljs-built_in">size</span>(<span class="hljs-number">0</span>), -<span class="hljs-number">1</span>)<br>        out = F.<span class="hljs-built_in">relu</span>(self.<span class="hljs-built_in">fc1</span>(out))<br>        out = F.<span class="hljs-built_in">relu</span>(self.<span class="hljs-built_in">fc2</span>(out))<br>        out = self.<span class="hljs-built_in">fc3</span>(out)<br>        return out<br></code></pre></td></tr></table></figure><p>当我们调用<code>net = LeNet(classes=2)</code>创建模型时，会调用<code>__init__()</code>方法创建模型的子模块。</p><p>当我们在训练时调用outputs &#x3D; net(inputs)时，会进入module.py的call()函数中：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">__call__</span>(<span class="hljs-params">self, *<span class="hljs-built_in">input</span>, **kwargs</span>):<br>    <span class="hljs-keyword">for</span> hook <span class="hljs-keyword">in</span> <span class="hljs-variable language_">self</span>._forward_pre_hooks.values():<br>        result = hook(<span class="hljs-variable language_">self</span>, <span class="hljs-built_in">input</span>)<br>        <span class="hljs-keyword">if</span> result <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>            <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> <span class="hljs-built_in">isinstance</span>(result, <span class="hljs-built_in">tuple</span>):<br>                result = (result,)<br>            <span class="hljs-built_in">input</span> = result<br>    <span class="hljs-keyword">if</span> torch._C._get_tracing_state():<br>        result = <span class="hljs-variable language_">self</span>._slow_forward(*<span class="hljs-built_in">input</span>, **kwargs)<br>    <span class="hljs-keyword">else</span>:<br>        result = <span class="hljs-variable language_">self</span>.forward(*<span class="hljs-built_in">input</span>, **kwargs)<br>    ...<br>    ...<br>    ...<br></code></pre></td></tr></table></figure><p>最终会调用result &#x3D; self.forward(*input, **kwargs)函数，该函数会进入模型的forward()函数中，进行前向传播。</p><p>在 torch.nn中包含 4 个模块，如下图所示。<br><img src="/img/pytorchtrain3/3.png"><br>其中所有网络模型都是继承于nn.Module的，下面重点分析nn.Module模块。</p><h2 id="nn-Module"><a href="#nn-Module" class="headerlink" title="nn.Module"></a>nn.Module</h2><p>nn.Module 有 8 个属性，都是OrderDict(有序字典)。在 LeNet 的__init__()方法中会调用父类nn.Module的__init__()方法，创建这 8 个属性。</p><figure class="highlight isbl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs isbl"><span class="hljs-variable">def</span> <span class="hljs-function"><span class="hljs-title">__init__</span>(<span class="hljs-variable">self</span>):</span><br><span class="hljs-function">    <span class="hljs-string">&quot;&quot;</span><span class="hljs-string">&quot;</span></span><br><span class="hljs-string"><span class="hljs-function">    Initializes internal Module state, shared by both nn.Module and ScriptModule.</span></span><br><span class="hljs-string"><span class="hljs-function">    &quot;</span><span class="hljs-string">&quot;&quot;</span></span><br><span class="hljs-function">    <span class="hljs-variable">torch._C._log_api_usage_once</span>(<span class="hljs-string">&quot;python.nn_module&quot;</span>)</span><br><br>    <span class="hljs-variable">self.training</span> = <span class="hljs-variable"><span class="hljs-literal">True</span></span><br>    <span class="hljs-variable">self._parameters</span> = <span class="hljs-function"><span class="hljs-title">OrderedDict</span>()</span><br>    <span class="hljs-variable">self._buffers</span> = <span class="hljs-function"><span class="hljs-title">OrderedDict</span>()</span><br>    <span class="hljs-variable">self._backward_hooks</span> = <span class="hljs-function"><span class="hljs-title">OrderedDict</span>()</span><br>    <span class="hljs-variable">self._forward_hooks</span> = <span class="hljs-function"><span class="hljs-title">OrderedDict</span>()</span><br>    <span class="hljs-variable">self._forward_pre_hooks</span> = <span class="hljs-function"><span class="hljs-title">OrderedDict</span>()</span><br>    <span class="hljs-variable">self._state_dict_hooks</span> = <span class="hljs-function"><span class="hljs-title">OrderedDict</span>()</span><br>    <span class="hljs-variable">self._load_state_dict_pre_hooks</span> = <span class="hljs-function"><span class="hljs-title">OrderedDict</span>()</span><br>    <span class="hljs-variable">self._modules</span> = <span class="hljs-function"><span class="hljs-title">OrderedDict</span>()</span><br></code></pre></td></tr></table></figure><ul><li>_parameters 属性：存储管理 nn.Parameter 类型的参数</li><li>_modules 属性：存储管理 nn.Module 类型的参数</li><li>_buffers 属性：存储管理缓冲属性，如 BN 层中的 running_mean</li><li>5 个 *_hooks 属性：存储管理钩子函数</li></ul><p>其中比较重要的是parameters和modules属性。</p><p>在 LeNet 的__init__()中创建了 5 个子模块，nn.Conv2d()和nn.Linear()都是 继承于nn.module，也就是说一个 module 都是包含多个子 module 的。</p><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs ruby"><span class="hljs-keyword">class</span> <span class="hljs-title class_">LeNet</span>(nn.<span class="hljs-title class_">Module</span>):<br>    <span class="hljs-comment"># 子模块创建</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params"><span class="hljs-variable language_">self</span>, classes</span>):<br>        <span class="hljs-variable language_">super</span>(<span class="hljs-title class_">LeNet</span>, <span class="hljs-variable language_">self</span>).__init__()<br>        <span class="hljs-variable language_">self</span>.conv1 = nn.<span class="hljs-title class_">Conv2d</span>(<span class="hljs-number">3</span>, <span class="hljs-number">6</span>, <span class="hljs-number">5</span>)<br>        <span class="hljs-variable language_">self</span>.conv2 = nn.<span class="hljs-title class_">Conv2d</span>(<span class="hljs-number">6</span>, <span class="hljs-number">16</span>, <span class="hljs-number">5</span>)<br>        <span class="hljs-variable language_">self</span>.fc1 = nn.<span class="hljs-title class_">Linear</span>(<span class="hljs-number">16</span>*<span class="hljs-number">5</span>*<span class="hljs-number">5</span>, <span class="hljs-number">120</span>)<br>        <span class="hljs-variable language_">self</span>.fc2 = nn.<span class="hljs-title class_">Linear</span>(<span class="hljs-number">120</span>, <span class="hljs-number">84</span>)<br>        <span class="hljs-variable language_">self</span>.fc3 = nn.<span class="hljs-title class_">Linear</span>(<span class="hljs-number">84</span>, classes)<br>        ...<br>        ...<br>        ...<br></code></pre></td></tr></table></figure><p>当调用net &#x3D; LeNet(classes&#x3D;2)创建模型后，net对象的 modules 属性就包含了这 5 个子网络模块。<br><img src="/img/pytorchtrain3/4.png"><br> 下面看下每个子模块是如何添加到 LeNet 的_modules 属性中的。以self.conv1 &#x3D; nn.Conv2d(3, 6, 5)为例，当我们运行到这一行时，首先 Step Into 进入 Conv2d的构造，然后 Step Out。右键Evaluate Expression查看nn.Conv2d(3, 6, 5)的属性。<br><img src="/img/pytorchtrain3/5.png"><br> 上面说了Conv2d也是一个 module，里面的_modules属性为空，_parameters属性里包含了该卷积层的可学习参数，这些参数的类型是 Parameter，继承自 Tensor。</p><p>此时只是完成了nn.Conv2d(3, 6, 5) module 的创建。还没有赋值给self.conv1。在nn.Module里有一个机制，会拦截所有的类属性赋值操作(self.conv1是类属性)，进入到__setattr__()函数中。我们再次 Step Into 就可以进入__setattr__()。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">__setattr__</span>(<span class="hljs-params">self, name, value</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">remove_from</span>(<span class="hljs-params">*dicts</span>):<br>        <span class="hljs-keyword">for</span> d <span class="hljs-keyword">in</span> dicts:<br>            <span class="hljs-keyword">if</span> name <span class="hljs-keyword">in</span> d:<br>                <span class="hljs-keyword">del</span> d[name]<br><br>    params = <span class="hljs-variable language_">self</span>.__dict__.get(<span class="hljs-string">&#x27;_parameters&#x27;</span>)<br>    <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(value, Parameter):<br>        <span class="hljs-keyword">if</span> params <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>            <span class="hljs-keyword">raise</span> AttributeError(<br>                <span class="hljs-string">&quot;cannot assign parameters before Module.__init__() call&quot;</span>)<br>        remove_from(<span class="hljs-variable language_">self</span>.__dict__, <span class="hljs-variable language_">self</span>._buffers, <span class="hljs-variable language_">self</span>._modules)<br>        <span class="hljs-variable language_">self</span>.register_parameter(name, value)<br>    <span class="hljs-keyword">elif</span> params <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span> <span class="hljs-keyword">and</span> name <span class="hljs-keyword">in</span> params:<br>        <span class="hljs-keyword">if</span> value <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>            <span class="hljs-keyword">raise</span> TypeError(<span class="hljs-string">&quot;cannot assign &#x27;&#123;&#125;&#x27; as parameter &#x27;&#123;&#125;&#x27; &quot;</span><br>                            <span class="hljs-string">&quot;(torch.nn.Parameter or None expected)&quot;</span><br>                            .<span class="hljs-built_in">format</span>(torch.typename(value), name))<br>        <span class="hljs-variable language_">self</span>.register_parameter(name, value)<br>    <span class="hljs-keyword">else</span>:<br>        modules = <span class="hljs-variable language_">self</span>.__dict__.get(<span class="hljs-string">&#x27;_modules&#x27;</span>)<br>        <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(value, Module):<br>            <span class="hljs-keyword">if</span> modules <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>                <span class="hljs-keyword">raise</span> AttributeError(<br>                    <span class="hljs-string">&quot;cannot assign module before Module.__init__() call&quot;</span>)<br>            remove_from(<span class="hljs-variable language_">self</span>.__dict__, <span class="hljs-variable language_">self</span>._parameters, <span class="hljs-variable language_">self</span>._buffers)<br>            modules[name] = value<br>        <span class="hljs-keyword">elif</span> modules <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span> <span class="hljs-keyword">and</span> name <span class="hljs-keyword">in</span> modules:<br>            <span class="hljs-keyword">if</span> value <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>                <span class="hljs-keyword">raise</span> TypeError(<span class="hljs-string">&quot;cannot assign &#x27;&#123;&#125;&#x27; as child module &#x27;&#123;&#125;&#x27; &quot;</span><br>                                <span class="hljs-string">&quot;(torch.nn.Module or None expected)&quot;</span><br>                                .<span class="hljs-built_in">format</span>(torch.typename(value), name))<br>            modules[name] = value<br>        ...<br>        ...<br>        ...<br></code></pre></td></tr></table></figure><p>在这里判断 value 的类型是Parameter还是Module，存储到对应的有序字典中。</p><p>这里nn.Conv2d(3, 6, 5)的类型是Module，因此会执行modules[name] &#x3D; value，key 是类属性的名字conv1，value 就是nn.Conv2d(3, 6, 5)。</p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><ul><li>一个 module 里可包含多个子 module。比如 LeNet 是一个 Module，里面包括多个卷积层、池化层、全连接层等子 module</li><li>一个 module 相当于一个运算，必须实现 forward() 函数</li><li>每个 module 都有 8 个字典管理自己的属性</li></ul><h2 id="模型容器"><a href="#模型容器" class="headerlink" title="模型容器"></a>模型容器</h2><p>除了上述的模块之外，还有一个重要的概念是模型容器 (Containers)，常用的容器有 3 个，这些容器都是继承自nn.Module。</p><ul><li>nn.Sequetial：按照顺序包装多个网络层</li><li>nn.ModuleList：像 python 的 list 一样包装多个网络层，可以迭代</li><li>nn.ModuleDict：像 python 的 dict一样包装多个网络层，通过 (key, value) 的方式为每个网络层指定名称。</li></ul><h3 id="nn-Sequetial"><a href="#nn-Sequetial" class="headerlink" title="nn.Sequetial"></a>nn.Sequetial</h3><p>在传统的机器学习中，有一个步骤是特征工程，我们需要从数据中人为地提取特征，然后把特征输入到分类器中预测。在深度学习的时代，特征工程的概念被弱化了，特征提取和分类器这两步被融合到了一个神经网络中。在卷积神经网络中，前面的卷积层以及池化层可以认为是特征提取部分，而后面的全连接层可以认为是分类器部分。比如 LeNet 就可以分为特征提取和分类器两部分，这 2 部分都可以分别使用 nn.Seuqtial 来包装。<br><img src="/img/pytorchtrain3/6.png"></p><p>代码如下：</p><figure class="highlight haskell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><code class="hljs haskell"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-type">LeNetSequetial</span>(<span class="hljs-title">nn</span>.<span class="hljs-type">Module</span>):</span><br><span class="hljs-class">    def __init__(<span class="hljs-title">self</span>, <span class="hljs-title">classes</span>):</span><br><span class="hljs-class">        super(<span class="hljs-type">LeNet2</span>, <span class="hljs-title">self</span>).__init__()</span><br><span class="hljs-class">        self.features = nn.<span class="hljs-type">Sequential</span>(</span><br><span class="hljs-class">            <span class="hljs-title">nn</span>.<span class="hljs-type">Conv2d</span>(3, 6, 5),</span><br><span class="hljs-class">            nn.<span class="hljs-type">ReLU</span>(),</span><br><span class="hljs-class">            nn.<span class="hljs-type">AvgPool2d</span>(2, 2),</span><br><span class="hljs-class">            nn.<span class="hljs-type">Conv2d</span>(6, 16, 5),</span><br><span class="hljs-class">            nn.<span class="hljs-type">ReLU</span>(),</span><br><span class="hljs-class">            nn.<span class="hljs-type">AvgPool2d</span>(2, 2)</span><br><span class="hljs-class">        )</span><br><span class="hljs-class">        self.classifier = nn.<span class="hljs-type">Sequential</span>(</span><br><span class="hljs-class">            <span class="hljs-title">nn</span>.<span class="hljs-type">Linear</span>(16*5*5, 120),</span><br><span class="hljs-class">            nn.<span class="hljs-type">ReLU</span>(),</span><br><span class="hljs-class">            nn.<span class="hljs-type">Linear</span>(120, 84),</span><br><span class="hljs-class">            nn.<span class="hljs-type">ReLU</span>(),</span><br><span class="hljs-class">            nn.<span class="hljs-type">Linear</span>(84, <span class="hljs-title">classes</span>)</span><br><span class="hljs-class">        )</span><br><span class="hljs-class"></span><br><span class="hljs-class">    def forward(<span class="hljs-title">self</span>, <span class="hljs-title">x</span>):</span><br><span class="hljs-class">        x = self.features(<span class="hljs-title">x</span>)</span><br><span class="hljs-class">        x = x.view(<span class="hljs-title">x</span>.<span class="hljs-title">size</span>()[0], -1)</span><br><span class="hljs-class">        x = self.classifier(<span class="hljs-title">x</span>)</span><br><span class="hljs-class">        return x</span><br></code></pre></td></tr></table></figure><p>在初始化时，nn.Sequetial会调用__init__()方法，将每一个子 module 添加到 自身的_modules属性中。这里可以看到，我们传入的参数可以是一个 list，或者一个 OrderDict。如果是一个 OrderDict，那么则使用 OrderDict 里的 key，否则使用数字作为 key (OrderDict 的情况会在下面提及)。</p><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs ruby"><span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params"><span class="hljs-variable language_">self</span>, *args</span>):<br>    <span class="hljs-variable language_">super</span>(<span class="hljs-title class_">Sequential</span>, <span class="hljs-variable language_">self</span>).__init__()<br>    <span class="hljs-keyword">if</span> len(args) == <span class="hljs-number">1</span> <span class="hljs-keyword">and</span> isinstance(args[<span class="hljs-number">0</span>], <span class="hljs-title class_">OrderedDict</span>):<br>        <span class="hljs-keyword">for</span> key, <span class="hljs-keyword">module</span> <span class="hljs-keyword">in</span> args[<span class="hljs-number">0</span>].items():<br>            <span class="hljs-variable language_">self</span>.add_module(key, <span class="hljs-keyword">module</span>)<br>    <span class="hljs-symbol">else:</span><br>        <span class="hljs-keyword">for</span> idx, <span class="hljs-keyword">module</span> <span class="hljs-keyword">in</span> enumerate(args):<br>            <span class="hljs-variable language_">self</span>.add_module(str(idx), <span class="hljs-keyword">module</span>)<br></code></pre></td></tr></table></figure><p>网络初始化完成后有两个子 module：features和classifier。</p><p>在进行前向传播时，会进入 LeNet 的forward()函数，首先调用第一个Sequetial容器：self.features，由于self.features也是一个 module，因此会调用__call__()函数，里面调用 result &#x3D; self.forward(*input, **kwargs)，进入nn.Seuqetial的forward()函数，在这里依次调用所有的 module。</p><figure class="highlight lua"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs lua">def forward(<span class="hljs-built_in">self</span>, <span class="hljs-built_in">input</span>):<br>    <span class="hljs-keyword">for</span> <span class="hljs-built_in">module</span> <span class="hljs-keyword">in</span> <span class="hljs-built_in">self</span>:<br>        <span class="hljs-built_in">input</span> = <span class="hljs-built_in">module</span>(<span class="hljs-built_in">input</span>)<br>    <span class="hljs-keyword">return</span> <span class="hljs-built_in">input</span><br></code></pre></td></tr></table></figure><p>在nn.Sequetial中，里面的每个子网络层 module 是使用序号来索引的，即使用数字来作为 key。一旦网络层增多，难以查找特定的网络层，这种情况可以使用 OrderDict (有序字典)。代码中使用</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><code class="hljs routeros">class LeNetSequentialOrderDict(nn.Module):<br>    def __init__(self, classes):<br>        super(LeNetSequentialOrderDict, self).__init__()<br><br>        self.features = nn.Sequential(OrderedDict(&#123;<br>            <span class="hljs-string">&#x27;conv1&#x27;</span>: nn.Conv2d(3, 6, 5),<br>            <span class="hljs-string">&#x27;relu1&#x27;</span>: nn.ReLU(<span class="hljs-attribute">inplace</span>=<span class="hljs-literal">True</span>),<br>            <span class="hljs-string">&#x27;pool1&#x27;</span>: nn.MaxPool2d(<span class="hljs-attribute">kernel_size</span>=2, <span class="hljs-attribute">stride</span>=2),<br><br>            <span class="hljs-string">&#x27;conv2&#x27;</span>: nn.Conv2d(6, 16, 5),<br>            <span class="hljs-string">&#x27;relu2&#x27;</span>: nn.ReLU(<span class="hljs-attribute">inplace</span>=<span class="hljs-literal">True</span>),<br>            <span class="hljs-string">&#x27;pool2&#x27;</span>: nn.MaxPool2d(<span class="hljs-attribute">kernel_size</span>=2, <span class="hljs-attribute">stride</span>=2),<br>        &#125;))<br><br>        self.classifier = nn.Sequential(OrderedDict(&#123;<br>            <span class="hljs-string">&#x27;fc1&#x27;</span>: nn.Linear(16<span class="hljs-number">*5</span><span class="hljs-number">*5</span>, 120),<br>            <span class="hljs-string">&#x27;relu3&#x27;</span>: nn.ReLU(),<br><br>            <span class="hljs-string">&#x27;fc2&#x27;</span>: nn.Linear(120, 84),<br>            <span class="hljs-string">&#x27;relu4&#x27;</span>: nn.ReLU(<span class="hljs-attribute">inplace</span>=<span class="hljs-literal">True</span>),<br><br>            <span class="hljs-string">&#x27;fc3&#x27;</span>: nn.Linear(84, classes),<br>        &#125;))<br>        <span class="hljs-built_in">..</span>.<br>        <span class="hljs-built_in">..</span>.<br>        <span class="hljs-built_in">..</span>.<br></code></pre></td></tr></table></figure><p>总结<br>nn.Sequetial是nn.Module的容器，用于按顺序包装一组网络层，有以下两个特性。</p><ul><li>顺序性：各网络层之间严格按照顺序构建，我们在构建网络时，一定要注意前后网络层之间输入和输出数据之间的形状是否匹配</li><li>自带forward()函数：在nn.Sequetial的forward()函数里通过 for 循环依次读取每个网络层，执行前向传播运算。这使得我们我们构建的模型更加简洁</li></ul><h3 id="nn-ModuleList"><a href="#nn-ModuleList" class="headerlink" title="nn.ModuleList"></a>nn.ModuleList</h3><p>nn.ModuleList是nn.Module的容器，用于包装一组网络层，以迭代的方式调用网络层，主要有以下 3 个方法：</p><ul><li>append()：在 ModuleList 后面添加网络层</li><li>extend()：拼接两个 ModuleList</li><li>insert()：在 ModuleList 的指定位置中插入网络层</li></ul><p>下面的代码通过列表生成式来循环迭代创建 20 个全连接层，非常方便，只是在 forward()函数中需要手动调用每个网络层。</p><figure class="highlight haskell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs haskell"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-type">ModuleList</span>(<span class="hljs-title">nn</span>.<span class="hljs-type">Module</span>):</span><br><span class="hljs-class">    def __init__(<span class="hljs-title">self</span>):</span><br><span class="hljs-class">        super(<span class="hljs-type">ModuleList</span>, <span class="hljs-title">self</span>).__init__()</span><br><span class="hljs-class">        self.linears = nn.<span class="hljs-type">ModuleList</span>([<span class="hljs-title">nn</span>.<span class="hljs-type">Linear</span>(10, 10) for i in range(20)])</span><br><span class="hljs-class"></span><br><span class="hljs-class">    def forward(<span class="hljs-title">self</span>, <span class="hljs-title">x</span>):</span><br><span class="hljs-class">        for i, linear in enumerate(<span class="hljs-title">self</span>.<span class="hljs-title">linears</span>):</span><br><span class="hljs-class">            x = linear(<span class="hljs-title">x</span>)</span><br><span class="hljs-class">        return x</span><br><span class="hljs-class"></span><br><span class="hljs-class"></span><br><span class="hljs-class">net = <span class="hljs-type">ModuleList</span>()</span><br><span class="hljs-class"></span><br><span class="hljs-class">print(<span class="hljs-title">net</span>)</span><br><span class="hljs-class"></span><br><span class="hljs-class">fake_data = torch.ones((10, 10))</span><br><span class="hljs-class"></span><br><span class="hljs-class">output = net(<span class="hljs-title">fake_data</span>)</span><br><span class="hljs-class"></span><br><span class="hljs-class">print(<span class="hljs-title">output</span>)</span><br></code></pre></td></tr></table></figure><h3 id="nn-ModuleDict"><a href="#nn-ModuleDict" class="headerlink" title="nn.ModuleDict"></a>nn.ModuleDict</h3><p>nn.ModuleDict是nn.Module的容器，用于包装一组网络层，以索引的方式调用网络层，主要有以下 5 个方法：</p><ul><li>clear()：清空  ModuleDict</li><li>items()：返回可迭代的键值对 (key, value)</li><li>keys()：返回字典的所有 key</li><li>values()：返回字典的所有 value</li><li>pop()：返回一对键值，并从字典中删除</li></ul><p>下面的模型创建了两个ModuleDict：self.choices和self.activations，在前向传播时通过传入对应的 key 来执行对应的网络层。</p><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><code class="hljs ruby"><span class="hljs-keyword">class</span> <span class="hljs-title class_">ModuleDict</span>(nn.<span class="hljs-title class_">Module</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params"><span class="hljs-variable language_">self</span></span>):<br>        <span class="hljs-variable language_">super</span>(<span class="hljs-title class_">ModuleDict</span>, <span class="hljs-variable language_">self</span>).__init__()<br>        <span class="hljs-variable language_">self</span>.choices = nn.<span class="hljs-title class_">ModuleDict</span>(&#123;<br>            <span class="hljs-string">&#x27;conv&#x27;</span>: nn.<span class="hljs-title class_">Conv2d</span>(<span class="hljs-number">10</span>, <span class="hljs-number">10</span>, <span class="hljs-number">3</span>),<br>            <span class="hljs-string">&#x27;pool&#x27;</span>: nn.<span class="hljs-title class_">MaxPool2d</span>(<span class="hljs-number">3</span>)<br>        &#125;)<br><br>        <span class="hljs-variable language_">self</span>.activations = nn.<span class="hljs-title class_">ModuleDict</span>(&#123;<br>            <span class="hljs-string">&#x27;relu&#x27;</span>: nn.<span class="hljs-title class_">Re</span>LU(),<br>            <span class="hljs-string">&#x27;prelu&#x27;</span>: nn.<span class="hljs-title class_">PRe</span>LU()<br>        &#125;)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params"><span class="hljs-variable language_">self</span>, x, choice, act</span>):<br>        x = <span class="hljs-variable language_">self</span>.choices[choice](x)<br>        x = <span class="hljs-variable language_">self</span>.activations[act](x)<br>        <span class="hljs-keyword">return</span> x<br><br><br>net = <span class="hljs-title class_">ModuleDict</span>()<br><br>fake_img = torch.randn((<span class="hljs-number">4</span>, <span class="hljs-number">10</span>, <span class="hljs-number">32</span>, <span class="hljs-number">32</span>))<br><br>output = net(fake_img, <span class="hljs-string">&#x27;conv&#x27;</span>, <span class="hljs-string">&#x27;relu&#x27;</span>)<br><span class="hljs-comment"># output = net(fake_img, &#x27;conv&#x27;, &#x27;prelu&#x27;)</span><br>print(output)<br></code></pre></td></tr></table></figure><h3 id="容器总结"><a href="#容器总结" class="headerlink" title="容器总结"></a>容器总结</h3><ul><li>nn.Sequetial：顺序性，各网络层之间严格按照顺序执行，常用于 block 构建，在前向传播时的代码调用变得简洁</li><li>nn.ModuleList：迭代行，常用于大量重复网络构建，通过 for 循环实现重复构建</li><li>nn.ModuleDict：索引性，常用于可选择的网络层</li></ul><h2 id="卷积层"><a href="#卷积层" class="headerlink" title="卷积层"></a>卷积层</h2><h3 id="1D-2D-3D-卷积"><a href="#1D-2D-3D-卷积" class="headerlink" title="1D&#x2F;2D&#x2F;3D 卷积"></a>1D&#x2F;2D&#x2F;3D 卷积</h3><p>卷积有一维卷积、二维卷积、三维卷积。一般情况下，卷积核在几个维度上滑动，就是几维卷积。比如在图片上的卷积就是二维卷积。</p><h4 id="一维卷积"><a href="#一维卷积" class="headerlink" title="一维卷积"></a>一维卷积</h4><p><img src="/img/pytorchtrain3/1d.gif"></p><h4 id="二维卷积"><a href="#二维卷积" class="headerlink" title="二维卷积"></a>二维卷积</h4><p><img src="/img/pytorchtrain3/2d.gif"></p><h4 id="三维卷积"><a href="#三维卷积" class="headerlink" title="三维卷积"></a>三维卷积</h4><p><img src="/img/pytorchtrain3/3d.gif"></p><h3 id="二维卷积：nn-Conv2d"><a href="#二维卷积：nn-Conv2d" class="headerlink" title="二维卷积：nn.Conv2d()"></a>二维卷积：nn.Conv2d()</h3><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs routeros">nn.Conv2d(self, in_channels, out_channels, kernel_size, <span class="hljs-attribute">stride</span>=1,<br>                 <span class="hljs-attribute">padding</span>=0, <span class="hljs-attribute">dilation</span>=1, <span class="hljs-attribute">groups</span>=1,<br>                 <span class="hljs-attribute">bias</span>=<span class="hljs-literal">True</span>, <span class="hljs-attribute">padding_mode</span>=<span class="hljs-string">&#x27;zeros&#x27;</span>)<br></code></pre></td></tr></table></figure><p>这个函数的功能是对多个二维信号进行二维卷积，主要参数如下：</p><ul><li>in_channels：输入通道数</li><li>out_channels：输出通道数，等价于卷积核个数</li><li>kernel_size：卷积核尺寸</li><li>stride：步长</li><li>padding：填充宽度，主要是为了调整输出的特征图大小，一般把 padding 设置合适的值后，保持输入和输出的图像尺寸不变。</li><li>dilation：空洞卷积大小，默认为1，这时是标准卷积，常用于图像分割任务中，主要是为了提升感受野</li><li>groups：分组卷积设置，主要是为了模型的轻量化，如在 ShuffleNet、MobileNet、SqueezeNet中用到</li><li>bias：偏置</li></ul><h3 id="卷积尺寸计算"><a href="#卷积尺寸计算" class="headerlink" title="卷积尺寸计算"></a>卷积尺寸计算</h3><h4 id="简化版卷积尺寸计算"><a href="#简化版卷积尺寸计算" class="headerlink" title="简化版卷积尺寸计算"></a>简化版卷积尺寸计算</h4><p>这里不考虑空洞卷积，假设输入图片大小为 $I \times I$，卷积核大小为 $k \times k$，stride 为 $s$，padding 的像素数为 $p$，图片经过卷积之后的尺寸 $O$ 如下：<br>$O &#x3D; \displaystyle\frac{I -k + 2 \times p}{s} +1$<br>下面例子的输入图片大小为 $5 \times 5$，卷积大小为 $3 \times 3$，stride 为 1，padding 为 0，所以输出图片大小为 $\displaystyle\frac{5 -3 + 2 \times 0}{1} +1 &#x3D; 3$。</p><h4 id="完整版卷积尺寸计算"><a href="#完整版卷积尺寸计算" class="headerlink" title="完整版卷积尺寸计算"></a>完整版卷积尺寸计算</h4><p>完整版卷积尺寸计算考虑了空洞卷积，假设输入图片大小为 $I \times I$，卷积核大小为 $k \times k$，stride 为 $s$，padding 的像素数为 $p$，dilation 为 $d$，图片经过卷积之后的尺寸 $O$ 如下：。<br>$O &#x3D; \displaystyle\frac{I - d \times (k-1) + 2 \times p -1}{s} +1$</p><h3 id="卷积网络示例（非完整训练）"><a href="#卷积网络示例（非完整训练）" class="headerlink" title="卷积网络示例（非完整训练）"></a>卷积网络示例（非完整训练）</h3><p>这里使用 inputchannel 为 3，output_channel 为 1 ，卷积核大小为 $3 \times 3$ 的卷积核nn.Conv2d(3, 1, 3)，使用<code>nn.init.xavier_normal()</code>方法初始化网络的权值。代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> os<br><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<br><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image<br><span class="hljs-keyword">from</span> torchvision <span class="hljs-keyword">import</span> transforms<br><span class="hljs-keyword">from</span> matplotlib <span class="hljs-keyword">import</span> pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-keyword">from</span> common_tools <span class="hljs-keyword">import</span> transform_invert, set_seed<br><br>set_seed(<span class="hljs-number">3</span>)  <span class="hljs-comment"># 设置随机种子</span><br><br><span class="hljs-comment"># ================================= load img ==================================</span><br>path_img = os.path.join(os.path.dirname(os.path.abspath(__file__)), <span class="hljs-string">&quot;imgs&quot;</span>, <span class="hljs-string">&quot;lena.png&quot;</span>)<br><span class="hljs-built_in">print</span>(path_img)<br>img = Image.<span class="hljs-built_in">open</span>(path_img).convert(<span class="hljs-string">&#x27;RGB&#x27;</span>)  <span class="hljs-comment"># 0~255</span><br><br><span class="hljs-comment"># convert to tensor</span><br>img_transform = transforms.Compose([transforms.ToTensor()])<br>img_tensor = img_transform(img)<br><span class="hljs-comment"># 添加 batch 维度</span><br>img_tensor.unsqueeze_(dim=<span class="hljs-number">0</span>)    <span class="hljs-comment"># C*H*W to B*C*H*W</span><br><br><span class="hljs-comment"># ================================= create convolution layer ==================================</span><br><br><span class="hljs-comment"># ================ 2d</span><br>flag = <span class="hljs-number">1</span><br><span class="hljs-comment"># flag = 0</span><br><span class="hljs-keyword">if</span> flag:<br>    conv_layer = nn.Conv2d(<span class="hljs-number">3</span>, <span class="hljs-number">1</span>, <span class="hljs-number">3</span>)   <span class="hljs-comment"># input:(i, o, size) weights:(o, i , h, w)</span><br>    <span class="hljs-comment"># 初始化卷积层权值</span><br>    nn.init.xavier_normal_(conv_layer.weight.data)<br>    <span class="hljs-comment"># nn.init.xavier_uniform_(conv_layer.weight.data)</span><br>    <span class="hljs-comment"># calculation</span><br>    img_conv = conv_layer(img_tensor)<br><br><span class="hljs-comment"># ================ transposed</span><br><span class="hljs-comment"># flag = 1</span><br>flag = <span class="hljs-number">0</span><br><span class="hljs-keyword">if</span> flag:<br>    conv_layer = nn.ConvTranspose2d(<span class="hljs-number">3</span>, <span class="hljs-number">1</span>, <span class="hljs-number">3</span>, stride=<span class="hljs-number">2</span>)   <span class="hljs-comment"># input:(input_channel, output_channel, size)</span><br>    <span class="hljs-comment"># 初始化网络层的权值</span><br>    nn.init.xavier_normal_(conv_layer.weight.data)<br><br>    <span class="hljs-comment"># calculation</span><br>    img_conv = conv_layer(img_tensor)<br><br><span class="hljs-comment"># ================================= visualization ==================================</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;卷积前尺寸:&#123;&#125;\n卷积后尺寸:&#123;&#125;&quot;</span>.<span class="hljs-built_in">format</span>(img_tensor.shape, img_conv.shape))<br>img_conv = transform_invert(img_conv[<span class="hljs-number">0</span>, <span class="hljs-number">0</span>:<span class="hljs-number">1</span>, ...], img_transform)<br>img_raw = transform_invert(img_tensor.squeeze(), img_transform)<br>plt.subplot(<span class="hljs-number">122</span>).imshow(img_conv, cmap=<span class="hljs-string">&#x27;gray&#x27;</span>)<br>plt.subplot(<span class="hljs-number">121</span>).imshow(img_raw)<br>plt.show()<br></code></pre></td></tr></table></figure><p>卷积前后的图片如下 (左边是原图片，右边是卷积后的图片)：<br><img src="/img/pytorchtrain3/7.png"><br>当改为使用nn.init.xavier_uniform_()方法初始化网络的权值时，卷积前后图片如下：<br><img src="/img/pytorchtrain3/8.png"><br>我们通过conv_layer.weight.shape查看卷积核的 shape 是(1, 3, 3, 3)，对应是(output_channel, input_channel, kernel_size, kernel_size)。所以第一个维度对应的是卷积核的个数，每个卷积核都是(3,3,3)。虽然每个卷积核都是 3 维的，执行的却是 2 维卷积。下面这个图展示了这个过程。<br><img src="/img/pytorchtrain3/9.png"><br>也就是每个卷积核在 input_channel 维度再划分，这里 input_channel 为 3，那么这时每个卷积核的 shape 是(3, 3)。3 个卷积核在输入图像的每个 channel 上卷积后得到 3 个数，把这 3 个数相加，再加上 bias，得到最后的一个输出。<br><img src="/img/pytorchtrain3/10.png"></p><h3 id="转置卷积：nn-ConvTranspose"><a href="#转置卷积：nn-ConvTranspose" class="headerlink" title="转置卷积：nn.ConvTranspose()"></a>转置卷积：nn.ConvTranspose()</h3><p>转置卷积又称为反卷积 (Deconvolution) 和部分跨越卷积 (Fractionally strided Convolution)，用于对图像进行上采样。<br>正常卷积如下：<br><img src="/img/pytorchtrain3/4.gif"><br>原始的图片尺寸为 $4 \times 4$，卷积核大小为 $3 \times 3$，$padding &#x3D;0$，$stride &#x3D; 1$。由于卷积操作可以通过矩阵运算来解决，因此原始图片可以看作 $16 \times 1$ 的矩阵 $I{16 \times 1}$，卷积核可以看作 $4 \times 16$ 的矩阵$K{4 \times 16}$，那么输出是 $K{4 \times 16} \times I{16 \times 1} &#x3D; O_{4 \times 1}$ 。<br>转置卷积如下：<br><img src="/img/pytorchtrain3/5.gif"><br>原始的图片尺寸为 $2 \times 2$，卷积核大小为 $3 \times 3$，$padding &#x3D;0$，$stride &#x3D; 1$。由于卷积操作可以通过矩阵运算来解决，因此原始图片可以看作 $4 \times 1$ 的矩阵 $I{4 \times 1}$，卷积核可以看作 $4 \times 16$ 的矩阵$K{16 \times 4}$，那么输出是 $K{16 \times 4} \times I{4 \times 1} &#x3D; O_{16 \times 1}$ 。<br>正常卷积核转置卷积矩阵的形状刚好是转置关系，因此称为转置卷积，但里面的权值不是一样的，卷积操作也是不可逆的。</p><p>PyTorch 中的转置卷积函数如下：</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs routeros">nn.ConvTranspose2d(self, in_channels, out_channels, kernel_size, <span class="hljs-attribute">stride</span>=1,<br>                 <span class="hljs-attribute">padding</span>=0, <span class="hljs-attribute">output_padding</span>=0, <span class="hljs-attribute">groups</span>=1, <span class="hljs-attribute">bias</span>=<span class="hljs-literal">True</span>,<br>                 <span class="hljs-attribute">dilation</span>=1, <span class="hljs-attribute">padding_mode</span>=<span class="hljs-string">&#x27;zeros&#x27;</span>)<br></code></pre></td></tr></table></figure><p>和普通卷积的参数基本相同，不再赘述。</p><h4 id="转置卷积尺寸计算"><a href="#转置卷积尺寸计算" class="headerlink" title="转置卷积尺寸计算"></a>转置卷积尺寸计算</h4><h5 id="简化版转置卷积尺寸计算"><a href="#简化版转置卷积尺寸计算" class="headerlink" title="简化版转置卷积尺寸计算"></a>简化版转置卷积尺寸计算</h5><p>这里不考虑空洞卷积，假设输入图片大小为 $ I \times I$，卷积核大小为 $k \times k$，stride 为 $s$，padding 的像素数为 $p$，图片经过卷积之后的尺寸 $ O $ 如下，刚好和普通卷积的计算是相反的：<br>$O &#x3D; (I-1) \times s + k$</p><h5 id="完整版简化版转置卷积尺寸计算"><a href="#完整版简化版转置卷积尺寸计算" class="headerlink" title="完整版简化版转置卷积尺寸计算"></a>完整版简化版转置卷积尺寸计算</h5><p>$O &#x3D; (I-1) \times s - 2 \times p + d \times (k-1) + out_padding + 1$</p><p>转置卷积代码示例如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> os<br><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<br><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image<br><span class="hljs-keyword">from</span> torchvision <span class="hljs-keyword">import</span> transforms<br><span class="hljs-keyword">from</span> matplotlib <span class="hljs-keyword">import</span> pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-keyword">from</span> common_tools <span class="hljs-keyword">import</span> transform_invert, set_seed<br><br>set_seed(<span class="hljs-number">3</span>)  <span class="hljs-comment"># 设置随机种子</span><br><br><span class="hljs-comment"># ================================= load img ==================================</span><br>path_img = os.path.join(os.path.dirname(os.path.abspath(__file__)), <span class="hljs-string">&quot;imgs&quot;</span>, <span class="hljs-string">&quot;lena.png&quot;</span>)<br><span class="hljs-built_in">print</span>(path_img)<br>img = Image.<span class="hljs-built_in">open</span>(path_img).convert(<span class="hljs-string">&#x27;RGB&#x27;</span>)  <span class="hljs-comment"># 0~255</span><br><br><span class="hljs-comment"># convert to tensor</span><br>img_transform = transforms.Compose([transforms.ToTensor()])<br>img_tensor = img_transform(img)<br><span class="hljs-comment"># 添加 batch 维度</span><br>img_tensor.unsqueeze_(dim=<span class="hljs-number">0</span>)    <span class="hljs-comment"># C*H*W to B*C*H*W</span><br><br><span class="hljs-comment"># ================================= create convolution layer ==================================</span><br><br><span class="hljs-comment"># ================ 2d</span><br><span class="hljs-comment"># flag = 1</span><br>flag = <span class="hljs-number">0</span><br><span class="hljs-keyword">if</span> flag:<br>    conv_layer = nn.Conv2d(<span class="hljs-number">3</span>, <span class="hljs-number">1</span>, <span class="hljs-number">3</span>)   <span class="hljs-comment"># input:(i, o, size) weights:(o, i , h, w)</span><br>    <span class="hljs-comment"># 初始化卷积层权值</span><br>    nn.init.xavier_normal_(conv_layer.weight.data)<br>    <span class="hljs-comment"># nn.init.xavier_uniform_(conv_layer.weight.data)</span><br><br>    <span class="hljs-comment"># calculation</span><br>    img_conv = conv_layer(img_tensor)<br><br><span class="hljs-comment"># ================ transposed</span><br>flag = <span class="hljs-number">1</span><br><span class="hljs-comment"># flag = 0</span><br><span class="hljs-keyword">if</span> flag:<br>    conv_layer = nn.ConvTranspose2d(<span class="hljs-number">3</span>, <span class="hljs-number">1</span>, <span class="hljs-number">3</span>, stride=<span class="hljs-number">2</span>)   <span class="hljs-comment"># input:(input_channel, output_channel, size)</span><br>    <span class="hljs-comment"># 初始化网络层的权值</span><br>    nn.init.xavier_normal_(conv_layer.weight.data)<br><br>    <span class="hljs-comment"># calculation</span><br>    img_conv = conv_layer(img_tensor)<br><br><span class="hljs-comment"># ================================= visualization ==================================</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;卷积前尺寸:&#123;&#125;\n卷积后尺寸:&#123;&#125;&quot;</span>.<span class="hljs-built_in">format</span>(img_tensor.shape, img_conv.shape))<br>img_conv = transform_invert(img_conv[<span class="hljs-number">0</span>, <span class="hljs-number">0</span>:<span class="hljs-number">1</span>, ...], img_transform)<br>img_raw = transform_invert(img_tensor.squeeze(), img_transform)<br>plt.subplot(<span class="hljs-number">122</span>).imshow(img_conv, cmap=<span class="hljs-string">&#x27;gray&#x27;</span>)<br>plt.subplot(<span class="hljs-number">121</span>).imshow(img_raw)<br>plt.show()<br></code></pre></td></tr></table></figure><p>转置卷积前后图片显示如下，左边原图片的尺寸是 (512, 512)，右边转置卷积后的图片尺寸是 (1025, 1025)。<br><img src="/img/pytorchtrain3/11.png"><br>转置卷积后的图片一般都会有棋盘效应，像一格一格的棋盘，这是转置卷积的通病。</p><h2 id="池化层、线性层和激活函数层"><a href="#池化层、线性层和激活函数层" class="headerlink" title="池化层、线性层和激活函数层"></a>池化层、线性层和激活函数层</h2><h3 id="池化层"><a href="#池化层" class="headerlink" title="池化层"></a>池化层</h3><p>池化的作用则体现在降采样：保留显著特征、降低特征维度，增大kernel的感受野。 另外一点值得注意：pooling也可以提供一些旋转不变性。 池化层可对提取到的特征信息进行降维，一方面使特征图变小，简化网络计算复杂度并在一定程度上避免过拟合的出现；一方面进行特征压缩，提取主要特征。</p><p>有最大池化和平均池化两张方式。</p><h4 id="最大池化：nn-MaxPool2d"><a href="#最大池化：nn-MaxPool2d" class="headerlink" title="最大池化：nn.MaxPool2d()"></a>最大池化：nn.MaxPool2d()</h4><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs routeros">nn.MaxPool2d(kernel_size, <span class="hljs-attribute">stride</span>=None, <span class="hljs-attribute">padding</span>=0, <span class="hljs-attribute">dilation</span>=1, <span class="hljs-attribute">return_indices</span>=<span class="hljs-literal">False</span>, <span class="hljs-attribute">ceil_mode</span>=<span class="hljs-literal">False</span>)<br></code></pre></td></tr></table></figure><p>这个函数的功能是进行 2 维的最大池化，主要参数如下：</p><ul><li>kernel_size：池化核尺寸</li><li>stride：步长，通常与 kernel_size 一致</li><li>padding：填充宽度，主要是为了调整输出的特征图大小，一般把 padding 设置合适的值后，保持输入和输出的图像尺寸不变。</li><li>dilation：池化间隔大小，默认为1。常用于图像分割任务中，主要是为了提升感受野</li><li>ceil_mode：默认为 False，尺寸向下取整。为 True 时，尺寸向上取整</li><li>return_indices：为 True 时，返回最大池化所使用的像素的索引，这些记录的索引通常在反最大池化时使用，把小的特征图反池化到大的特征图时，每一个像素放在哪个位置。</li></ul><p>下图 (a) 表示反池化，(b) 表示上采样，(c) 表示反卷积。<br><img src="/img/pytorchtrain3/12.png"><br>下面是最大池化的代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> os<br><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<br><span class="hljs-keyword">from</span> torchvision <span class="hljs-keyword">import</span> transforms<br><span class="hljs-keyword">from</span> matplotlib <span class="hljs-keyword">import</span> pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image<br><span class="hljs-keyword">from</span> common_tools <span class="hljs-keyword">import</span> transform_invert, set_seed<br><br>set_seed(<span class="hljs-number">1</span>)  <span class="hljs-comment"># 设置随机种子</span><br><br><span class="hljs-comment"># ================================= load img ==================================</span><br>path_img = os.path.join(os.path.dirname(os.path.abspath(__file__)), <span class="hljs-string">&quot;imgs/lena.png&quot;</span>)<br>img = Image.<span class="hljs-built_in">open</span>(path_img).convert(<span class="hljs-string">&#x27;RGB&#x27;</span>)  <span class="hljs-comment"># 0~255</span><br><br><span class="hljs-comment"># convert to tensor</span><br>img_transform = transforms.Compose([transforms.ToTensor()])<br>img_tensor = img_transform(img)<br>img_tensor.unsqueeze_(dim=<span class="hljs-number">0</span>)    <span class="hljs-comment"># C*H*W to B*C*H*W</span><br><br><span class="hljs-comment"># ================================= create convolution layer ==================================</span><br><br><span class="hljs-comment"># ================ maxpool</span><br>flag = <span class="hljs-number">1</span><br><span class="hljs-comment"># flag = 0</span><br><span class="hljs-keyword">if</span> flag:<br>    maxpool_layer = nn.MaxPool2d((<span class="hljs-number">2</span>, <span class="hljs-number">2</span>), stride=(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>))   <span class="hljs-comment"># input:(i, o, size) weights:(o, i , h, w)</span><br>    img_pool = maxpool_layer(img_tensor)<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;池化前尺寸:&#123;&#125;\n池化后尺寸:&#123;&#125;&quot;</span>.<span class="hljs-built_in">format</span>(img_tensor.shape, img_pool.shape))<br>img_pool = transform_invert(img_pool[<span class="hljs-number">0</span>, <span class="hljs-number">0</span>:<span class="hljs-number">3</span>, ...], img_transform)<br>img_raw = transform_invert(img_tensor.squeeze(), img_transform)<br>plt.subplot(<span class="hljs-number">122</span>).imshow(img_pool)<br>plt.subplot(<span class="hljs-number">121</span>).imshow(img_raw)<br>plt.show()<br></code></pre></td></tr></table></figure><p>结果如下：</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs stylus">池化前尺寸:torch<span class="hljs-selector-class">.Size</span>(<span class="hljs-selector-attr">[1, 3, 512, 512]</span>)<br>池化后尺寸:torch<span class="hljs-selector-class">.Size</span>(<span class="hljs-selector-attr">[1, 3, 256, 256]</span>)<br></code></pre></td></tr></table></figure><p><img src="/img/pytorchtrain3/13.png"></p><h4 id="nn-AvgPool2d"><a href="#nn-AvgPool2d" class="headerlink" title="nn.AvgPool2d()"></a>nn.AvgPool2d()</h4><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs routeros">torch.nn.AvgPool2d(kernel_size, <span class="hljs-attribute">stride</span>=None, <span class="hljs-attribute">padding</span>=0, <span class="hljs-attribute">ceil_mode</span>=<span class="hljs-literal">False</span>, <span class="hljs-attribute">count_include_pad</span>=<span class="hljs-literal">True</span>, <span class="hljs-attribute">divisor_override</span>=None)<br></code></pre></td></tr></table></figure><p>这个函数的功能是进行 2 维的平均池化，主要参数如下：</p><ul><li>kernel_size：池化核尺寸</li><li>stride：步长，通常与 kernel_size 一致</li><li>padding：填充宽度，主要是为了调整输出的特征图大小，一般把 padding 设置合适的值后，保持输入和输出的图像尺寸不变。</li><li>ilation：池化间隔大小，默认为1。常用于图像分割任务中，主要是为了提升感受野</li><li>ceil_mode：默认为 False，尺寸向下取整。为 True 时，尺寸向上取整</li><li>count_include_pad：在计算平均值时，是否把填充值考虑在内计算</li><li>divisor_override：除法因子。在计算平均值时，分子是像素值的总和，分母默认是像素值的个数。如果设置了 divisor_override，把分母改为 divisor_override。</li></ul><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">img_tensor</span> = torch.ones((<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">4</span>, <span class="hljs-number">4</span>))<br><span class="hljs-attribute">avgpool_layer</span> = nn.AvgPool2d((<span class="hljs-number">2</span>, <span class="hljs-number">2</span>), stride=(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>))<br><span class="hljs-attribute">img_pool</span> = avgpool_layer(img_tensor)<br><span class="hljs-attribute">print</span>(<span class="hljs-string">&quot;raw_img:\n&#123;&#125;\npooling_img:\n&#123;&#125;&quot;</span>.format(img_tensor, img_pool))<br></code></pre></td></tr></table></figure><p>输出如下：</p><figure class="highlight inform7"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs inform7">raw_img:<br>tensor(<span class="hljs-comment">[<span class="hljs-comment">[<span class="hljs-comment">[<span class="hljs-comment">[1., 1., 1., 1.]</span>,</span></span></span><br><span class="hljs-comment"><span class="hljs-comment"><span class="hljs-comment">          <span class="hljs-comment">[1., 1., 1., 1.]</span>,</span></span></span><br><span class="hljs-comment"><span class="hljs-comment"><span class="hljs-comment">          <span class="hljs-comment">[1., 1., 1., 1.]</span>,</span></span></span><br><span class="hljs-comment"><span class="hljs-comment"><span class="hljs-comment">          <span class="hljs-comment">[1., 1., 1., 1.]</span>]</span>]</span>]</span>)<br>pooling_img:<br>tensor(<span class="hljs-comment">[<span class="hljs-comment">[<span class="hljs-comment">[<span class="hljs-comment">[1., 1.]</span>,</span></span></span><br><span class="hljs-comment"><span class="hljs-comment"><span class="hljs-comment">          <span class="hljs-comment">[1., 1.]</span>]</span>]</span>]</span>)<br></code></pre></td></tr></table></figure><p>加上divisor_override&#x3D;3后，输出如下：</p><figure class="highlight inform7"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs inform7">raw_img:<br>tensor(<span class="hljs-comment">[<span class="hljs-comment">[<span class="hljs-comment">[<span class="hljs-comment">[1., 1., 1., 1.]</span>,</span></span></span><br><span class="hljs-comment"><span class="hljs-comment"><span class="hljs-comment">          <span class="hljs-comment">[1., 1., 1., 1.]</span>,</span></span></span><br><span class="hljs-comment"><span class="hljs-comment"><span class="hljs-comment">          <span class="hljs-comment">[1., 1., 1., 1.]</span>,</span></span></span><br><span class="hljs-comment"><span class="hljs-comment"><span class="hljs-comment">          <span class="hljs-comment">[1., 1., 1., 1.]</span>]</span>]</span>]</span>)<br>pooling_img:<br>tensor(<span class="hljs-comment">[<span class="hljs-comment">[<span class="hljs-comment">[<span class="hljs-comment">[1.3333, 1.3333]</span>,</span></span></span><br><span class="hljs-comment"><span class="hljs-comment"><span class="hljs-comment">          <span class="hljs-comment">[1.3333, 1.3333]</span>]</span>]</span>]</span>)<br></code></pre></td></tr></table></figure><h4 id="nn-MaxUnpool2d"><a href="#nn-MaxUnpool2d" class="headerlink" title="nn.MaxUnpool2d()"></a>nn.MaxUnpool2d()</h4><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs routeros">nn.MaxUnpool2d(kernel_size, <span class="hljs-attribute">stride</span>=None, <span class="hljs-attribute">padding</span>=0)<br></code></pre></td></tr></table></figure><p>功能是对二维信号（图像）进行最大值反池化，主要参数如下：</p><ul><li>kernel_size：池化核尺寸</li><li>stride：步长，通常与 kernel_size 一致</li><li>padding：填充宽度</li></ul><p>代码如下：</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-comment"># pooling</span><br><span class="hljs-attribute">img_tensor</span> = torch.randint(high=<span class="hljs-number">5</span>, size=(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">4</span>, <span class="hljs-number">4</span>), dtype=torch.float)<br><span class="hljs-attribute">maxpool_layer</span> = nn.MaxPool2d((<span class="hljs-number">2</span>, <span class="hljs-number">2</span>), stride=(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>), return_indices=True)<br><span class="hljs-attribute">img_pool</span>, indices = maxpool_layer(img_tensor)<br><br><span class="hljs-comment"># unpooling</span><br><span class="hljs-attribute">img_reconstruct</span> = torch.randn_like(img_pool, dtype=torch.float)<br><span class="hljs-attribute">maxunpool_layer</span> = nn.MaxUnpool2d((<span class="hljs-number">2</span>, <span class="hljs-number">2</span>), stride=(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>))<br><span class="hljs-attribute">img_unpool</span> = maxunpool_layer(img_reconstruct, indices)<br><br><span class="hljs-attribute">print</span>(<span class="hljs-string">&quot;raw_img:\n&#123;&#125;\nimg_pool:\n&#123;&#125;&quot;</span>.format(img_tensor, img_pool))<br><span class="hljs-attribute">print</span>(<span class="hljs-string">&quot;img_reconstruct:\n&#123;&#125;\nimg_unpool:\n&#123;&#125;&quot;</span>.format(img_reconstruct, img_unpool))<br></code></pre></td></tr></table></figure><p>输出如下：</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-comment"># pooling</span><br><span class="hljs-attribute">img_tensor</span> = torch.randint(high=<span class="hljs-number">5</span>, size=(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">4</span>, <span class="hljs-number">4</span>), dtype=torch.float)<br><span class="hljs-attribute">maxpool_layer</span> = nn.MaxPool2d((<span class="hljs-number">2</span>, <span class="hljs-number">2</span>), stride=(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>), return_indices=True)<br><span class="hljs-attribute">img_pool</span>, indices = maxpool_layer(img_tensor)<br><br><span class="hljs-comment"># unpooling</span><br><span class="hljs-attribute">img_reconstruct</span> = torch.randn_like(img_pool, dtype=torch.float)<br><span class="hljs-attribute">maxunpool_layer</span> = nn.MaxUnpool2d((<span class="hljs-number">2</span>, <span class="hljs-number">2</span>), stride=(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>))<br><span class="hljs-attribute">img_unpool</span> = maxunpool_layer(img_reconstruct, indices)<br><br><span class="hljs-attribute">print</span>(<span class="hljs-string">&quot;raw_img:\n&#123;&#125;\nimg_pool:\n&#123;&#125;&quot;</span>.format(img_tensor, img_pool))<br><span class="hljs-attribute">print</span>(<span class="hljs-string">&quot;img_reconstruct:\n&#123;&#125;\nimg_unpool:\n&#123;&#125;&quot;</span>.format(img_reconstruct, img_unpool))<br></code></pre></td></tr></table></figure><h3 id="线性层"><a href="#线性层" class="headerlink" title="线性层"></a>线性层</h3><p>线性层又称为全连接层，其每个神经元与上一个层所有神经元相连，实现对前一层的线性组合或线性变换。<br>代码如下：</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs stylus">inputs = torch<span class="hljs-selector-class">.tensor</span>(<span class="hljs-selector-attr">[[1., 2, 3]</span>])<br>linear_layer = nn<span class="hljs-selector-class">.Linear</span>(<span class="hljs-number">3</span>, <span class="hljs-number">4</span>)<br>linear_layer<span class="hljs-selector-class">.weight</span><span class="hljs-selector-class">.data</span> = torch<span class="hljs-selector-class">.tensor</span>(<span class="hljs-selector-attr">[[1., 1., 1.]</span>,<br><span class="hljs-selector-attr">[2., 2., 2.]</span>,<br><span class="hljs-selector-attr">[3., 3., 3.]</span>,<br><span class="hljs-selector-attr">[4., 4., 4.]</span>])<br><br>linear_layer<span class="hljs-selector-class">.bias</span><span class="hljs-selector-class">.data</span><span class="hljs-selector-class">.fill_</span>(<span class="hljs-number">0.5</span>)<br>output = <span class="hljs-built_in">linear_layer</span>(inputs)<br><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(inputs, inputs.shape)</span></span><br><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(linear_layer.weight.data, linear_layer.weight.data.shape)</span></span><br><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(output, output.shape)</span></span><br></code></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs stylus"><span class="hljs-function"><span class="hljs-title">tensor</span><span class="hljs-params">([[<span class="hljs-number">1</span>., <span class="hljs-number">2</span>., <span class="hljs-number">3</span>.]])</span></span> torch<span class="hljs-selector-class">.Size</span>(<span class="hljs-selector-attr">[1, 3]</span>)<br><span class="hljs-built_in">tensor</span>(<span class="hljs-selector-attr">[[1., 1., 1.]</span>,<br>        <span class="hljs-selector-attr">[2., 2., 2.]</span>,<br>        <span class="hljs-selector-attr">[3., 3., 3.]</span>,<br>        <span class="hljs-selector-attr">[4., 4., 4.]</span>]) torch<span class="hljs-selector-class">.Size</span>(<span class="hljs-selector-attr">[4, 3]</span>)<br><span class="hljs-function"><span class="hljs-title">tensor</span><span class="hljs-params">([[ <span class="hljs-number">6.5000</span>, <span class="hljs-number">12.5000</span>, <span class="hljs-number">18.5000</span>, <span class="hljs-number">24.5000</span>]], grad_fn=&lt;AddmmBackward&gt;)</span></span> torch<span class="hljs-selector-class">.Size</span>(<span class="hljs-selector-attr">[1, 4]</span>)<br></code></pre></td></tr></table></figure><h3 id="激活函数层"><a href="#激活函数层" class="headerlink" title="激活函数层"></a>激活函数层</h3><p>假设第一个隐藏层为：$H{1}&#x3D;X \times W{1}$，第二个隐藏层为：$H{2}&#x3D;H{1} \times W_{2}$，输出层为：</p><p><img src="/img/pytorchtrain3/1.jpg"><br>如果没有非线性变换，由于矩阵乘法的结合性，多个线性层的组合等价于一个线性层。</p><p>激活函数对特征进行非线性变换，赋予了多层神经网络具有深度的意义。下面介绍一些激活函数层。</p><h4 id="nn-Sigmoid"><a href="#nn-Sigmoid" class="headerlink" title="nn.Sigmoid"></a>nn.Sigmoid</h4><ul><li>计算公式：$y&#x3D;\frac{1}{1+e^{-x}}$</li><li>梯度公式：$y^{\prime}&#x3D;y *(1-y)$</li><li>特性：<ul><li>输出值在(0,1)，符合概率</li><li>导数范围是 [0, 0.25]，容易导致梯度消失</li><li>输出为非 0 均值，破坏数据分布<br><img src="/img/pytorchtrain3/15.png"></li></ul></li></ul><h4 id="nn-tanh"><a href="#nn-tanh" class="headerlink" title="nn.tanh"></a>nn.tanh</h4><ul><li>计算公式：$y&#x3D;\frac{\sin x}{\cos x}&#x3D;\frac{e^{x}-e^{-x}}{e^{-}+e^{-x}}&#x3D;\frac{2}{1+e^{-2 x}}+1$</li><li>梯度公式：$y^{\prime}&#x3D;1-y^{2}$</li><li>特性：<ul><li>输出值在(-1, 1)，数据符合 0 均值</li><li>导数范围是 (0,1)，容易导致梯度消失<br><img src="/img/pytorchtrain3/16.png"></li></ul></li></ul><h4 id="nn-ReLU-修正线性单元"><a href="#nn-ReLU-修正线性单元" class="headerlink" title="nn.ReLU(修正线性单元)"></a>nn.ReLU(修正线性单元)</h4><ul><li>计算公式：$y&#x3D;max(0, x)$</li><li>梯度公式：<img src="/img/pytorchtrain3/2.jpg"></li><li>特性：<ul><li>输出值均为正数，负半轴的导数为 0，容易导致死神经元</li><li>导数是 1，缓解梯度消失，但容易引发梯度爆炸<br><img src="/img/pytorchtrain3/17.png"><br>针对 RuLU 会导致死神经元的缺点，出现了下面 3 种改进的激活函数。<br><img src="/img/pytorchtrain3/18.png"></li></ul></li></ul><h4 id="nn-LeakyReLU"><a href="#nn-LeakyReLU" class="headerlink" title="nn.LeakyReLU"></a>nn.LeakyReLU</h4><ul><li>有一个参数negative_slope：设置负半轴斜率</li></ul><h4 id="nn-PReLU"><a href="#nn-PReLU" class="headerlink" title="nn.PReLU"></a>nn.PReLU</h4><ul><li>有一个参数init：设置初始斜率，这个斜率是可学习的</li></ul><h4 id="nn-RReLU"><a href="#nn-RReLU" class="headerlink" title="nn.RReLU"></a>nn.RReLU</h4><p>R 是 random 的意思，负半轴每次斜率都是随机取 [lower, upper] 之间的一个数</p><ul><li>lower：均匀分布下限</li><li>upper：均匀分布上限</li></ul>]]></content>
    
    
    <categories>
      
      <category>深度学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>pytorch</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>深度学习-9-pytorch-2-数据处理</title>
    <link href="/2021/08/10/pytorchtrain2/"/>
    <url>/2021/08/10/pytorchtrain2/</url>
    
    <content type="html"><![CDATA[<blockquote><p>创作声明：主要内容参考于张贤同学<a href="https://zhuanlan.zhihu.com/p/265394674">https://zhuanlan.zhihu.com/p/265394674</a></p></blockquote><p>数据模块又可以细分为 4 个部分：</p><ul><li>数据收集：样本和标签。</li><li>数据划分：训练集、验证集和测试集</li><li>数据读取：对应于 PyTorch 的 DataLoader。其中 DataLoader 包括 Sampler 和 DataSet。Sampler 的功能是生成索引， DataSet 是根据生成的索引读取样本以及标签。</li><li>数据预处理：对应于 PyTorch 的 transforms</li></ul><h2 id="DataLoader-与-DataSet（数据读取）"><a href="#DataLoader-与-DataSet（数据读取）" class="headerlink" title="DataLoader 与 DataSet（数据读取）"></a>DataLoader 与 DataSet（数据读取）</h2><h3 id="torch-utils-data-DataLoader"><a href="#torch-utils-data-DataLoader" class="headerlink" title="torch.utils.data.DataLoader()"></a>torch.utils.data.DataLoader()</h3><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs routeros">torch.utils.data.DataLoader(dataset, <span class="hljs-attribute">batch_size</span>=1, <span class="hljs-attribute">shuffle</span>=<span class="hljs-literal">False</span>, <span class="hljs-attribute">sampler</span>=None, <span class="hljs-attribute">batch_sampler</span>=None, <span class="hljs-attribute">num_workers</span>=0, <span class="hljs-attribute">collate_fn</span>=None, <span class="hljs-attribute">pin_memory</span>=<span class="hljs-literal">False</span>, <span class="hljs-attribute">drop_last</span>=<span class="hljs-literal">False</span>, <span class="hljs-attribute">timeout</span>=0, <span class="hljs-attribute">worker_init_fn</span>=None, <span class="hljs-attribute">multiprocessing_context</span>=None)<br></code></pre></td></tr></table></figure><p>功能：构建可迭代的数据装载器</p><ul><li>dataset: Dataset 类，决定数据从哪里读取以及如何读取</li><li>batchsize: 批大小</li><li>num_works:num_works: 是否多进程读取数据</li><li>sheuffle: 每个 epoch 是否乱序</li><li>drop_last: 当样本数不能被 batchsize 整除时，是否舍弃最后一批数据</li></ul><p><strong>Epoch, Iteration, Batchsize</strong></p><ul><li>Epoch: 所有训练样本都已经输入到模型中，称为一个 Epoch</li><li>Iteration: 一批样本输入到模型中，称为一个 Iteration</li><li>Batchsize: 批大小，决定一个 iteration 有多少样本，也决定了一个 Epoch 有多少个 Iteration</li></ul><p>假设样本总数有 80，设置 Batchsize 为 8，则共有 $80 \div 8&#x3D;10$ 个 Iteration。这里 $1 Epoch &#x3D; 10 Iteration$。</p><p>假设样本总数有 86，设置 Batchsize 为 8。如果drop_last&#x3D;True则共有 10 个 Iteration；如果drop_last&#x3D;False则共有 11 个 Iteration。</p><h3 id="torch-utils-data-Dataset"><a href="#torch-utils-data-Dataset" class="headerlink" title="torch.utils.data.Dataset"></a>torch.utils.data.Dataset</h3><p>功能：Dataset 是抽象类，所有自定义的 Dataset 都需要继承该类，并且重写<code>__getitem()__</code>方法和<code>__len__()</code>方法 。<code>__getitem()__</code>方法的作用是接收一个索引，返回索引对应的样本和标签，这是我们自己需要实现的逻辑。<code>__len__()</code>方法是返回所有样本的数量。</p><p>数据读取包含 3 个方面</p><ul><li>读取哪些数据：每个 Iteration 读取一个 Batchsize 大小的数据，每个 Iteration 应该读取哪些数据。</li><li>从哪里读取数据：如何找到硬盘中的数据，应该在哪里设置文件路径参数。</li><li>如何读取数据：不同的文件需要使用不同的读取方法和库。</li></ul><p>这里的路径结构如下，有两类人民币图片：1 元和 100 元，每一类各有 100 张图片。</p><ul><li>RMB_data<ul><li>1</li><li>100<br>首先划分数据集为训练集、验证集和测试集，比例为 8:1:1。<br>数据划分好后的路径构造如下：</li></ul></li><li>rmb_split<ul><li>train<ul><li>1</li><li>100</li></ul></li><li>valid<ul><li>1</li><li>100</li></ul></li><li>test<ul><li>1</li><li>100</li></ul></li></ul></li></ul><p>实现读取数据的 Dataset，编写一个<code>get_img_info()</code>方法，读取每一个图片的路径和对应的标签，组成一个元组，再把所有的元组作为 list 存放到<code>self.data_info</code>变量中，这里需要注意的是标签需要映射到 0 开始的整数: <code>rmb_label = &#123;&quot;1&quot;: 0, &quot;100&quot;: 1&#125;</code>。</p><figure class="highlight less"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs less"><span class="hljs-variable">@staticmethod</span><br>def <span class="hljs-built_in">get_img_info</span>(data_dir):<br>    data_info = <span class="hljs-built_in">list</span>()<br>    # data_dir 是训练集、验证集或者测试集的路径<br>    for root, dirs, _ in os.<span class="hljs-built_in">walk</span>(data_dir):<br>        # 遍历类别<br>        # dirs [<span class="hljs-string">&#x27;1&#x27;</span>, <span class="hljs-string">&#x27;100&#x27;</span>]<br>        for sub_dir in <span class="hljs-attribute">dirs</span>:<br>            # 文件列表<br>            img_names = os.<span class="hljs-built_in">listdir</span>(os.path.<span class="hljs-built_in">join</span>(root, sub_dir))<br>            # 取出 jpg 结尾的文件<br>            img_names = <span class="hljs-built_in">list</span>(<span class="hljs-built_in">filter</span>(lambda <span class="hljs-attribute">x</span>: x.<span class="hljs-built_in">endswith</span>(<span class="hljs-string">&#x27;.jpg&#x27;</span>), img_names))<br>            # 遍历图片<br>            for i in <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(img_names)):<br>                img_name = img_names[i]<br>                # 图片的绝对路径<br>                path_img = os.path.<span class="hljs-built_in">join</span>(root, sub_dir, img_name)<br>                # 标签，这里需要映射为 <span class="hljs-number">0</span>、<span class="hljs-number">1</span> 两个类别<br>                label = rmb_label[sub_dir]<br>                # 保存在 data_info 变量中<br>                data_info.<span class="hljs-built_in">append</span>((path_img, <span class="hljs-built_in">int</span>(label)))<br>    return data_info<br></code></pre></td></tr></table></figure><p>然后在Dataset 的初始化函数中调用get_img_info()方法。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, data_dir, transform=<span class="hljs-literal">None</span></span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    rmb面额分类任务的Dataset</span><br><span class="hljs-string">    :param data_dir: str, 数据集所在路径</span><br><span class="hljs-string">    :param transform: torch.transform，数据预处理</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-comment"># data_info存储所有图片路径和标签，在DataLoader中通过index读取样本</span><br>    <span class="hljs-variable language_">self</span>.data_info = <span class="hljs-variable language_">self</span>.get_img_info(data_dir)<br>    <span class="hljs-variable language_">self</span>.transform = transform<br></code></pre></td></tr></table></figure><p>然后在<code>__getitem__()</code>方法中根据index 读取self.data_info中路径对应的数据，并在这里做 transform 操作，返回的是样本和标签。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">__getitem__</span>(<span class="hljs-params">self, index</span>):<br>    <span class="hljs-comment"># 通过 index 读取样本</span><br>    path_img, label = <span class="hljs-variable language_">self</span>.data_info[index]<br>    <span class="hljs-comment"># 注意这里需要 convert(&#x27;RGB&#x27;)</span><br>    img = Image.<span class="hljs-built_in">open</span>(path_img).convert(<span class="hljs-string">&#x27;RGB&#x27;</span>)     <span class="hljs-comment"># 0~255</span><br>    <span class="hljs-keyword">if</span> <span class="hljs-variable language_">self</span>.transform <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>        img = <span class="hljs-variable language_">self</span>.transform(img)   <span class="hljs-comment"># 在这里做transform，转为tensor等等</span><br>    <span class="hljs-comment"># 返回是样本和标签</span><br>    <span class="hljs-keyword">return</span> img, label<br></code></pre></td></tr></table></figure><p>在<code>__len__()</code>方法中返回self.data_info的长度，即为所有样本的数量。</p><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs ruby"><span class="hljs-comment"># 返回所有样本的数量</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">__len__</span>(<span class="hljs-params"><span class="hljs-variable language_">self</span></span>):<br>    <span class="hljs-keyword">return</span> len(<span class="hljs-variable language_">self</span>.data_info)<br></code></pre></td></tr></table></figure><h3 id="简单的训练示范"><a href="#简单的训练示范" class="headerlink" title="简单的训练示范"></a>简单的训练示范</h3><p>在<code>train_lenet.py</code>中，分 5 步构建模型。</p><p>第 1 步设置数据。首先定义训练集、验证集、测试集的路径，定义训练集和测试集的transforms。然后构建训练集和验证集的RMBDataset对象，把对应的路径和transforms传进去。再构建DataLoder，设置 batch_size，其中训练集设置shuffle&#x3D;True，表示每个 Epoch 都打乱样本。</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs routeros"><span class="hljs-comment"># 构建MyDataset实例</span><br>train_data = RMBDataset(<span class="hljs-attribute">data_dir</span>=train_dir, <span class="hljs-attribute">transform</span>=train_transform)<br>valid_data = RMBDataset(<span class="hljs-attribute">data_dir</span>=valid_dir, <span class="hljs-attribute">transform</span>=valid_transform)<br><br><span class="hljs-comment"># 构建DataLoder</span><br><span class="hljs-comment"># 其中训练集设置 shuffle=True，表示每个 Epoch 都打乱样本</span><br>train_loader = DataLoader(<span class="hljs-attribute">dataset</span>=train_data, <span class="hljs-attribute">batch_size</span>=BATCH_SIZE, <span class="hljs-attribute">shuffle</span>=<span class="hljs-literal">True</span>)<br>valid_loader = DataLoader(<span class="hljs-attribute">dataset</span>=valid_data, <span class="hljs-attribute">batch_size</span>=BATCH_SIZE)<br></code></pre></td></tr></table></figure><p>第 2 步构建模型，这里采用经典的 Lenet 图片分类网络。</p><figure class="highlight abnf"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs abnf"><span class="hljs-attribute">net</span> <span class="hljs-operator">=</span> LeNet(classes<span class="hljs-operator">=</span><span class="hljs-number">2</span>)<br>net.initialize_weights()<br></code></pre></td></tr></table></figure><p>第 3 步设置损失函数，这里使用交叉熵损失函数。</p><figure class="highlight abnf"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs abnf"><span class="hljs-attribute">criterion</span> <span class="hljs-operator">=</span> nn.CrossEntropyLoss()<br></code></pre></td></tr></table></figure><p>第 4 步设置优化器。这里采用 SGD 优化器。</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs routeros">optimizer = optim.SGD(net.parameters(), <span class="hljs-attribute">lr</span>=LR, <span class="hljs-attribute">momentum</span>=0.9)                        # 选择优化器<span class="hljs-built_in"></span><br><span class="hljs-built_in">scheduler </span>= torch.optim.lr_scheduler.StepLR(optimizer, <span class="hljs-attribute">step_size</span>=10, <span class="hljs-attribute">gamma</span>=0.1)     # 设置学习率下降策略<br></code></pre></td></tr></table></figure><p>第 5 步迭代训练模型，在每一个 epoch 里面，需要遍历 train_loader 取出数据，每次取得数据是一个 batchsize 大小。这里又分为 4 步。第 1 步进行前向传播，第 2 步进行反向传播求导，第 3 步使用optimizer更新权重，第 4 步统计训练情况。每一个 epoch 完成时都需要使用scheduler更新学习率，和计算验证集的准确率、loss。</p><figure class="highlight scss"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><code class="hljs scss">for epoch in <span class="hljs-built_in">range</span>(MAX_EPOCH):<br><br>    loss_mean = <span class="hljs-number">0</span>.<br>    correct = <span class="hljs-number">0</span>.<br>    total = <span class="hljs-number">0</span>.<br><br>    net.<span class="hljs-built_in">train</span>()<br>    # 遍历 train_loader 取数据<br>    for i, data in <span class="hljs-built_in">enumerate</span>(train_loader):<br><br>        # forward<br>        inputs, labels = data<br>        outputs = <span class="hljs-built_in">net</span>(inputs)<br><br>        # backward<br>        optimizer.<span class="hljs-built_in">zero_grad</span>()<br>        loss = <span class="hljs-built_in">criterion</span>(outputs, labels)<br>        loss.<span class="hljs-built_in">backward</span>()<br><br>        # update weights<br>        optimizer.<span class="hljs-built_in">step</span>()<br><br>        # 统计分类情况<br>        _, predicted = torch.<span class="hljs-built_in">max</span>(outputs.data, <span class="hljs-number">1</span>)<br>        total += labels.<span class="hljs-built_in">size</span>(<span class="hljs-number">0</span>)<br>        correct += (predicted == labels).<span class="hljs-built_in">squeeze</span>().<span class="hljs-built_in">sum</span>().<span class="hljs-built_in">numpy</span>()<br><br>        # 打印训练信息<br>        loss_mean += loss.<span class="hljs-built_in">item</span>()<br>        train_curve.<span class="hljs-built_in">append</span>(loss.<span class="hljs-built_in">item</span>())<br>        if (i+<span class="hljs-number">1</span>) % log_interval == <span class="hljs-number">0</span>:<br>            loss_mean = loss_mean / log_interval<br>            <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Training:Epoch[&#123;:0&gt;3&#125;/&#123;:0&gt;3&#125;] Iteration[&#123;:0&gt;3&#125;/&#123;:0&gt;3&#125;] Loss: &#123;:.4f&#125; Acc:&#123;:.2%&#125;&quot;</span>.<span class="hljs-built_in">format</span>(<br>                epoch, MAX_EPOCH, i+<span class="hljs-number">1</span>, <span class="hljs-built_in">len</span>(train_loader), loss_mean, correct / total))<br>            loss_mean = <span class="hljs-number">0</span>.<br><br>    scheduler.<span class="hljs-built_in">step</span>()  # 更新学习率<br>    # 每个 epoch 计算验证集得准确率和loss<br>    ...<br>    ...<br></code></pre></td></tr></table></figure><p>我们可以看到每个 iteration，我们是从train_loader中取出数据的。</p><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs ruby"><span class="hljs-keyword">def</span> <span class="hljs-title function_">__iter__</span>(<span class="hljs-params"><span class="hljs-variable language_">self</span></span>):<br>    <span class="hljs-keyword">if</span> <span class="hljs-variable language_">self</span>.num_workers == <span class="hljs-number">0</span>:<br>        <span class="hljs-keyword">return</span> _SingleProcessDataLoaderIter(<span class="hljs-variable language_">self</span>)<br>    <span class="hljs-symbol">else:</span><br>        <span class="hljs-keyword">return</span> _MultiProcessingDataLoaderIter(<span class="hljs-variable language_">self</span>)<br></code></pre></td></tr></table></figure><p>这里我们没有设置多进程，会执行<code>_SingleProcessDataLoaderIter</code>的方法。我们以<code>_SingleProcessDataLoaderIter</code>为例。在<code>_SingleProcessDataLoaderIter</code>里只有一个方法<code>_next_data()</code>，如下：</p><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs ruby"><span class="hljs-keyword">def</span> <span class="hljs-title function_">_next_data</span>(<span class="hljs-params"><span class="hljs-variable language_">self</span></span>):<br>    index = <span class="hljs-variable language_">self</span>._next_index()  <span class="hljs-comment"># may raise StopIteration</span><br>    data = <span class="hljs-variable language_">self</span>._dataset_fetcher.fetch(index)  <span class="hljs-comment"># may raise StopIteration</span><br>    <span class="hljs-keyword">if</span> <span class="hljs-variable language_">self</span>.<span class="hljs-symbol">_pin_memory:</span><br>        data = _utils.pin_memory.pin_memory(data)<br>    <span class="hljs-keyword">return</span> data<br></code></pre></td></tr></table></figure><p>在该方法中，<code>self._next_index()</code>是获取一个 batchsize 大小的 index 列表，代码如下：</p><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs ruby"><span class="hljs-keyword">def</span> <span class="hljs-title function_">_next_index</span>(<span class="hljs-params"><span class="hljs-variable language_">self</span></span>):<br>    <span class="hljs-keyword">return</span> <span class="hljs-keyword">next</span>(<span class="hljs-variable language_">self</span>._sampler_iter)  <span class="hljs-comment"># may raise StopIteration</span><br></code></pre></td></tr></table></figure><p>其中调用的sampler类的__iter__()方法返回 batch_size 大小的随机 index 列表。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">__iter__</span>(<span class="hljs-params">self</span>):<br>    batch = []<br>    <span class="hljs-keyword">for</span> idx <span class="hljs-keyword">in</span> <span class="hljs-variable language_">self</span>.sampler:<br>        batch.append(idx)<br>        <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(batch) == <span class="hljs-variable language_">self</span>.batch_size:<br>            <span class="hljs-keyword">yield</span> batch<br>            batch = []<br>    <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(batch) &gt; <span class="hljs-number">0</span> <span class="hljs-keyword">and</span> <span class="hljs-keyword">not</span> <span class="hljs-variable language_">self</span>.drop_last:<br>        <span class="hljs-keyword">yield</span> batch<br></code></pre></td></tr></table></figure><p>然后再返回看 dataloader的_next_data()方法：</p><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs ruby"><span class="hljs-keyword">def</span> <span class="hljs-title function_">_next_data</span>(<span class="hljs-params"><span class="hljs-variable language_">self</span></span>):<br>    index = <span class="hljs-variable language_">self</span>._next_index()  <span class="hljs-comment"># may raise StopIteration</span><br>    data = <span class="hljs-variable language_">self</span>._dataset_fetcher.fetch(index)  <span class="hljs-comment"># may raise StopIteration</span><br>    <span class="hljs-keyword">if</span> <span class="hljs-variable language_">self</span>.<span class="hljs-symbol">_pin_memory:</span><br>        data = _utils.pin_memory.pin_memory(data)<br>    <span class="hljs-keyword">return</span> data<br></code></pre></td></tr></table></figure><p>在第二行中调用了self._dataset_fetcher.fetch(index)获取数据。这里会调用_MapDatasetFetcher中的fetch()函数：</p><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs ruby"><span class="hljs-keyword">def</span> <span class="hljs-title function_">fetch</span>(<span class="hljs-params"><span class="hljs-variable language_">self</span>, possibly_batched_index</span>):<br>    <span class="hljs-keyword">if</span> <span class="hljs-variable language_">self</span>.<span class="hljs-symbol">auto_collation:</span><br>        data = [<span class="hljs-variable language_">self</span>.dataset[idx] <span class="hljs-keyword">for</span> idx <span class="hljs-keyword">in</span> possibly_batched_index]<br>    <span class="hljs-symbol">else:</span><br>        data = <span class="hljs-variable language_">self</span>.dataset[possibly_batched_index]<br>    <span class="hljs-keyword">return</span> <span class="hljs-variable language_">self</span>.collate_fn(data)<br></code></pre></td></tr></table></figure><p>这里调用了self.dataset[idx]，这个函数会调用dataset.<strong>getitem</strong>()方法获取具体的数据，所以__getitem__()方法是我们必须实现的。我们拿到的data是一个 list，每个元素是一个 tunple，每个 tunple 包括样本和标签。所以最后要使用self.collate_fn(data)把 data 转换为两个 list，第一个 元素 是样本的batch 形式，形状为 [16, 3, 32, 32] (16 是 batch size，[3, 32, 32] 是图片像素)；第二个元素是标签的 batch 形式，形状为 [16]。</p><p>所以在代码中，我们使用inputs, labels &#x3D; data来接收数据。</p><p>PyTorch 数据读取流程图<br><img src="/img/pytorchtrain2/1.png"></p><p>首先在 for 循环中遍历DataLoader，然后根据是否采用多进程，决定使用单进程或者多进程的DataLoaderIter。</p><p>在DataLoaderIter里调用Sampler生成Index的 list，再调用DatasetFetcher根据index获取数据。</p><p>在DatasetFetcher里会调用Dataset的__getitem__()方法获取真正的数据。这里获取的数据是一个 list，其中每个元素是 (img, label) 的元组。</p><p>再使用 collate_fn()函数整理成一个 list，里面包含两个元素，分别是 img 和 label 的tenser。</p><h2 id="PyTorch-的数据增强（数据预处理）"><a href="#PyTorch-的数据增强（数据预处理）" class="headerlink" title="PyTorch 的数据增强（数据预处理）"></a>PyTorch 的数据增强（数据预处理）</h2><p>我们在安装PyTorch时，还安装了torchvision，这是一个计算机视觉工具包。有 3 个主要的模块：</p><ul><li>torchvision.transforms:  里面包括常用的图像预处理方法</li><li>torchvision.datasets: 里面包括常用数据集如 mnist、CIFAR-10、Image-Net 等</li><li>torchvision.models: 里面包括常用的预训练好的模型，如 AlexNet、VGG、ResNet、GoogleNet 等</li></ul><p>深度学习模型是由数据驱动的，数据的数量和分布对模型训练的结果起到决定性作用。所以我们需要对数据进行预处理和数据增强。下面是用数据增强，从一张图片经过各种变换生成 64 张图片，增加了数据的多样性，这可以提高模型的泛化能力。<br><img src="/img/pytorchtrain2/2.png"></p><p>常用的图像预处理方法有：</p><ul><li>数据中心化</li><li>数据标准化</li><li>缩放</li><li>裁剪</li><li>旋转</li><li>翻转</li><li>填充</li><li>噪声添加</li><li>灰度变换</li><li>线性变换</li><li>仿射变换</li><li>亮度、饱和度以及对比度变换。</li></ul><p>在人民币图片二分类实验中，我们对数据进行了一定的增强。</p><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs ini"><span class="hljs-comment"># 设置训练集的数据增强和转化</span><br><span class="hljs-attr">train_transform</span> = transforms.Compose([<br>    transforms.Resize((<span class="hljs-number">32</span>, <span class="hljs-number">32</span>)),<span class="hljs-comment"># 缩放</span><br>    transforms.RandomCrop(<span class="hljs-number">32</span>, padding=<span class="hljs-number">4</span>), <span class="hljs-comment">#裁剪</span><br>    transforms.ToTensor(), <span class="hljs-comment"># 转为张量，同时归一化</span><br>    transforms.Normalize(norm_mean, norm_std),<span class="hljs-comment"># 标准化</span><br>])<br><br><span class="hljs-comment"># 设置验证集的数据增强和转化，不需要 RandomCrop</span><br><span class="hljs-attr">valid_transform</span> = transforms.Compose([<br>    transforms.Resize((<span class="hljs-number">32</span>, <span class="hljs-number">32</span>)),<br>    transforms.ToTensor(),<br>    transforms.Normalize(norm_mean, norm_std),<br>])<br></code></pre></td></tr></table></figure><p>当我们需要多个transforms操作时，需要作为一个list放在transforms.Compose中。需要注意的是transforms.ToTensor()是把图片转换为张量，同时进行归一化操作，把每个通道 0<del>255 的值归一化为 0</del>1。在验证集的数据增强中，不再需要transforms.RandomCrop()操作。然后把这两个transform操作作为参数传给Dataset，在Dataset的__getitem__()方法中做图像增强。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">__getitem__</span>(<span class="hljs-params">self, index</span>):<br>    <span class="hljs-comment"># 通过 index 读取样本</span><br>    path_img, label = <span class="hljs-variable language_">self</span>.data_info[index]<br>    <span class="hljs-comment"># 注意这里需要 convert(&#x27;RGB&#x27;)</span><br>    img = Image.<span class="hljs-built_in">open</span>(path_img).convert(<span class="hljs-string">&#x27;RGB&#x27;</span>)     <span class="hljs-comment"># 0~255</span><br>    <span class="hljs-keyword">if</span> <span class="hljs-variable language_">self</span>.transform <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>        img = <span class="hljs-variable language_">self</span>.transform(img)   <span class="hljs-comment"># 在这里做transform，转为tensor等等</span><br>    <span class="hljs-comment"># 返回是样本和标签</span><br>    <span class="hljs-keyword">return</span> img, label<br></code></pre></td></tr></table></figure><p>其中self.transform(img)会调用Compose的__call__()函数：</p><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs ruby"><span class="hljs-keyword">def</span> <span class="hljs-title function_">__call__</span>(<span class="hljs-params"><span class="hljs-variable language_">self</span>, img</span>):<br>    <span class="hljs-keyword">for</span> t <span class="hljs-keyword">in</span> <span class="hljs-variable language_">self</span>.<span class="hljs-symbol">transforms:</span><br>        img = t(img)<br>    <span class="hljs-keyword">return</span> img<br></code></pre></td></tr></table></figure><p>可以看到，这里是遍历transforms中的函数，按顺序应用到 img 中。</p><p><strong>transforms.Normalize</strong></p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs routeros">torchvision.transforms.Normalize(mean, std, <span class="hljs-attribute">inplace</span>=<span class="hljs-literal">False</span>)<br></code></pre></td></tr></table></figure><p>功能：逐 channel 地对图像进行标准化<br>output &#x3D; ( input - mean ) &#x2F; std</p><ul><li>mean: 各通道的均值</li><li>std: 各通道的标准差</li><li>inplace: 是否原地操作<br>该方法调用的是F.normalize(tensor, self.mean, self.std, self.inplace)<br>而<code>F.normalize()</code>方法如下：</li></ul><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs routeros">def normalize(tensor, mean, std, <span class="hljs-attribute">inplace</span>=<span class="hljs-literal">False</span>):<br>    <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> _is_tensor_image(tensor):<br>        raise TypeError(<span class="hljs-string">&#x27;tensor is not a torch image.&#x27;</span>)<br><br>    <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> inplace:<br>        tensor = tensor.clone()<br><br>    dtype = tensor.dtype<br>    mean = torch.as_tensor(mean, <span class="hljs-attribute">dtype</span>=dtype, <span class="hljs-attribute">device</span>=tensor.device)<br>    std = torch.as_tensor(std, <span class="hljs-attribute">dtype</span>=dtype, <span class="hljs-attribute">device</span>=tensor.device)<br>    tensor.sub_(mean[:, None, None]).div_(std[:, None, None])<br>    return tensor<br></code></pre></td></tr></table></figure><p>首先判断是否为 tensor，如果不是 tensor 则抛出异常。然后根据inplace是否为 true 进行 clone，接着把mean 和 std 都转换为tensor (原本是 list)，最后减去均值除以方差：tensor.sub_(mean[:, None, None]).div_(std[:, None, None])<br>对数据进行均值为 0，标准差为 1 的标准化，可以加快模型的收敛。</p><p>在逻辑回归的实验中，我们的数据生成代码如下：</p><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs ini"><span class="hljs-attr">sample_nums</span> = <span class="hljs-number">100</span><br><span class="hljs-attr">mean_value</span> = <span class="hljs-number">1.7</span><br><span class="hljs-attr">bias</span> = <span class="hljs-number">1</span><br><span class="hljs-attr">n_data</span> = torch.<span class="hljs-literal">on</span>es(sample_nums, <span class="hljs-number">2</span>)<br><span class="hljs-comment"># 使用正态分布随机生成样本，均值为张量，方差为标量</span><br><span class="hljs-attr">x0</span> = torch.normal(mean_value * n_data, <span class="hljs-number">1</span>) + bias      <span class="hljs-comment"># 类别0 数据 shape=(100, 2)</span><br><span class="hljs-comment"># 生成对应标签</span><br><span class="hljs-attr">y0</span> = torch.zeros(sample_nums)                         <span class="hljs-comment"># 类别0 标签 shape=(100, 1)</span><br><span class="hljs-comment"># 使用正态分布随机生成样本，均值为张量，方差为标量</span><br><span class="hljs-attr">x1</span> = torch.normal(-mean_value * n_data, <span class="hljs-number">1</span>) + bias     <span class="hljs-comment"># 类别1 数据 shape=(100, 2)</span><br><span class="hljs-comment"># 生成对应标签</span><br><span class="hljs-attr">y1</span> = torch.<span class="hljs-literal">on</span>es(sample_nums)                          <span class="hljs-comment"># 类别1 标签 shape=(100, 1)</span><br><span class="hljs-attr">train_x</span> = torch.cat((x0, x1), <span class="hljs-number">0</span>)<br><span class="hljs-attr">train_y</span> = torch.cat((y0, y1), <span class="hljs-number">0</span>)<br></code></pre></td></tr></table></figure><h2 id="二十二种-transforms-图片数据预处理方法"><a href="#二十二种-transforms-图片数据预处理方法" class="headerlink" title="二十二种 transforms 图片数据预处理方法"></a>二十二种 transforms 图片数据预处理方法</h2><p>这篇主要分为几个部分介绍 transforms:</p><ul><li>裁剪</li><li>旋转和翻转</li><li>图像变换</li><li>transforms 方法操作</li><li>自定义 transforms 方法</li></ul><p>由于图片经过 transform 操作之后是 tensor，像素值在 0~1 之间，并且标准差和方差不是正常图片的。所以定义了transform_invert()方法。功能是对 tensor 进行反标准化操作，并且把 tensor 转换为 image，方便可视化。</p><p>我们主要修改的是transforms.Compose代码块中的内容，其中transforms.Resize((224, 224))是把图片缩放到 (224, 224) 大小 (下面的所有操作都是基于缩放之后的图片进行的)，然后再进行其他 transform 操作。</p><p>原图如下：<br><img src="/img/pytorchtrain2/3.jpg"></p><p>经过缩放之后，图片如下：<br><img src="/img/pytorchtrain2/4.png"></p><h3 id="裁剪"><a href="#裁剪" class="headerlink" title="裁剪"></a>裁剪</h3><h4 id="transforms-CenterCrop"><a href="#transforms-CenterCrop" class="headerlink" title="transforms.CenterCrop"></a>transforms.CenterCrop</h4><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs stylus">torchvision<span class="hljs-selector-class">.transforms</span><span class="hljs-selector-class">.CenterCrop</span>(size)<br></code></pre></td></tr></table></figure><p>功能：从图像中心裁剪图片</p><ul><li>size: 所需裁剪的图片尺寸</li></ul><p>transforms.CenterCrop(196)的效果如下：<br><img src="/img/pytorchtrain2/5.png"></p><p>如果裁剪的 size 比原图大，那么会填充值为 0 的像素。transforms.CenterCrop(512)的效果如下：<br><img src="/img/pytorchtrain2/6.png"></p><h4 id="transforms-RandomCrop"><a href="#transforms-RandomCrop" class="headerlink" title="transforms.RandomCrop"></a>transforms.RandomCrop</h4><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs routeros">torchvision.transforms.RandomCrop(size, <span class="hljs-attribute">padding</span>=None, <span class="hljs-attribute">pad_if_needed</span>=<span class="hljs-literal">False</span>, <span class="hljs-attribute">fill</span>=0, <span class="hljs-attribute">padding_mode</span>=<span class="hljs-string">&#x27;constant&#x27;</span>)<br></code></pre></td></tr></table></figure><p>功能：从图片中随机裁剪出尺寸为 size 的图片，如果有 padding，那么先进行 padding，再随机裁剪 size 大小的图片。</p><ul><li>size</li><li>padding: 设置填充大小<ul><li>当为 a 时，上下左右均填充 a 个像素</li><li>当为 (a, b) 时，左右填充 a 个像素，上下填充 b 个像素</li><li>当为 (a, b, c, d) 时，左上右下分别填充 a，b，c，d</li></ul></li><li>pad_if_need: 当图片小于设置的 size，是否填充</li><li>padding_mode:<ul><li>constant: 像素值由 fill 设定</li><li>edge: 像素值由图像边缘像素设定</li><li>reflect: 镜像填充，最后一个像素不镜像。([1,2,3,4] -&gt; [3,2,1,2,3,4,3,2])</li><li>symmetric: 镜像填充，最后一个像素也镜像。([1,2,3,4] -&gt; [2,1,1,2,3,4,4,4,3])</li></ul></li><li>fill: 当 padding_mode 为 constant 时，设置填充的像素值<br>transforms.RandomCrop(224, padding&#x3D;16)的效果如下，这里的 padding 为 16，所以会先在 4 边进行 16 的padding，默认填充 0，然后随机裁剪出 (224,224) 大小的图片，这里裁剪了左上角的区域。</li></ul><h4 id="transforms-RandomResizedCrop"><a href="#transforms-RandomResizedCrop" class="headerlink" title="transforms.RandomResizedCrop"></a>transforms.RandomResizedCrop</h4><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">torchvision</span>.transforms.RandomResizedCrop(size, scale=(<span class="hljs-number">0</span>.<span class="hljs-number">08</span>, <span class="hljs-number">1</span>.<span class="hljs-number">0</span>), ratio=(<span class="hljs-number">0</span>.<span class="hljs-number">75</span>, <span class="hljs-number">1</span>.<span class="hljs-number">3333333333333333</span>), interpolation=<span class="hljs-number">2</span>)<br></code></pre></td></tr></table></figure><p>功能：随机大小、随机宽高比裁剪图片。首先根据 scale 的比例裁剪原图，然后根据 ratio 的长宽比再裁剪，最后使用插值法把图片变换为 size 大小。</p><ul><li>size: 裁剪的图片尺寸</li><li>scale: 随机缩放面积比例，默认随机选取 (0.08, 1) 之间的一个数</li><li>ratio: 随机长宽比，默认随机选取 ($\displaystyle\frac{3}{4}$, $\displaystyle\frac{4}{3}$ ) 之间的一个数。因为超过这个比例会有明显的失真</li><li>interpolation: 当裁剪出来的图片小于 size 时，就要使用插值方法 resize<ul><li>PIL.Image.NEAREST</li><li>PIL.Image.BILINEAR</li><li>PIL.Image.BICUBIC</li></ul></li></ul><h4 id="transforms-FiveCrop-TenCrop"><a href="#transforms-FiveCrop-TenCrop" class="headerlink" title="transforms.FiveCrop(TenCrop)"></a>transforms.FiveCrop(TenCrop)</h4><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs stylus">torchvision<span class="hljs-selector-class">.transforms</span><span class="hljs-selector-class">.FiveCrop</span>(size)<br>torchvision<span class="hljs-selector-class">.transforms</span><span class="hljs-selector-class">.TenCrop</span>(size, vertical_flip=False)<br></code></pre></td></tr></table></figure><p>功能：FiveCrop在图像的上下左右以及中心裁剪出尺寸为 size 的 5 张图片。Tencrop对这 5 张图片进行水平或者垂直镜像获得 10 张图片。</p><ul><li>size: 最后裁剪的图片尺寸</li><li>vertical_flip: 是否垂直翻转<br>由于这两个方法返回的是 tuple，每个元素表示一个图片，我们还需要把这个 tuple 转换为一张图片的tensor。代码如下：</li></ul><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs stylus">transforms<span class="hljs-selector-class">.FiveCrop</span>(<span class="hljs-number">112</span>),<br>transforms<span class="hljs-selector-class">.Lambda</span>(lambda crops: torch<span class="hljs-selector-class">.stack</span>(<span class="hljs-selector-attr">[(transforms.ToTensor()(crop)) for crop in crops]</span>))<br></code></pre></td></tr></table></figure><p>并且把transforms.Compose中最后两行注释：</p><figure class="highlight scss"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs scss"># transforms<span class="hljs-selector-class">.ToTensor</span>(), # <span class="hljs-built_in">ToTensor</span>()接收的参数是 <span class="hljs-selector-tag">Image</span>，由于上面已经进行了 <span class="hljs-built_in">ToTensor</span>(), 因此这里注释<br># transforms<span class="hljs-selector-class">.Normalize</span>(norm_mean, norm_std), # 由于是 <span class="hljs-number">4</span> 维的 Tensor，因此不能执行 <span class="hljs-built_in">Normalize</span>() 方法<br></code></pre></td></tr></table></figure><ul><li>transforms.ToTensor()接收的参数是 Image，由于上面已经进行了 ToTensor()。因此注释这一行。</li><li>transforms.Normalize()方法接收的是 3 维的 tensor (在 _is_tensor_image()方法 里检查是否满足这一条件，不满足则报错)，而经过transforms.FiveCrop返回的是 4 维张量，因此注释这一行。</li></ul><p>最后的 tensor 形状是 [ncrops, c, h, w]，图片可视化的代码也需要做修改：</p><figure class="highlight pgsql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs pgsql">## 展示 FiveCrop 和 TenCrop 的图片<br>ncrops, c, h, w = img_tensor.shape<br><span class="hljs-keyword">columns</span>=<span class="hljs-number">2</span> # 两列<br><span class="hljs-keyword">rows</span>= math.ceil(ncrops/<span class="hljs-number">2</span>) # 计算多少行<br># 把每个 tensor ([c,h,w]) 转换为 image<br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(ncrops):<br>    img = transform_invert(img_tensor[i], train_transform)<br>    plt.subplot(<span class="hljs-keyword">rows</span>, <span class="hljs-keyword">columns</span>, i+<span class="hljs-number">1</span>)<br>    plt.imshow(img)<br>plt.<span class="hljs-keyword">show</span>()<br></code></pre></td></tr></table></figure><h3 id="旋转和翻转"><a href="#旋转和翻转" class="headerlink" title="旋转和翻转"></a>旋转和翻转</h3><h4 id="transforms-RandomHorizontalFlip-RandomVerticalFlip"><a href="#transforms-RandomHorizontalFlip-RandomVerticalFlip" class="headerlink" title="transforms.RandomHorizontalFlip(RandomVerticalFlip)"></a>transforms.RandomHorizontalFlip(RandomVerticalFlip)</h4><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs css">transforms<span class="hljs-selector-class">.RandomHorizontalFlip</span>(<span class="hljs-selector-tag">p</span>)<br></code></pre></td></tr></table></figure><p>功能：根据概率，在水平或者垂直方向翻转图片</p><ul><li>p: 翻转概率</li></ul><p>transforms.RandomHorizontalFlip(p&#x3D;0.5)，那么一半的图片会被水平翻转。</p><p>transforms.RandomHorizontalFlip(p&#x3D;1)，那么所有图片会被水平翻转。</p><h4 id="transforms-RandomRotation"><a href="#transforms-RandomRotation" class="headerlink" title="transforms.RandomRotation"></a>transforms.RandomRotation</h4><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs routeros">torchvision.transforms.RandomRotation(degrees, <span class="hljs-attribute">resample</span>=<span class="hljs-literal">False</span>, <span class="hljs-attribute">expand</span>=<span class="hljs-literal">False</span>, <span class="hljs-attribute">center</span>=None, <span class="hljs-attribute">fill</span>=None)<br></code></pre></td></tr></table></figure><p>功能：随机旋转图片</p><ul><li>degree: 旋转角度<ul><li>当为 a 时，在 (-a, a) 之间随机选择旋转角度</li><li>当为 (a, b) 时，在 (a, b) 之间随机选择旋转角度</li></ul></li><li>resample: 重采样方法</li><li>expand: 是否扩大矩形框，以保持原图信息。根据中心旋转点计算扩大后的图片。如果旋转点不是中心，即使设置 expand &#x3D; True，还是会有部分信息丢失。</li><li>center: 旋转点设置，是坐标，默认中心旋转。如设置左上角为：(0, 0)</li></ul><h3 id="图像变换"><a href="#图像变换" class="headerlink" title="图像变换"></a>图像变换</h3><h4 id="Pad"><a href="#Pad" class="headerlink" title="Pad"></a>Pad</h4><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs routeros">torchvision.transforms.Pad(padding, <span class="hljs-attribute">fill</span>=0, <span class="hljs-attribute">padding_mode</span>=<span class="hljs-string">&#x27;constant&#x27;</span>)<br></code></pre></td></tr></table></figure><p>功能：对图像边缘进行填充</p><ul><li>padding: 设置填充大小<ul><li>当为 a 时，上下左右均填充 a 个像素</li><li>当为 (a, b) 时，左右填充 a 个像素，上下填充 b 个像素</li><li>当为 (a, b, c, d) 时，左上右下分别填充 a，b，c，d</li><li>padding_mode: 填充模式，有 4 种模式，constant、edge、reflect、symmetric</li><li>fill: 当 padding_mode 为 constant 时，设置填充的像素值，(R, G, B) 或者 (Gray)</li></ul></li></ul><h4 id="torchvision-transforms-ColorJitter"><a href="#torchvision-transforms-ColorJitter" class="headerlink" title="torchvision.transforms.ColorJitter"></a>torchvision.transforms.ColorJitter</h4><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs routeros">torchvision.transforms.ColorJitter(<span class="hljs-attribute">brightness</span>=0, <span class="hljs-attribute">contrast</span>=0, <span class="hljs-attribute">saturation</span>=0, <span class="hljs-attribute">hue</span>=0)<br></code></pre></td></tr></table></figure><p>功能：调整亮度、对比度、饱和度、色相。在照片的拍照过程中，可能会由于设备、光线问题，造成色彩上的偏差，因此需要调整这些属性，抵消这些因素带来的扰动。</p><ul><li>brightness: 亮度调整因子</li><li>contrast: 对比度参数</li><li>saturation: 饱和度参数</li><li>brightness、contrast、saturation参数：当为 a 时，从 [max(0, 1-a), 1+a] 中随机选择；当为 (a, b) 时，从 [a, b] 中选择。</li><li>hue: 色相参数<ul><li>当为 a 时，从 [-a, a] 中选择参数。其中 $0\le a \le 0.5$。</li><li>当为 (a, b) 时，从 [a, b] 中选择参数。其中 $0 \le a \le b \le 0.5$。</li></ul></li></ul><h4 id="transforms-Grayscale-RandomGrayscale"><a href="#transforms-Grayscale-RandomGrayscale" class="headerlink" title="transforms.Grayscale(RandomGrayscale)"></a>transforms.Grayscale(RandomGrayscale)</h4><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs routeros">torchvision.transforms.Grayscale(<span class="hljs-attribute">num_output_channels</span>=1)<br></code></pre></td></tr></table></figure><p>功能：将图片转换为灰度图</p><ul><li>num_output_channels: 输出的通道数。只能设置为 1 或者 3 (如果在后面使用了transforms.Normalize，则要设置为 3，因为transforms.Normalize只能接收 3 通道的输入)</li></ul><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs routeros">torchvision.transforms.RandomGrayscale(<span class="hljs-attribute">p</span>=0.1, <span class="hljs-attribute">num_output_channels</span>=1)<br></code></pre></td></tr></table></figure><ul><li>p: 概率值，图像被转换为灰度图的概率</li><li>num_output_channels: 输出的通道数。只能设置为 1 或者 3</li></ul><p>功能：根据一定概率将图片转换为灰度图</p><h4 id="transforms-RandomAffine"><a href="#transforms-RandomAffine" class="headerlink" title="transforms.RandomAffine"></a>transforms.RandomAffine</h4><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs routeros">torchvision.transforms.RandomAffine(degrees, <span class="hljs-attribute">translate</span>=None, <span class="hljs-attribute">scale</span>=None, <span class="hljs-attribute">shear</span>=None, <span class="hljs-attribute">resample</span>=<span class="hljs-literal">False</span>, <span class="hljs-attribute">fillcolor</span>=0)<br></code></pre></td></tr></table></figure><p>功能：对图像进行仿射变换，仿射变换是 2 维的线性变换，由 5 种基本操作组成，分别是旋转、平移、缩放、错切和翻转。</p><ul><li>degree: 旋转角度设置</li><li>translate: 平移区间设置，如 (a, b)，a 设置宽 (width)，b 设置高 (height)。图像在宽维度平移的区间为 $- img_width \times a &lt; dx &lt; img_width \times a$，高同理</li><li>scale: 缩放比例，以面积为单位</li><li>fillcolor: 填充颜色设置</li><li>shear: 错切角度设置，有水平错切和垂直错切<ul><li>若为 a，则仅在 x 轴错切，在 (-a, a) 之间随机选择错切角度</li><li>若为 (a, b)，x 轴在 (-a, a) 之间随机选择错切角度，y 轴在 (-b, b) 之间随机选择错切角度</li><li>若为 (a, b, c, d)，x 轴在 (a, b) 之间随机选择错切角度，y 轴在 (c, d) 之间随机选择错切角度</li></ul></li><li>resample: 重采样方式，有 NEAREST、BILINEAR、BICUBIC。</li></ul><h4 id="transforms-RandomErasing"><a href="#transforms-RandomErasing" class="headerlink" title="transforms.RandomErasing"></a>transforms.RandomErasing</h4><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">torchvision</span>.transforms.RandomErasing(p=<span class="hljs-number">0</span>.<span class="hljs-number">5</span>, scale=(<span class="hljs-number">0</span>.<span class="hljs-number">02</span>, <span class="hljs-number">0</span>.<span class="hljs-number">33</span>), ratio=(<span class="hljs-number">0</span>.<span class="hljs-number">3</span>, <span class="hljs-number">3</span>.<span class="hljs-number">3</span>), value=<span class="hljs-number">0</span>, inplace=False)<br></code></pre></td></tr></table></figure><p>功能：对图像进行随机遮挡。这个操作接收的输入是 tensor。因此在此之前需要先执行transforms.ToTensor()。同时注释掉后面的transforms.ToTensor()。</p><ul><li>p: 概率值，执行该操作的概率</li><li>scale: 遮挡区域的面积。如(a, b)，则会随机选择 (a, b) 中的一个遮挡比例</li><li>ratio: 遮挡区域长宽比。如(a, b)，则会随机选择 (a, b) 中的一个长宽比</li><li>value: 设置遮挡区域的像素值。(R, G, B) 或者 Gray，或者任意字符串。由于之前执行了transforms.ToTensor()，像素值归一化到了 0~1 之间，因此这里设置的 (R, G, B) 要除以 255</li></ul><h4 id="transforms-Lambda"><a href="#transforms-Lambda" class="headerlink" title="transforms.Lambda"></a>transforms.Lambda</h4><p>自定义 transform 方法。在上面的FiveCrop中就用到了transforms.Lambda。</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs stylus">transforms<span class="hljs-selector-class">.FiveCrop</span>(<span class="hljs-number">112</span>, vertical_flip=False),<br>transforms<span class="hljs-selector-class">.Lambda</span>(lambda crops: torch<span class="hljs-selector-class">.stack</span>(<span class="hljs-selector-attr">[(transforms.ToTensor()(crop)) for crop in crops]</span>))<br></code></pre></td></tr></table></figure><p>transforms.FiveCrop返回的是长度为 5 的 tuple，因此需要使用transforms.Lambda 把 tuple 转换为 4D 的 tensor。</p><h3 id="transforms-的操作"><a href="#transforms-的操作" class="headerlink" title="transforms 的操作"></a>transforms 的操作</h3><h4 id="torchvision-transforms-RandomChoice"><a href="#torchvision-transforms-RandomChoice" class="headerlink" title="torchvision.transforms.RandomChoice"></a>torchvision.transforms.RandomChoice</h4><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs stylus">torchvision<span class="hljs-selector-class">.transforms</span><span class="hljs-selector-class">.RandomChoice</span>(<span class="hljs-selector-attr">[transforms1, transforms2, transforms3]</span>)<br></code></pre></td></tr></table></figure><p>功能：从一系列 transforms 方法中随机选择一个</p><h4 id="transforms-RandomApply"><a href="#transforms-RandomApply" class="headerlink" title="transforms.RandomApply"></a>transforms.RandomApply</h4><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs stylus">torchvision<span class="hljs-selector-class">.transforms</span><span class="hljs-selector-class">.RandomApply</span>(<span class="hljs-selector-attr">[transforms1, transforms2, transforms3]</span>, p=<span class="hljs-number">0.5</span>)<br></code></pre></td></tr></table></figure><p>功能：根据概率执行一组 transforms 操作，要么全部执行，要么全部不执行。</p><h4 id="transforms-RandomOrder"><a href="#transforms-RandomOrder" class="headerlink" title="transforms.RandomOrder"></a>transforms.RandomOrder</h4><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs stylus">transforms<span class="hljs-selector-class">.RandomOrder</span>(<span class="hljs-selector-attr">[transforms1, transforms2, transforms3]</span>)<br></code></pre></td></tr></table></figure><p>功能：对一组 transforms 操作打乱顺序</p><h3 id="自定义transforms"><a href="#自定义transforms" class="headerlink" title="自定义transforms"></a>自定义transforms</h3><p>自定义 transforms 有两个要素：仅接受一个参数，返回一个参数；注意上下游的输入与输出，上一个 transform 的输出是下一个 transform 的输入。</p><p>我们这里通过自定义 transforms 实现椒盐噪声。椒盐噪声又称为脉冲噪声，是一种随机出现的白点或者黑点，白点称为盐噪声，黑点称为椒噪声。信噪比 (Signal-Noise Rate，SNR) 是衡量噪声的比例，图像中正常像素占全部像素的占比。</p><p>我们定义一个AddPepperNoise类，作为添加椒盐噪声的 transform。在构造函数中传入信噪比和概率，在__call__()函数中执行具体的逻辑，返回的是 image。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> random<br><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image<br><br><span class="hljs-comment"># 自定义添加椒盐噪声的 transform</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">AddPepperNoise</span>(<span class="hljs-title class_ inherited__">object</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;增加椒盐噪声</span><br><span class="hljs-string">    Args:</span><br><span class="hljs-string">        snr （float）: Signal Noise Rate</span><br><span class="hljs-string">        p (float): 概率值，依概率执行该操作</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, snr, p=<span class="hljs-number">0.9</span></span>):<br>        <span class="hljs-keyword">assert</span> <span class="hljs-built_in">isinstance</span>(snr, <span class="hljs-built_in">float</span>) <span class="hljs-keyword">or</span> (<span class="hljs-built_in">isinstance</span>(p, <span class="hljs-built_in">float</span>))<br>        <span class="hljs-variable language_">self</span>.snr = snr<br>        <span class="hljs-variable language_">self</span>.p = p<br><br>    <span class="hljs-comment"># transform 会调用该方法</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__call__</span>(<span class="hljs-params">self, img</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        Args:</span><br><span class="hljs-string">            img (PIL Image): PIL Image</span><br><span class="hljs-string">        Returns:</span><br><span class="hljs-string">            PIL Image: PIL image.</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        <span class="hljs-comment"># 如果随机概率小于 seld.p，则执行 transform</span><br>        <span class="hljs-keyword">if</span> random.uniform(<span class="hljs-number">0</span>, <span class="hljs-number">1</span>) &lt; <span class="hljs-variable language_">self</span>.p:<br>            <span class="hljs-comment"># 把 image 转为 array</span><br>            img_ = np.array(img).copy()<br>            <span class="hljs-comment"># 获得 shape</span><br>            h, w, c = img_.shape<br>            <span class="hljs-comment"># 信噪比</span><br>            signal_pct = <span class="hljs-variable language_">self</span>.snr<br>            <span class="hljs-comment"># 椒盐噪声的比例 = 1 -信噪比</span><br>            noise_pct = (<span class="hljs-number">1</span> - <span class="hljs-variable language_">self</span>.snr)<br>            <span class="hljs-comment"># 选择的值为 (0, 1, 2)，每个取值的概率分别为 [signal_pct, noise_pct/2., noise_pct/2.]</span><br>            <span class="hljs-comment"># 椒噪声和盐噪声分别占 noise_pct 的一半</span><br>            <span class="hljs-comment"># 1 为盐噪声，2 为 椒噪声</span><br>            mask = np.random.choice((<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>), size=(h, w, <span class="hljs-number">1</span>), p=[signal_pct, noise_pct/<span class="hljs-number">2.</span>, noise_pct/<span class="hljs-number">2.</span>])<br>            mask = np.repeat(mask, c, axis=<span class="hljs-number">2</span>)<br>            img_[mask == <span class="hljs-number">1</span>] = <span class="hljs-number">255</span>   <span class="hljs-comment"># 盐噪声</span><br>            img_[mask == <span class="hljs-number">2</span>] = <span class="hljs-number">0</span>     <span class="hljs-comment"># 椒噪声</span><br>            <span class="hljs-comment"># 再转换为 image</span><br>            <span class="hljs-keyword">return</span> Image.fromarray(img_.astype(<span class="hljs-string">&#x27;uint8&#x27;</span>)).convert(<span class="hljs-string">&#x27;RGB&#x27;</span>)<br>        <span class="hljs-comment"># 如果随机概率大于 seld.p，则直接返回原图</span><br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-keyword">return</span> img<br></code></pre></td></tr></table></figure><h3 id="综合代码"><a href="#综合代码" class="headerlink" title="综合代码"></a>综合代码</h3><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br></pre></td><td class="code"><pre><code class="hljs routeros"><span class="hljs-comment"># -*- coding: utf-8 -*-</span><br><br>import os<br>import numpy as np<br>import torch<br>import random<br>import math<br>import torchvision.transforms as transforms<br><span class="hljs-keyword">from</span> PIL import Image<br><span class="hljs-keyword">from</span> matplotlib import pyplot as plt<br><span class="hljs-keyword">from</span> enviroments import rmb_split_dir<br><span class="hljs-keyword">from</span> lesson2.transforms.addPepperNoise import AddPepperNoise<br>def set_seed(<span class="hljs-attribute">seed</span>=1):<br>    random.seed(seed)<br>    np.random.seed(seed)<br>    torch.manual_seed(seed)<br>    torch.cuda.manual_seed(seed)<br><br>set_seed(1)  # 设置随机种子<br><br><span class="hljs-comment"># 参数设置</span><br>MAX_EPOCH = 10<br>BATCH_SIZE = 1<br>LR = 0.01<br>log_interval = 10<br>val_interval = 1<br>rmb_label = &#123;<span class="hljs-string">&quot;1&quot;</span>: 0, <span class="hljs-string">&quot;100&quot;</span>: 1&#125;<br><br><span class="hljs-comment">#对 tensor 进行反标准化操作，并且把 tensor 转换为 image，方便可视化。</span><br>def transform_invert(img_, transform_train):<br>    <span class="hljs-string">&quot;&quot;</span><span class="hljs-string">&quot;</span><br><span class="hljs-string">    将data 进行反transfrom操作</span><br><span class="hljs-string">    :param img_: tensor</span><br><span class="hljs-string">    :param transform_train: torchvision.transforms</span><br><span class="hljs-string">    :return: PIL image</span><br><span class="hljs-string">    &quot;</span><span class="hljs-string">&quot;&quot;</span><br><br>    # 如果有标准化操作<br>    <span class="hljs-keyword">if</span> <span class="hljs-string">&#x27;Normalize&#x27;</span> <span class="hljs-keyword">in</span> str(transform_train):<br>        # 取出标准化的 transform<br>        norm_transform = list(filter(lambda x: isinstance(x, transforms.Normalize), transform_train.transforms))<br>        # 取出均值<br>        mean = torch.tensor(norm_transform[0].mean, <span class="hljs-attribute">dtype</span>=img_.dtype, <span class="hljs-attribute">device</span>=img_.device)<br>        # 取出标准差<br>        std = torch.tensor(norm_transform[0].std, <span class="hljs-attribute">dtype</span>=img_.dtype, <span class="hljs-attribute">device</span>=img_.device)<br>        # 乘以标准差，加上均值<br>        img_.mul_(std[:, None, None]).add_(mean[:, None, None])<br><br>    # 把 C*H*W 变为 H*W<span class="hljs-number">*C</span><br>    img_ = img_.transpose(0, 2).transpose(0, 1)  # C*H*W --&gt; H*W<span class="hljs-number">*C</span><br>    # 把 0~1 的值变为 0~255<br>    img_ = np.array(img_) * 255<br><br>    # 如果是 RGB 图<br>    <span class="hljs-keyword">if</span> img_.shape[2] == 3:<br>        img_ = Image.fromarray(img_.astype(<span class="hljs-string">&#x27;uint8&#x27;</span>)).convert(<span class="hljs-string">&#x27;RGB&#x27;</span>)<br>        # 如果是灰度图<br>    elif img_.shape[2] == 1:<br>        img_ = Image.fromarray(img_.astype(<span class="hljs-string">&#x27;uint8&#x27;</span>).squeeze())<br>    <span class="hljs-keyword">else</span>:<br>        raise Exception(<span class="hljs-string">&quot;Invalid img shape, expected 1 or 3 in axis 2, but got &#123;&#125;!&quot;</span>.format(img_.shape[2]) )<br><br>    return img_<br><br><br>norm_mean = [0.485, 0.456, 0.406]<br>norm_std = [0.229, 0.224, 0.225]<br><br>train_transform = transforms.Compose([<br>    # 缩放到 (224, 224) 大小，会拉伸<br>    transforms.Resize((224, 224)),<br><br>    # 1 CenterCrop 中心裁剪<br>    # transforms.CenterCrop(512),     # 512<br>    # transforms.CenterCrop(196),     # 512<br><br>    # 2 RandomCrop<br>    # transforms.RandomCrop(224, <span class="hljs-attribute">padding</span>=16),<br>    # transforms.RandomCrop(224, padding=(16, 64)),<br>    # transforms.RandomCrop(224, <span class="hljs-attribute">padding</span>=16, fill=(255, 0, 0)),<br>    # transforms.RandomCrop(512, <span class="hljs-attribute">pad_if_needed</span>=<span class="hljs-literal">True</span>),   # <span class="hljs-attribute">pad_if_needed</span>=<span class="hljs-literal">True</span><br>    # transforms.RandomCrop(224, <span class="hljs-attribute">padding</span>=64, <span class="hljs-attribute">padding_mode</span>=<span class="hljs-string">&#x27;edge&#x27;</span>),<br>    # transforms.RandomCrop(224, <span class="hljs-attribute">padding</span>=64, <span class="hljs-attribute">padding_mode</span>=<span class="hljs-string">&#x27;reflect&#x27;</span>),<br>    # transforms.RandomCrop(1024, <span class="hljs-attribute">padding</span>=1024, <span class="hljs-attribute">padding_mode</span>=<span class="hljs-string">&#x27;symmetric&#x27;</span>),<br><br>    # 3 RandomResizedCrop<br>    # transforms.RandomResizedCrop(<span class="hljs-attribute">size</span>=224, scale=(0.08, 1)),<br>    # transforms.RandomResizedCrop(<span class="hljs-attribute">size</span>=224, scale=(0.5, 0.5)),<br><br>    # 4 FiveCrop<br>    # transforms.FiveCrop(112),<br>    # 返回的是 tuple，因此需要转换为 tensor<br>    # transforms.Lambda(lambda crops: torch.stack([(transforms.ToTensor()(crop)) <span class="hljs-keyword">for</span> crop <span class="hljs-keyword">in</span> crops])),<br><br>    # 5 TenCrop<br>    # transforms.TenCrop(112, <span class="hljs-attribute">vertical_flip</span>=<span class="hljs-literal">False</span>),<br>    # transforms.Lambda(lambda crops: torch.stack([(transforms.ToTensor()(crop)) <span class="hljs-keyword">for</span> crop <span class="hljs-keyword">in</span> crops])),<br><br>    # 1 Horizontal Flip<br>    # transforms.RandomHorizontalFlip(<span class="hljs-attribute">p</span>=1),<br><br>    # 2 Vertical Flip<br>    # transforms.RandomVerticalFlip(<span class="hljs-attribute">p</span>=1),<br><br>    # 3 RandomRotation<br>    # transforms.RandomRotation(90),<br>    # transforms.RandomRotation((90), <span class="hljs-attribute">expand</span>=<span class="hljs-literal">True</span>),<br>    # transforms.RandomRotation(30, center=(0, 0)),<br>    # transforms.RandomRotation(30, center=(0, 0), <span class="hljs-attribute">expand</span>=<span class="hljs-literal">True</span>),   # expand only <span class="hljs-keyword">for</span> center rotation<br><br><br>    # 1 Pad<br>    # transforms.Pad(<span class="hljs-attribute">padding</span>=32, fill=(255, 0, 0), <span class="hljs-attribute">padding_mode</span>=<span class="hljs-string">&#x27;constant&#x27;</span>),<br>    # transforms.Pad(padding=(8, 64), fill=(255, 0, 0), <span class="hljs-attribute">padding_mode</span>=<span class="hljs-string">&#x27;constant&#x27;</span>),<br>    # transforms.Pad(padding=(8, 16, 32, 64), fill=(255, 0, 0), <span class="hljs-attribute">padding_mode</span>=<span class="hljs-string">&#x27;constant&#x27;</span>),<br>    # transforms.Pad(padding=(8, 16, 32, 64), fill=(255, 0, 0), <span class="hljs-attribute">padding_mode</span>=<span class="hljs-string">&#x27;symmetric&#x27;</span>),<br><br>    # 2 ColorJitter<br>    # transforms.ColorJitter(<span class="hljs-attribute">brightness</span>=0.5),<br>    # transforms.ColorJitter(<span class="hljs-attribute">contrast</span>=0.5),<br>    # transforms.ColorJitter(<span class="hljs-attribute">saturation</span>=0.5),<br>    # transforms.ColorJitter(<span class="hljs-attribute">hue</span>=0.3),<br><br>    # 3 Grayscale<br>    # transforms.Grayscale(<span class="hljs-attribute">num_output_channels</span>=3),<br><br>    # 4 Affine<br>    # transforms.RandomAffine(<span class="hljs-attribute">degrees</span>=30),<br>    # transforms.RandomAffine(<span class="hljs-attribute">degrees</span>=0, translate=(0.2, 0.2), fillcolor=(255, 0, 0)),<br>    # transforms.RandomAffine(<span class="hljs-attribute">degrees</span>=0, scale=(0.7, 0.7)),<br>    # transforms.RandomAffine(<span class="hljs-attribute">degrees</span>=0, shear=(0, 0, 0, 45)),<br>    # transforms.RandomAffine(<span class="hljs-attribute">degrees</span>=0, <span class="hljs-attribute">shear</span>=90, fillcolor=(255, 0, 0)),<br><br>    # 5 Erasing<br>    # transforms.ToTensor(),<br>    # transforms.RandomErasing(<span class="hljs-attribute">p</span>=1, scale=(0.02, 0.33), ratio=(0.3, 3.3), value=(254/255, 0, 0)),<br>    # transforms.RandomErasing(<span class="hljs-attribute">p</span>=1, scale=(0.02, 0.33), ratio=(0.3, 3.3), <span class="hljs-attribute">value</span>=<span class="hljs-string">&#x27;fads43&#x27;</span>),<br><br>    # 1 RandomChoice<br>    # transforms.RandomChoice([transforms.RandomVerticalFlip(<span class="hljs-attribute">p</span>=1), transforms.RandomHorizontalFlip(<span class="hljs-attribute">p</span>=1)]),<br><br>    # 2 RandomApply<br>    # transforms.RandomApply([transforms.RandomAffine(<span class="hljs-attribute">degrees</span>=0, <span class="hljs-attribute">shear</span>=45, fillcolor=(255, 0, 0)),<br>    #                         transforms.Grayscale(<span class="hljs-attribute">num_output_channels</span>=3)], <span class="hljs-attribute">p</span>=0.5),<br>    # 3 RandomOrder<br>    # transforms.RandomOrder([transforms.RandomRotation(15),<br>    #                         transforms.Pad(<span class="hljs-attribute">padding</span>=32),<br>    #                         transforms.RandomAffine(<span class="hljs-attribute">degrees</span>=0, translate=(0.01, 0.1), scale=(0.9, 1.1))]),<br><br>    AddPepperNoise(0.9, <span class="hljs-attribute">p</span>=0.5),<br>    transforms.ToTensor(),<br>    transforms.Normalize(norm_mean, norm_std),<br>])<br><br><span class="hljs-attribute">path_img</span>=os.path.join(rmb_split_dir, <span class="hljs-string">&quot;train&quot;</span>, <span class="hljs-string">&quot;100&quot;</span>,<span class="hljs-string">&quot;0A4DSPGE.jpg&quot;</span>)<br>img = Image.open(path_img).convert(<span class="hljs-string">&#x27;RGB&#x27;</span>)  # 0~255<br><span class="hljs-attribute">img</span>=transforms.Resize((224, 224))(img)<br>img_tensor = train_transform(img)<br><br><br><br><span class="hljs-comment">## 展示单张图片</span><br><span class="hljs-comment"># 这里把转换后的 tensor 再转换为图片</span><br><span class="hljs-attribute">convert_img</span>=transform_invert(img_tensor, train_transform)<br>plt.subplot(1, 2, 1)<br>plt.imshow(img)<br>plt.subplot(1, 2, 2)<br>plt.imshow(convert_img)<br>plt.show()<br>plt.pause(0.5)<br>plt.close()<br><br><br><span class="hljs-comment">## 展示 FiveCrop 和 TenCrop 的图片</span><br><span class="hljs-comment"># ncrops, c, h, w = img_tensor.shape</span><br><span class="hljs-comment"># columns=2 # 两列</span><br><span class="hljs-comment"># rows= math.ceil(ncrops/2) # 计算多少行</span><br><span class="hljs-comment"># # 把每个 tensor ([c,h,w]) 转换为 image</span><br><span class="hljs-comment"># for i in range(ncrops):</span><br><span class="hljs-comment">#     img = transform_invert(img_tensor[i], train_transform)</span><br><span class="hljs-comment">#     plt.subplot(rows, columns, i+1)</span><br><span class="hljs-comment">#     plt.imshow(img)</span><br><span class="hljs-comment"># plt.show()</span><br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>深度学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>pytorch</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>深度学习-9-pytorch-1-基本概念</title>
    <link href="/2021/08/09/pytorchtrain/"/>
    <url>/2021/08/09/pytorchtrain/</url>
    
    <content type="html"><![CDATA[<blockquote><p>创作声明：主要内容参考于张贤同学<a href="https://zhuanlan.zhihu.com/p/265394674">https://zhuanlan.zhihu.com/p/265394674</a></p></blockquote><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>在使用pytorch进行深度学习训练时，我们需要有一个规范的步骤流程。这个流程可以让构建自己的代码和阅读别人的代码时思路清晰，心中有底。</p><h2 id="步骤思想"><a href="#步骤思想" class="headerlink" title="步骤思想"></a>步骤思想</h2><ol><li>数据处理（包括数据读取，数据清洗，进行数据划分和数据预处理，比如读取图片如何预处理及数据增强，使数据变成网络能够处理的tensor类型，输入数据）</li><li>模型构建（包括构建模型模块，组织复杂网络，初始化网络参数，定义网络层）</li><li>损失函数（包括创建损失函数，设置损失函数超参数，根据不同任务选择合适的损失函数）</li><li>优化器（包括根据梯度使用某种优化器更新参数，管理模型参数，管理多个参数组实现不同学习率，调整学习率。）</li><li>迭代训练（组织上面 4 个模块进行反复训练。还包括观察训练效果，绘制 Loss&#x2F; Accuracy 曲线，用 TensorBoard 进行可视化分析）</li></ol><h2 id="Tensor-操作"><a href="#Tensor-操作" class="headerlink" title="Tensor 操作"></a>Tensor 操作</h2><h3 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h3><p>Tensor 中文为张量。张量的意思是一个多维数组，它是标量、向量、矩阵的高维扩展。</p><p>标量可以称为 0 维张量，向量可以称为 1 维张量，矩阵可以称为 2 维张量，RGB 图像可以表示 3 维张量。你可以把张量看作多维数组。</p><p>Tensor的属性：</p><ul><li>data: 被包装的 Tensor。</li><li>grad: data 的梯度。</li><li>grad_fn: 创建 Tensor 所使用的 Function，是自动求导的关键，因为根据所记录的函数才能计算出导数。</li><li>requires_grad: 指示是否需要梯度，并不是所有的张量都需要计算梯度。</li><li>is_leaf: 指示张量是否叶子节点，叶子节点的概念在计算图中会用到，后面详细介绍。</li><li>dtype: 张量的数据类型，如 torch.FloatTensor，torch.cuda.FloatTensor。</li><li>shape: 张量的形状。如 (64, 3, 224, 224)</li><li>device: 张量所在设备 (CPU&#x2F;GPU)，GPU 是加速计算的关键。</li></ul><h3 id="创建Tensor"><a href="#创建Tensor" class="headerlink" title="创建Tensor"></a>创建Tensor</h3><h4 id="直接创建"><a href="#直接创建" class="headerlink" title="直接创建"></a>直接创建</h4><ol><li><strong>torch.tensor()</strong></li></ol><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs routeros">torch.tensor(data, <span class="hljs-attribute">dtype</span>=None, <span class="hljs-attribute">device</span>=None, <span class="hljs-attribute">requires_grad</span>=<span class="hljs-literal">False</span>, <span class="hljs-attribute">pin_memory</span>=<span class="hljs-literal">False</span>)<br></code></pre></td></tr></table></figure><ul><li>data: 数据，可以是 list，numpy</li><li>dtype: 数据类型，默认与 data 的一致</li><li>device: 所在设备，cuda&#x2F;cpu</li><li>requires_grad: 是否需要梯度</li><li>pin_memory: 是否存于锁页内存</li></ul><p>代码示例：</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs routeros">arr = np.ones((3, 3))<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;ndarray的数据类型：&quot;</span>, arr.dtype)<br><span class="hljs-comment"># 创建存放在 GPU 的数据</span><br><span class="hljs-comment"># t = torch.tensor(arr, device=&#x27;cuda&#x27;)</span><br>t= torch.tensor(arr)<br><span class="hljs-built_in">print</span>(t)<br></code></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight lua"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs lua">ndarray的数据类型： float64<br>tensor(<span class="hljs-string">[[1., 1., 1.],</span><br><span class="hljs-string">        [1., 1., 1.],</span><br><span class="hljs-string">        [1., 1., 1.]]</span>, dtype=torch.float64)<br></code></pre></td></tr></table></figure><ol start="2"><li><strong>torch.from_numpy(ndarray)</strong><br>从 numpy 创建 tensor。利用这个方法创建的 tensor 和原来的 ndarray 共享内存，当修改其中一个数据，另外一个也会被改动。</li></ol><p>代码示例：</p><figure class="highlight lua"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs lua">arr = np.array(<span class="hljs-string">[[1, 2, 3], [4, 5, 6]]</span>)<br>t = torch.from_numpy(arr)<br><br># 修改 array，tensor 也会被修改<br># <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;\n修改arr&quot;</span>)<br># arr[<span class="hljs-number">0</span>, <span class="hljs-number">0</span>] = <span class="hljs-number">0</span><br># <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;numpy array: &quot;</span>, arr)<br># <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;tensor : &quot;</span>, t)<br><br># 修改 tensor，array 也会被修改<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;\n修改tensor&quot;</span>)<br>t[<span class="hljs-number">0</span>, <span class="hljs-number">0</span>] = <span class="hljs-number">-1</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;numpy array: &quot;</span>, arr)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;tensor : &quot;</span>, t)<br></code></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight lua"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs lua">修改tensor<br>numpy array:  <span class="hljs-string">[[-1  2  3]</span><br><span class="hljs-string"> [ 4  5  6]]</span><br>tensor :  tensor(<span class="hljs-string">[[-1,  2,  3],</span><br><span class="hljs-string">        [ 4,  5,  6]]</span>, dtype=torch.int32)<br></code></pre></td></tr></table></figure><h4 id="根据数值创建-Tensor"><a href="#根据数值创建-Tensor" class="headerlink" title="根据数值创建 Tensor"></a>根据数值创建 Tensor</h4><ol><li><strong>torch.zeros()</strong></li></ol><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs routeros">torch.zeros(*size, <span class="hljs-attribute">out</span>=None, <span class="hljs-attribute">dtype</span>=None, <span class="hljs-attribute">layout</span>=torch.strided, <span class="hljs-attribute">device</span>=None, <span class="hljs-attribute">requires_grad</span>=<span class="hljs-literal">False</span>)<br></code></pre></td></tr></table></figure><p>功能：根据 size 创建全 0 张量</p><ul><li>size: 张量的形状</li><li>out: 输出的张量，如果指定了 out，那么将新建的zero张量写入到out张量中，不指定out则新建一个张量，然后无论如何，返回表示这个新的全0矩阵的张量。</li><li>layout: 内存中布局形式，有 strided，sparse_coo 等。当是稀疏矩阵时，设置为 sparse_coo 可以减少内存占用。</li><li>device: 所在设备，cuda&#x2F;cpu</li><li>requires_grad: 是否需要梯度</li></ul><p>代码示例：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python">out_t = torch.tensor([<span class="hljs-number">1</span>])<br><span class="hljs-comment"># 这里制定了 out</span><br>t = torch.zeros((<span class="hljs-number">3</span>, <span class="hljs-number">3</span>), out=out_t)<br><span class="hljs-built_in">print</span>(t, <span class="hljs-string">&#x27;\n&#x27;</span>, out_t)<br><span class="hljs-comment"># id 是取内存地址。最终 t 和 out_t 是同一个内存地址</span><br><span class="hljs-built_in">print</span>(<span class="hljs-built_in">id</span>(t), <span class="hljs-built_in">id</span>(out_t), <span class="hljs-built_in">id</span>(t) == <span class="hljs-built_in">id</span>(out_t))<br></code></pre></td></tr></table></figure><p>输出是：</p><figure class="highlight lua"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs lua">tensor(<span class="hljs-string">[[0, 0, 0],</span><br><span class="hljs-string">        [0, 0, 0],</span><br><span class="hljs-string">        [0, 0, 0]]</span>)<br> tensor(<span class="hljs-string">[[0, 0, 0],</span><br><span class="hljs-string">        [0, 0, 0],</span><br><span class="hljs-string">        [0, 0, 0]]</span>)<br><span class="hljs-number">2984903203072</span> <span class="hljs-number">2984903203072</span> True<br></code></pre></td></tr></table></figure><ol start="2"><li><strong>torch.zeros_like</strong></li></ol><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs routeros">torch.zeros_like(input, <span class="hljs-attribute">dtype</span>=None, <span class="hljs-attribute">layout</span>=None, <span class="hljs-attribute">device</span>=None, <span class="hljs-attribute">requires_grad</span>=<span class="hljs-literal">False</span>, <span class="hljs-attribute">memory_format</span>=torch.preserve_format)<br></code></pre></td></tr></table></figure><p>功能：根据 input 形状创建全 0 张量</p><ul><li>input: 创建与 input 同形状的全 0 张量</li><li>dtype: 数据类型</li><li>layout: 内存中布局形式，有 strided，sparse_coo 等。当是稀疏矩阵时，设置为 sparse_coo 可以减少内存占用。</li></ul><p>同理还有全 1 张量的创建方法：torch.ones()，torch.ones_like()。</p><ol start="3"><li><strong>torch.full()，torch.full_like()</strong></li></ol><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs routeros">torch.full(size, fill_value, <span class="hljs-attribute">out</span>=None, <span class="hljs-attribute">dtype</span>=None, <span class="hljs-attribute">layout</span>=torch.strided, <span class="hljs-attribute">device</span>=None, <span class="hljs-attribute">requires_grad</span>=<span class="hljs-literal">False</span>)<br></code></pre></td></tr></table></figure><p>功能：创建自定义数值的张量</p><ul><li>size: 张量的形状，如 (3,3)</li><li>fill_value: 张量中每一个元素的值</li></ul><p>代码示例：</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs stylus">t = torch<span class="hljs-selector-class">.full</span>((<span class="hljs-number">3</span>, <span class="hljs-number">3</span>), <span class="hljs-number">1</span>)<br><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(t)</span></span><br></code></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight lua"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs lua">tensor(<span class="hljs-string">[[1., 1., 1.],</span><br><span class="hljs-string">        [1., 1., 1.],</span><br><span class="hljs-string">        [1., 1., 1.]]</span>)<br></code></pre></td></tr></table></figure><ol start="4"><li><strong>torch.arange()</strong></li></ol><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs routeros">torch.arange(<span class="hljs-attribute">start</span>=0, end, <span class="hljs-attribute">step</span>=1, <span class="hljs-attribute">out</span>=None, <span class="hljs-attribute">dtype</span>=None, <span class="hljs-attribute">layout</span>=torch.strided, <span class="hljs-attribute">device</span>=None, <span class="hljs-attribute">requires_grad</span>=<span class="hljs-literal">False</span>)<br></code></pre></td></tr></table></figure><p>功能：创建等差的 1 维张量。注意区间为[start, end)。</p><ul><li>start: 数列起始值</li><li>end: 数列结束值，开区间，取不到结束值</li><li>step: 数列公差，默认为 1</li></ul><p>代码示例：</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs stylus">t = torch<span class="hljs-selector-class">.arange</span>(<span class="hljs-number">2</span>, <span class="hljs-number">10</span>, <span class="hljs-number">2</span>)<br><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(t)</span></span><br></code></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">tensor</span>([<span class="hljs-number">2</span>, <span class="hljs-number">4</span>, <span class="hljs-number">6</span>, <span class="hljs-number">8</span>])<br></code></pre></td></tr></table></figure><ol start="5"><li><strong>torch.linspace()</strong></li></ol><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs routeros">torch.linspace(start, end, <span class="hljs-attribute">steps</span>=100, <span class="hljs-attribute">out</span>=None, <span class="hljs-attribute">dtype</span>=None, <span class="hljs-attribute">layout</span>=torch.strided, <span class="hljs-attribute">device</span>=None, <span class="hljs-attribute">requires_grad</span>=<span class="hljs-literal">False</span>)<br></code></pre></td></tr></table></figure><p>功能：创建均分的 1 维张量。数值区间为 [start, end]</p><ul><li>start: 数列起始值</li><li>end: 数列结束值</li><li>steps: 数列长度 (元素个数)</li></ul><p>代码示例：</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-comment"># t = torch.linspace(2, 10, 5)</span><br><span class="hljs-attribute">t</span> = torch.linspace(<span class="hljs-number">2</span>, <span class="hljs-number">10</span>, <span class="hljs-number">6</span>)<br><span class="hljs-attribute">print</span>(t)<br></code></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">tensor</span>([ <span class="hljs-number">2</span>.<span class="hljs-number">0000</span>,  <span class="hljs-number">3</span>.<span class="hljs-number">6000</span>,  <span class="hljs-number">5</span>.<span class="hljs-number">2000</span>,  <span class="hljs-number">6</span>.<span class="hljs-number">8000</span>,  <span class="hljs-number">8</span>.<span class="hljs-number">4000</span>, <span class="hljs-number">10</span>.<span class="hljs-number">0000</span>])<br></code></pre></td></tr></table></figure><ol start="6"><li><strong>torch.logspace()</strong></li></ol><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs routeros">torch.logspace(start, end, <span class="hljs-attribute">steps</span>=100, <span class="hljs-attribute">base</span>=10.0, <span class="hljs-attribute">out</span>=None, <span class="hljs-attribute">dtype</span>=None, <span class="hljs-attribute">layout</span>=torch.strided, <span class="hljs-attribute">device</span>=None, <span class="hljs-attribute">requires_grad</span>=<span class="hljs-literal">False</span>)<br></code></pre></td></tr></table></figure><p>功能：创建对数均分的 1 维张量。数值区间为 [start, end]，底为 base。</p><ul><li>start: 数列起始值</li><li>end: 数列结束值</li><li>steps: 数列长度 (元素个数)</li><li>base: 对数函数的底，默认为 10</li></ul><p>代码示例：</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-comment"># t = torch.logspace(2, 10, 5)</span><br><span class="hljs-attribute">t</span> = torch.logspace(<span class="hljs-number">2</span>, <span class="hljs-number">10</span>, <span class="hljs-number">6</span>)<br><span class="hljs-attribute">print</span>(t)<br></code></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">tensor</span>([ <span class="hljs-number">2</span>.<span class="hljs-number">0000</span>,  <span class="hljs-number">3</span>.<span class="hljs-number">6000</span>,  <span class="hljs-number">5</span>.<span class="hljs-number">2000</span>,  <span class="hljs-number">6</span>.<span class="hljs-number">8000</span>,  <span class="hljs-number">8</span>.<span class="hljs-number">4000</span>, <span class="hljs-number">10</span>.<span class="hljs-number">0000</span>])<br></code></pre></td></tr></table></figure><ol start="7"><li><strong>torch.eye()</strong></li></ol><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs routeros">torch.eye(n, <span class="hljs-attribute">m</span>=None, <span class="hljs-attribute">out</span>=None, <span class="hljs-attribute">dtype</span>=None, <span class="hljs-attribute">layout</span>=torch.strided, <span class="hljs-attribute">device</span>=None, <span class="hljs-attribute">requires_grad</span>=<span class="hljs-literal">False</span>)<br></code></pre></td></tr></table></figure><p>功能：创建单位对角矩阵( 2 维张量)，默认为方阵</p><ul><li>vn: 矩阵行数。通常只设置 n，为方阵。</li><li>m: 矩阵列数</li></ul><h4 id="根据概率创建-Tensor"><a href="#根据概率创建-Tensor" class="headerlink" title="根据概率创建 Tensor"></a>根据概率创建 Tensor</h4><ol><li><strong>torch.normal()</strong></li></ol><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs routeros">torch.normal(mean, std, *, <span class="hljs-attribute">generator</span>=None, <span class="hljs-attribute">out</span>=None)<br></code></pre></td></tr></table></figure><p>功能：生成正态分布 (高斯分布)</p><ul><li>mean: 均值</li><li>std: 标准差</li></ul><p>有 4 种模式：</p><ul><li>mean 为标量，std 为标量。</li></ul><p>代码示例：</p><figure class="highlight nsis"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs nsis"><span class="hljs-comment"># mean：标量 std: 标量 这里需要设置 size </span><br>t_<span class="hljs-params">normal</span> = torch.<span class="hljs-params">normal</span>(<span class="hljs-number">0</span>., <span class="hljs-number">1</span>., size=(<span class="hljs-number">4</span>,)) <br><span class="hljs-literal">print</span>(t_<span class="hljs-params">normal</span>)<br></code></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">tensor</span>([<span class="hljs-number">0</span>.<span class="hljs-number">6614</span>, <span class="hljs-number">0</span>.<span class="hljs-number">2669</span>, <span class="hljs-number">0</span>.<span class="hljs-number">0617</span>, <span class="hljs-number">0</span>.<span class="hljs-number">6213</span>])<br></code></pre></td></tr></table></figure><ul><li>mean 为标量，std 为张量</li><li>mean 为张量，std 为标量</li></ul><p>代码示例：</p><figure class="highlight maxima"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs maxima"># <span class="hljs-built_in">mean</span>：张量 <span class="hljs-built_in">std</span>: 标量<br><span class="hljs-built_in">mean</span> = torch.arange(<span class="hljs-number">1</span>, <span class="hljs-number">5</span>, dtype=torch.<span class="hljs-built_in">float</span>)<br><span class="hljs-built_in">std</span> = <span class="hljs-number">1</span><br>t_normal = torch.normal(<span class="hljs-built_in">mean</span>, <span class="hljs-built_in">std</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;mean:&#123;&#125;\nstd:&#123;&#125;&quot;</span>.format(<span class="hljs-built_in">mean</span>, <span class="hljs-built_in">std</span>))<br><span class="hljs-built_in">print</span>(t_normal)<br></code></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">mean</span>:tensor([<span class="hljs-number">1</span>., <span class="hljs-number">2</span>., <span class="hljs-number">3</span>., <span class="hljs-number">4</span>.])<br><span class="hljs-attribute">std</span>:<span class="hljs-number">1</span><br><span class="hljs-attribute">tensor</span>([<span class="hljs-number">1</span>.<span class="hljs-number">6614</span>, <span class="hljs-number">2</span>.<span class="hljs-number">2669</span>, <span class="hljs-number">3</span>.<span class="hljs-number">0617</span>, <span class="hljs-number">4</span>.<span class="hljs-number">6213</span>])<br></code></pre></td></tr></table></figure><p>这 4 个数采样分布的均值不同，但是方差都是 1。</p><ul><li>mean 为张量，std 为张量</li></ul><p>代码示例：</p><figure class="highlight maxima"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs maxima"># <span class="hljs-built_in">mean</span>：张量 <span class="hljs-built_in">std</span>: 张量<br><span class="hljs-built_in">mean</span> = torch.arange(<span class="hljs-number">1</span>, <span class="hljs-number">5</span>, dtype=torch.<span class="hljs-built_in">float</span>)<br><span class="hljs-built_in">std</span> = torch.arange(<span class="hljs-number">1</span>, <span class="hljs-number">5</span>, dtype=torch.<span class="hljs-built_in">float</span>)<br>t_normal = torch.normal(<span class="hljs-built_in">mean</span>, <span class="hljs-built_in">std</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;mean:&#123;&#125;\nstd:&#123;&#125;&quot;</span>.format(<span class="hljs-built_in">mean</span>, <span class="hljs-built_in">std</span>))<br><span class="hljs-built_in">print</span>(t_normal)<br></code></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">mean</span>:tensor([<span class="hljs-number">1</span>., <span class="hljs-number">2</span>., <span class="hljs-number">3</span>., <span class="hljs-number">4</span>.])<br><span class="hljs-attribute">std</span>:tensor([<span class="hljs-number">1</span>., <span class="hljs-number">2</span>., <span class="hljs-number">3</span>., <span class="hljs-number">4</span>.])<br><span class="hljs-attribute">tensor</span>([<span class="hljs-number">1</span>.<span class="hljs-number">6614</span>, <span class="hljs-number">2</span>.<span class="hljs-number">5338</span>, <span class="hljs-number">3</span>.<span class="hljs-number">1850</span>, <span class="hljs-number">6</span>.<span class="hljs-number">4853</span>])<br></code></pre></td></tr></table></figure><p>其中 1.6614 是从正态分布$N(1,1)$ 中采样得到的，其他数字以此类推。</p><ol start="2"><li><strong>torch.randn() 和 torch.randn_like()</strong></li></ol><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs routeros">torch.randn(*size, <span class="hljs-attribute">out</span>=None, <span class="hljs-attribute">dtype</span>=None, <span class="hljs-attribute">layout</span>=torch.strided, <span class="hljs-attribute">device</span>=None, <span class="hljs-attribute">requires_grad</span>=<span class="hljs-literal">False</span>)<br></code></pre></td></tr></table></figure><p>功能：生成标准正态分布。</p><ul><li>size: 张量的形状</li></ul><ol start="3"><li><strong>torch.rand() 和 torch.rand_like()</strong></li></ol><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs routeros">torch.rand(*size, <span class="hljs-attribute">out</span>=None, <span class="hljs-attribute">dtype</span>=None, <span class="hljs-attribute">layout</span>=torch.strided, <span class="hljs-attribute">device</span>=None, <span class="hljs-attribute">requires_grad</span>=<span class="hljs-literal">False</span>)<br></code></pre></td></tr></table></figure><p>功能：在区间 [0, 1) 上生成均匀分布。<br>4. <strong>torch.randint() 和 torch.randint_like()</strong></p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs routeros">randint(<span class="hljs-attribute">low</span>=0, high, size, *, <span class="hljs-attribute">generator</span>=None, <span class="hljs-attribute">out</span>=None,<br><span class="hljs-attribute">dtype</span>=None, <span class="hljs-attribute">layout</span>=torch.strided, <span class="hljs-attribute">device</span>=None, <span class="hljs-attribute">requires_grad</span>=<span class="hljs-literal">False</span>)<br></code></pre></td></tr></table></figure><p>功能：在区间 [low, high) 上生成整数均匀分布。</p><ul><li>n: 张量的形状</li></ul><ol start="5"><li><strong>torch.randperm()</strong></li></ol><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs routeros">torch.randperm(n, <span class="hljs-attribute">out</span>=None, <span class="hljs-attribute">dtype</span>=torch.int64, <span class="hljs-attribute">layout</span>=torch.strided, <span class="hljs-attribute">device</span>=None, <span class="hljs-attribute">requires_grad</span>=<span class="hljs-literal">False</span>)<br></code></pre></td></tr></table></figure><p>功能：生成从 0 到 n-1 的随机排列。常用于生成索引。</p><ul><li>n: 张量的长度</li></ul><ol start="6"><li><strong>torch.bernoulli()</strong></li></ol><figure class="highlight pgsql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs pgsql">torch.bernoulli(<span class="hljs-keyword">input</span>, *, generator=<span class="hljs-keyword">None</span>, <span class="hljs-keyword">out</span>=<span class="hljs-keyword">None</span>)<br></code></pre></td></tr></table></figure><p>功能：以 input 为概率，生成伯努利分布 (0-1 分布，两点分布)</p><ul><li>input: 概率值</li></ul><h3 id="拼接切分Tensor"><a href="#拼接切分Tensor" class="headerlink" title="拼接切分Tensor"></a>拼接切分Tensor</h3><ol><li><strong>torch.cat()</strong></li></ol><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs routeros">torch.cat(tensors, <span class="hljs-attribute">dim</span>=0, <span class="hljs-attribute">out</span>=None)<br></code></pre></td></tr></table></figure><p>功能：将张量按照 dim 维度进行拼接</p><ul><li>tensors: 张量序列</li><li>dim: 要拼接的维度</li></ul><p>代码示例：</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs stylus">t = torch<span class="hljs-selector-class">.ones</span>((<span class="hljs-number">2</span>, <span class="hljs-number">3</span>))<br>t_0 = torch<span class="hljs-selector-class">.cat</span>(<span class="hljs-selector-attr">[t, t]</span>, dim=<span class="hljs-number">0</span>)<br>t_1 = torch<span class="hljs-selector-class">.cat</span>(<span class="hljs-selector-attr">[t, t]</span>, dim=<span class="hljs-number">1</span>)<br><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(<span class="hljs-string">&quot;t_0:&#123;&#125; shape:&#123;&#125;\nt_1:&#123;&#125; shape:&#123;&#125;&quot;</span>.format(t_0, t_0.shape, t_1, t_1.shape)</span></span>)<br></code></pre></td></tr></table></figure><p>输出：</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs stylus">t_0:<span class="hljs-built_in">tensor</span>(<span class="hljs-selector-attr">[[1., 1., 1.]</span>,<br>        <span class="hljs-selector-attr">[1., 1., 1.]</span>,<br>        <span class="hljs-selector-attr">[1., 1., 1.]</span>,<br>        <span class="hljs-selector-attr">[1., 1., 1.]</span>]) shape:torch<span class="hljs-selector-class">.Size</span>(<span class="hljs-selector-attr">[4, 3]</span>)<br>t_1:<span class="hljs-built_in">tensor</span>(<span class="hljs-selector-attr">[[1., 1., 1., 1., 1., 1.]</span>,<br>        <span class="hljs-selector-attr">[1., 1., 1., 1., 1., 1.]</span>]) shape:torch<span class="hljs-selector-class">.Size</span>(<span class="hljs-selector-attr">[2, 6]</span>)<br></code></pre></td></tr></table></figure><ol start="2"><li><strong>torch.stack()</strong></li></ol><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs routeros">torch.stack(tensors, <span class="hljs-attribute">dim</span>=0, <span class="hljs-attribute">out</span>=None)<br></code></pre></td></tr></table></figure><p>功能：将张量在新创建的 dim 维度上进行拼接</p><ul><li>tensors: 张量序列</li><li>dim: 要拼接的维度</li></ul><p>代码示例：</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs routeros">t = torch.ones((2, 3))<br><span class="hljs-comment"># dim =2</span><br>t_stack = torch.stack([t, t, t], <span class="hljs-attribute">dim</span>=2)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;\nt_stack.shape:&#123;&#125;&quot;</span>.format(t_stack.shape))<br><span class="hljs-comment"># dim =0</span><br>t_stack = torch.stack([t, t, t], <span class="hljs-attribute">dim</span>=0)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;\nt_stack.shape:&#123;&#125;&quot;</span>.format(t_stack.shape))<br></code></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">t_stack</span>.shape:torch.Size([<span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">3</span>])<br><span class="hljs-attribute">t_stack</span>.shape:torch.Size([<span class="hljs-number">3</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>])<br></code></pre></td></tr></table></figure><p>第一次指定拼接的维度 dim &#x3D;2，结果的维度是 [2, 3, 3]。后面指定拼接的维度 dim &#x3D;0，由于原来的 tensor 已经有了维度 0，因此会把 tensor 往后移动一个维度变为 [1,2,3]，再拼接变为 [3,2,3]。<br>3. <strong>torch.chunk()</strong></p><figure class="highlight hsp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs hsp">torch.chunk(<span class="hljs-keyword">input</span>, chunks, <span class="hljs-keyword">dim</span>=<span class="hljs-number">0</span>)<br></code></pre></td></tr></table></figure><p>功能：将张量按照维度 dim 进行平均切分。若不能整除，则最后一份张量小于其他张量。</p><ul><li>input: 要切分的张量</li><li>chunks: 要切分的份数</li><li>dim: 要切分的维度</li></ul><p>代码示例：</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">a</span> = torch.ones((<span class="hljs-number">2</span>, <span class="hljs-number">7</span>))  # <span class="hljs-number">7</span><br><span class="hljs-attribute">list_of_tensors</span> = torch.chunk(a, dim=<span class="hljs-number">1</span>, chunks=<span class="hljs-number">3</span>)   # <span class="hljs-number">3</span><br><span class="hljs-attribute">for</span> idx, t in enumerate(list_of_tensors):<br><span class="hljs-attribute">print</span>(<span class="hljs-string">&quot;第&#123;&#125;个张量：&#123;&#125;, shape is &#123;&#125;&quot;</span>.format(idx+<span class="hljs-number">1</span>, t, t.shape))<br></code></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight inform7"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs inform7">第1个张量：tensor(<span class="hljs-comment">[<span class="hljs-comment">[1., 1., 1.]</span>,</span><br><span class="hljs-comment">        <span class="hljs-comment">[1., 1., 1.]</span>]</span>), shape <span class="hljs-keyword">is</span> torch.Size(<span class="hljs-comment">[2, 3]</span>)<br>第2个张量：tensor(<span class="hljs-comment">[<span class="hljs-comment">[1., 1., 1.]</span>,</span><br><span class="hljs-comment">        <span class="hljs-comment">[1., 1., 1.]</span>]</span>), shape <span class="hljs-keyword">is</span> torch.Size(<span class="hljs-comment">[2, 3]</span>)<br>第3个张量：tensor(<span class="hljs-comment">[<span class="hljs-comment">[1.]</span>,</span><br><span class="hljs-comment">        <span class="hljs-comment">[1.]</span>]</span>), shape <span class="hljs-keyword">is</span> torch.Size(<span class="hljs-comment">[2, 1]</span>)<br></code></pre></td></tr></table></figure><p>由于 7 不能整除 3，7&#x2F;3 再向上取整是 3，因此前两个维度是 [2, 3]，所以最后一个切分的张量维度是 [2,1]。<br>4. torch.split()</p><figure class="highlight maxima"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs maxima">torch.<span class="hljs-built_in">split</span>(tensor, split_size_or_sections, <span class="hljs-built_in">dim</span>=<span class="hljs-number">0</span>)<br></code></pre></td></tr></table></figure><p>功能：将张量按照维度 dim 进行平均切分。可以指定每一个分量的切分长度。</p><ul><li>tensor: 要切分的张量</li><li>split_size_or_sections: 为 int 时，表示每一份的长度，如果不能被整除，则最后一份张量小于其他张量；为 list 时，按照 list 元素作为每一个分量的长度切分。如果 list 元素之和不等于切分维度 (dim) 的值，就会报错。</li><li>dim: 要切分的维度</li></ul><p>代码示例：</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs stylus">t = torch<span class="hljs-selector-class">.ones</span>((<span class="hljs-number">2</span>, <span class="hljs-number">5</span>))<br>list_of_tensors = torch<span class="hljs-selector-class">.split</span>(t, <span class="hljs-selector-attr">[2, 1, 2]</span>, dim=<span class="hljs-number">1</span>)<br><span class="hljs-keyword">for</span> idx, t <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(list_of_tensors):<br><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(<span class="hljs-string">&quot;第&#123;&#125;个张量：&#123;&#125;, shape is &#123;&#125;&quot;</span>.format(idx+<span class="hljs-number">1</span>, t, t.shape)</span></span>)<br></code></pre></td></tr></table></figure><p>结果为：</p><figure class="highlight inform7"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs inform7">第1个张量：tensor(<span class="hljs-comment">[<span class="hljs-comment">[1., 1.]</span>,</span><br><span class="hljs-comment">        <span class="hljs-comment">[1., 1.]</span>]</span>), shape <span class="hljs-keyword">is</span> torch.Size(<span class="hljs-comment">[2, 2]</span>)<br>第2个张量：tensor(<span class="hljs-comment">[<span class="hljs-comment">[1.]</span>,</span><br><span class="hljs-comment">        <span class="hljs-comment">[1.]</span>]</span>), shape <span class="hljs-keyword">is</span> torch.Size(<span class="hljs-comment">[2, 1]</span>)<br>第3个张量：tensor(<span class="hljs-comment">[<span class="hljs-comment">[1., 1.]</span>,</span><br><span class="hljs-comment">        <span class="hljs-comment">[1., 1.]</span>]</span>), shape <span class="hljs-keyword">is</span> torch.Size(<span class="hljs-comment">[2, 2]</span>)<br></code></pre></td></tr></table></figure><h3 id="Tensor-索引"><a href="#Tensor-索引" class="headerlink" title="Tensor 索引"></a>Tensor 索引</h3><ol><li><strong>torch.index_select()</strong></li></ol><figure class="highlight fortran"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs fortran">torch.index_select(input, <span class="hljs-built_in">dim</span>, <span class="hljs-built_in">index</span>, <span class="hljs-keyword">out</span>=<span class="hljs-keyword">None</span>)<br></code></pre></td></tr></table></figure><p>功能：在维度 dim 上，按照 index 索引取出数据拼接为张量返回。</p><ul><li>input: 要索引的张量</li><li>dim: 要索引的维度</li><li>index: 要索引数据的序号</li></ul><p>代码示例：</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs routeros"><span class="hljs-comment"># 创建均匀分布</span><br>t = torch.randint(0, 9, size=(3, 3))<br><span class="hljs-comment"># 注意 idx 的 dtype 不能指定为 torch.float</span><br>idx = torch.tensor([0, 2], <span class="hljs-attribute">dtype</span>=torch.long)<br><span class="hljs-comment"># 取出第 0 行和第 2 行</span><br>t_select = torch.index_select(t, <span class="hljs-attribute">dim</span>=0, <span class="hljs-attribute">index</span>=idx)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;t:\n&#123;&#125;\nt_select:\n&#123;&#125;&quot;</span>.format(t, t_select))<br></code></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight lua"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs lua">t:<br>tensor(<span class="hljs-string">[[4, 5, 0],</span><br><span class="hljs-string">        [5, 7, 1],</span><br><span class="hljs-string">        [2, 5, 8]]</span>)<br>t_select:<br>tensor(<span class="hljs-string">[[4, 5, 0],</span><br><span class="hljs-string">        [2, 5, 8]]</span>)<br></code></pre></td></tr></table></figure><ol start="2"><li><strong>torch.mask_select()</strong></li></ol><figure class="highlight pgsql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs pgsql">torch.masked_select(<span class="hljs-keyword">input</span>, mask, <span class="hljs-keyword">out</span>=<span class="hljs-keyword">None</span>)<br></code></pre></td></tr></table></figure><p>功能：按照 mask 中的 True 进行索引拼接得到一维张量返回。</p><ul><li>input:要索引的张量</li><li>mask: 与 input 同形状的布尔类型张量</li></ul><p>代码示例：</p><figure class="highlight nix"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs nix">t <span class="hljs-operator">=</span> torch.randint(<span class="hljs-number">0</span>, <span class="hljs-number">9</span>, size<span class="hljs-operator">=</span>(<span class="hljs-number">3</span>, <span class="hljs-number">3</span>))<br>m<span class="hljs-attr">ask</span> <span class="hljs-operator">=</span> t.le(<span class="hljs-number">5</span>)  <span class="hljs-comment"># ge is mean greater than or equal/   gt: greater than  le  lt</span><br><span class="hljs-comment"># 取出大于 5 的数</span><br>t<span class="hljs-attr">_select</span> <span class="hljs-operator">=</span> torch.masked_select(t, mask)<br>print(<span class="hljs-string">&quot;t:<span class="hljs-char escape_">\n</span>&#123;&#125;<span class="hljs-char escape_">\n</span>mask:<span class="hljs-char escape_">\n</span>&#123;&#125;<span class="hljs-char escape_">\n</span>t_select:<span class="hljs-char escape_">\n</span>&#123;&#125; &quot;</span>.format(t, mask, t_select))<br></code></pre></td></tr></table></figure><p>结果为：</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs stylus">t:<br><span class="hljs-built_in">tensor</span>(<span class="hljs-selector-attr">[[4, 5, 0]</span>,<br>        <span class="hljs-selector-attr">[5, 7, 1]</span>,<br>        <span class="hljs-selector-attr">[2, 5, 8]</span>])<br><span class="hljs-selector-tag">mask</span>:<br><span class="hljs-built_in">tensor</span>(<span class="hljs-selector-attr">[[ True,  True,  True]</span>,<br>        <span class="hljs-selector-attr">[ True, False,  True]</span>,<br>        <span class="hljs-selector-attr">[ True,  True, False]</span>])<br>t_select:<br><span class="hljs-function"><span class="hljs-title">tensor</span><span class="hljs-params">([<span class="hljs-number">4</span>, <span class="hljs-number">5</span>, <span class="hljs-number">0</span>, <span class="hljs-number">5</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">5</span>])</span></span><br></code></pre></td></tr></table></figure><p>最后返回的是一维张量。</p><h3 id="Tensor-变换"><a href="#Tensor-变换" class="headerlink" title="Tensor 变换"></a>Tensor 变换</h3><ol><li><strong>torch.reshape()</strong></li></ol><figure class="highlight fortran"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs fortran">torch.<span class="hljs-built_in">reshape</span>(input, <span class="hljs-built_in">shape</span>)<br></code></pre></td></tr></table></figure><p>功能：变换张量的形状。当张量在内存中是连续时，返回的张量和原来的张量共享数据内存，改变一个变量时，另一个变量也会被改变。</p><ul><li>input: 要变换的张量</li><li>shape: 新张量的形状</li></ul><p>代码示例：</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-comment"># 生成 0 到 8 的随机排列</span><br><span class="hljs-attribute">t</span> = torch.randperm(<span class="hljs-number">8</span>)<br><span class="hljs-comment"># -1 表示这个维度是根据其他维度计算得出的</span><br><span class="hljs-attribute">t_reshape</span> = torch.reshape(t, (-<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>))<br><span class="hljs-attribute">print</span>(<span class="hljs-string">&quot;t:&#123;&#125;\nt_reshape:\n&#123;&#125;&quot;</span>.format(t, t_reshape))<br></code></pre></td></tr></table></figure><p>结果为：</p><figure class="highlight lua"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs lua">t:tensor([<span class="hljs-number">5</span>, <span class="hljs-number">4</span>, <span class="hljs-number">2</span>, <span class="hljs-number">6</span>, <span class="hljs-number">7</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>])<br>t_reshape:<br>tensor(<span class="hljs-string">[[[5, 4],</span><br><span class="hljs-string">         [2, 6]]</span>,<br><br>        <span class="hljs-string">[[7, 3],</span><br><span class="hljs-string">         [1, 0]]</span>])<br></code></pre></td></tr></table></figure><p>在上面代码的基础上，修改原来的张量的一个元素，新张量也会被改变。</p><p>代码示例：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 修改张量 t 的第 0 个元素，张量 t_reshape 也会被改变</span><br>t[<span class="hljs-number">0</span>] = <span class="hljs-number">1024</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;t:&#123;&#125;\nt_reshape:\n&#123;&#125;&quot;</span>.<span class="hljs-built_in">format</span>(t, t_reshape))<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;t.data 内存地址:&#123;&#125;&quot;</span>.<span class="hljs-built_in">format</span>(<span class="hljs-built_in">id</span>(t.data)))<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;t_reshape.data 内存地址:&#123;&#125;&quot;</span>.<span class="hljs-built_in">format</span>(<span class="hljs-built_in">id</span>(t_reshape.data)))<br></code></pre></td></tr></table></figure><p>结果为：</p><figure class="highlight lua"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs lua">t:tensor([<span class="hljs-number">1024</span>,    <span class="hljs-number">4</span>,    <span class="hljs-number">2</span>,    <span class="hljs-number">6</span>,    <span class="hljs-number">7</span>,    <span class="hljs-number">3</span>,    <span class="hljs-number">1</span>,    <span class="hljs-number">0</span>])<br>t_reshape:<br>tensor(<span class="hljs-string">[[[1024,    4],</span><br><span class="hljs-string">         [   2,    6]]</span>,<br><br>        <span class="hljs-string">[[   7,    3],</span><br><span class="hljs-string">         [   1,    0]]</span>])<br>t.data 内存地址:<span class="hljs-number">2636803119936</span><br>t_reshape.data 内存地址:<span class="hljs-number">2636803119792</span><br></code></pre></td></tr></table></figure><ol start="2"><li><strong>torch.transpose()</strong></li></ol><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs stylus">torch<span class="hljs-selector-class">.transpose</span>(<span class="hljs-selector-tag">input</span>, dim0, dim1)<br></code></pre></td></tr></table></figure><p>功能：交换张量的两个维度。常用于图像的变换，比如把c<em>h</em>w变换为h<em>w</em>c。</p><ul><li>input: 要交换的变量</li><li>dim0: 要交换的第一个维度</li><li>dim1: 要交换的第二个维度<br>代码示例：</li></ul><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs routeros"><span class="hljs-comment">#把 c * h * w 变换为 h * w * c</span><br>t = torch.rand((2, 3, 4))<br>t_transpose = torch.transpose(t, <span class="hljs-attribute">dim0</span>=1, <span class="hljs-attribute">dim1</span>=2)    # c*h*w     h*w<span class="hljs-number">*c</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;t shape:&#123;&#125;\nt_transpose shape: &#123;&#125;&quot;</span>.format(t.shape, t_transpose.shape))<br></code></pre></td></tr></table></figure><p>结果为：</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">t</span> shape:torch.Size([<span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>])<br><span class="hljs-attribute">t_transpose</span> shape: torch.Size([<span class="hljs-number">2</span>, <span class="hljs-number">4</span>, <span class="hljs-number">3</span>])<br><span class="hljs-attribute">torch</span>.t()<br></code></pre></td></tr></table></figure><p>功能：2 维张量转置，对于 2 维矩阵而言，等价于torch.transpose(input, 0, 1)。</p><ol start="3"><li><strong>torch.squeeze()</strong></li></ol><figure class="highlight fortran"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs fortran">torch.squeeze(input, <span class="hljs-built_in">dim</span>=<span class="hljs-keyword">None</span>, <span class="hljs-keyword">out</span>=<span class="hljs-keyword">None</span>)<br></code></pre></td></tr></table></figure><p>功能：压缩长度为 1 的维度。</p><ul><li>dim: 若为 None，则移除所有长度为 1 的维度；若指定维度，则当且仅当该维度长度为 1 时可以移除。</li></ul><p>代码示例：</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs routeros"><span class="hljs-comment"># 维度 0 和 3 的长度是 1</span><br>    t = torch.rand((1, 2, 3, 1))<br>    # 可以移除维度 0 和 3<br>    t_sq = torch.squeeze(t)<br>    # 可以移除维度 0<br>    t_0 = torch.squeeze(t, <span class="hljs-attribute">dim</span>=0)<br>    # 不能移除 1<br>    t_1 = torch.squeeze(t, <span class="hljs-attribute">dim</span>=1)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;t.shape: &#123;&#125;&quot;</span>.format(t.shape))<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;t_sq.shape: &#123;&#125;&quot;</span>.format(t_sq.shape))<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;t_0.shape: &#123;&#125;&quot;</span>.format(t_0.shape))<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;t_1.shape: &#123;&#125;&quot;</span>.format(t_1.shape))<br></code></pre></td></tr></table></figure><p>结果为：</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">t</span>.shape: torch.Size([<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>])<br><span class="hljs-attribute">t_sq</span>.shape: torch.Size([<span class="hljs-number">2</span>, <span class="hljs-number">3</span>])<br><span class="hljs-attribute">t_0</span>.shape: torch.Size([<span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>])<br><span class="hljs-attribute">t_1</span>.shape: torch.Size([<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>])<br></code></pre></td></tr></table></figure><ol start="4"><li><strong>torch.unsqueeze()</strong></li></ol><figure class="highlight hsp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs hsp">torch.unsqueeze(<span class="hljs-keyword">input</span>, <span class="hljs-keyword">dim</span>)<br></code></pre></td></tr></table></figure><p>功能：根据 dim 扩展维度，长度为 1。</p><h3 id="Tensor-数学运算"><a href="#Tensor-数学运算" class="headerlink" title="Tensor 数学运算"></a>Tensor 数学运算</h3><p>主要分为 3 类：加减乘除，对数，指数，幂函数 和三角函数。</p><p>这里介绍一下常用的几种方法。</p><ol><li><strong>torch.add()</strong></li></ol><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs routeros">torch.<span class="hljs-built_in">add</span>(input, other, <span class="hljs-attribute">out</span>=None)<br>torch.<span class="hljs-built_in">add</span>(input, other, *, <span class="hljs-attribute">alpha</span>=1, <span class="hljs-attribute">out</span>=None)<br></code></pre></td></tr></table></figure><p>功能：逐元素计算 input + alpha * other。因为在深度学习中经常用到先乘后加的操作。</p><ul><li>input: 第一个张量</li><li>alpha: 乘项因子</li><li>other: 第二个张量</li></ul><ol start="2"><li><strong>torch.addcdiv()</strong></li></ol><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs routeros">torch.addcdiv(input, tensor1, tensor2, *, <span class="hljs-attribute">value</span>=1, <span class="hljs-attribute">out</span>=None)<br></code></pre></td></tr></table></figure><p>计算公式为：$out_i &#x3D; input_i+value\times {tensor1_i\over tensor2_i}$<br>3. <strong>torch.addcmul()</strong></p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs routeros">torch.addcmul(input, tensor1, tensor2, *, <span class="hljs-attribute">value</span>=1, <span class="hljs-attribute">out</span>=None)<br></code></pre></td></tr></table></figure><p>计算公式为：$out_i &#x3D; input_i+value\times tensor1_i\times tensor2_i$</p><h2 id="线性回归"><a href="#线性回归" class="headerlink" title="线性回归"></a>线性回归</h2><p>线性回归是分析一个变量 ($y$) 与另外一 (多) 个变量 ($x$) 之间的关系的方法。一般可以写成 $y&#x3D;wx+b$。线性回归的目的就是求解参数$w,b$。</p><p>线性回归的求解可以分为 3 步：</p><ol><li>确定模型：$y&#x3D;wx+b$</li><li>选择损失函数，一般使用均方误差 MSE：${1\over m} \sum_{i&#x3D;1}^m(y_i-\hat{y_i})^2$。其中$\hat{y_i}$是预测值，$y$是真实值。</li><li>使用梯度下降法求解梯度 (其中$lr$是学习率)，并更新参数：</li></ol><ul><li>$w&#x3D;w-lr*w.grad$</li><li>$b&#x3D;b-lr*b.grad$</li></ul><p>代码如下：</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">import</span> torch<br><span class="hljs-attribute">import</span> matplotlib.pyplot as plt<br><span class="hljs-attribute">torch</span>.manual_seed(<span class="hljs-number">10</span>)<br><br><span class="hljs-attribute">lr</span> = <span class="hljs-number">0</span>.<span class="hljs-number">05</span>  # 学习率<br><br><span class="hljs-comment"># 创建训练数据</span><br><span class="hljs-attribute">x</span> = torch.rand(<span class="hljs-number">20</span>, <span class="hljs-number">1</span>) * <span class="hljs-number">10</span>  # x data (tensor), shape=(<span class="hljs-number">20</span>, <span class="hljs-number">1</span>)<br><span class="hljs-comment"># torch.randn(20, 1) 用于添加噪声</span><br><span class="hljs-attribute">y</span> = <span class="hljs-number">2</span>*x + (<span class="hljs-number">5</span> + torch.randn(<span class="hljs-number">20</span>, <span class="hljs-number">1</span>))  # y data (tensor), shape=(<span class="hljs-number">20</span>, <span class="hljs-number">1</span>)<br><br><span class="hljs-comment"># 构建线性回归参数</span><br><span class="hljs-attribute">w</span> = torch.randn((<span class="hljs-number">1</span>), requires_grad=True) # 设置梯度求解为 true<br><span class="hljs-attribute">b</span> = torch.zeros((<span class="hljs-number">1</span>), requires_grad=True) # 设置梯度求解为 true<br><br><span class="hljs-comment"># 迭代训练 1000 次</span><br><span class="hljs-attribute">for</span> iteration in range(<span class="hljs-number">1000</span>):<br><br>    <span class="hljs-comment"># 前向传播，计算预测值</span><br>    <span class="hljs-attribute">wx</span> = torch.mul(w, x)<br>    <span class="hljs-attribute">y_pred</span> = torch.add(wx, b)<br><br>    <span class="hljs-comment"># 计算 MSE loss</span><br>    <span class="hljs-attribute">loss</span> = (<span class="hljs-number">0</span>.<span class="hljs-number">5</span> * (y - y_pred) ** <span class="hljs-number">2</span>).mean()<br><br>    <span class="hljs-comment"># 反向传播</span><br>    <span class="hljs-attribute">loss</span>.backward()<br><br>    <span class="hljs-comment"># 更新参数</span><br>    <span class="hljs-attribute">b</span>.data.sub_(lr * b.grad)<br>    <span class="hljs-attribute">w</span>.data.sub_(lr * w.grad)<br><br>    <span class="hljs-comment"># 每次更新参数之后，都要清零张量的梯度</span><br>    <span class="hljs-attribute">w</span>.grad.zero_()<br>    <span class="hljs-attribute">b</span>.grad.zero_()<br><br>    <span class="hljs-comment"># 绘图，每隔 20 次重新绘制直线</span><br>    <span class="hljs-attribute">if</span> iteration % <span class="hljs-number">20</span> == <span class="hljs-number">0</span>:<br><br>        <span class="hljs-attribute">plt</span>.scatter(x.data.numpy(), y.data.numpy())<br>        <span class="hljs-attribute">plt</span>.plot(x.data.numpy(), y_pred.data.numpy(), &#x27;r-&#x27;, lw=<span class="hljs-number">5</span>)<br>        <span class="hljs-attribute">plt</span>.text(<span class="hljs-number">2</span>, <span class="hljs-number">20</span>, &#x27;Loss=%.<span class="hljs-number">4</span>f&#x27; % loss.data.numpy(), fontdict=&#123;&#x27;size&#x27;: <span class="hljs-number">20</span>, &#x27;color&#x27;:  &#x27;red&#x27;&#125;)<br>        <span class="hljs-attribute">plt</span>.xlim(<span class="hljs-number">1</span>.<span class="hljs-number">5</span>, <span class="hljs-number">10</span>)<br>        <span class="hljs-attribute">plt</span>.ylim(<span class="hljs-number">8</span>, <span class="hljs-number">28</span>)<br>        <span class="hljs-attribute">plt</span>.title(<span class="hljs-string">&quot;Iteration: &#123;&#125;\nw: &#123;&#125; b: &#123;&#125;&quot;</span>.format(iteration, w.data.numpy(), b.data.numpy()))<br>        <span class="hljs-attribute">plt</span>.pause(<span class="hljs-number">0</span>.<span class="hljs-number">5</span>)<br><br>        <span class="hljs-comment"># 如果 MSE 小于 1，则停止训练</span><br>        <span class="hljs-attribute">if</span> loss.data.numpy() &lt; <span class="hljs-number">1</span>:<br>            <span class="hljs-attribute">break</span><br></code></pre></td></tr></table></figure><p>训练的直线的可视化如下：<br><img src="/img/pytorchtrain/liner1.gif"><br>在 80 次的时候，Loss 已经小于 1 了，因此停止了训练。</p><h2 id="静态图与动态图机制"><a href="#静态图与动态图机制" class="headerlink" title="静态图与动态图机制"></a>静态图与动态图机制</h2><h3 id="计算图"><a href="#计算图" class="headerlink" title="计算图"></a>计算图</h3><p>深度学习就是对张量进行一系列的操作，随着操作种类和数量的增多，会出现各种值得思考的问题。比如多个操作之间是否可以并行，如何协同底层的不同设备，如何避免冗余的操作，以实现最高效的计算效率，同时避免一些 bug。因此产生了计算图 (Computational Graph)。</p><p>计算图是用来描述运算的有向无环图，有两个主要元素：节点 (Node) 和边 (Edge)。节点表示数据，如向量、矩阵、张量。边表示运算，如加减乘除卷积等。</p><p>用计算图表示：$y&#x3D;(x+w)*(w+1)$，如下所示：<br><img src="/img/pytorchtrain/1.png"><br>可以看作， $y&#x3D;a \times b$ ，其中 $a&#x3D;x+w$，$b&#x3D;w+1$。</p><h3 id="计算图与梯度求导"><a href="#计算图与梯度求导" class="headerlink" title="计算图与梯度求导"></a>计算图与梯度求导</h3><p>这里求 $y$ 对 $w$ 的导数。根复合函数的求导法则，可以得到如下过程。<br>$\begin{aligned} \frac{\partial y}{\partial w} &amp;&#x3D;\frac{\partial y}{\partial a} \frac{\partial a}{\partial w}+\frac{\partial y}{\partial b} \frac{\partial b}{\partial w} \ &amp;&#x3D;b  1+a  1 \ &amp;&#x3D;b+a \ &amp;&#x3D;(w+1)+(x+w) \ &amp;&#x3D;2  w+x+1 \ &amp;&#x3D;2  1+2+1&#x3D;5\end{aligned}$<br>体现到计算图中，就是根节点 $y$ 到叶子节点 $w$ 有两条路径 y -&gt; a -&gt; w和y -&gt;b -&gt; w。根节点依次对每条路径的孩子节点求导，一直到叶子节点w，最后把每条路径的导数相加即可。</p><p>代码如下：</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs routeros">import torch<br>w = torch.tensor([1.], <span class="hljs-attribute">requires_grad</span>=<span class="hljs-literal">True</span>)<br>x = torch.tensor([2.], <span class="hljs-attribute">requires_grad</span>=<span class="hljs-literal">True</span>)<br><span class="hljs-comment"># y=(x+w)*(w+1)</span><br>a = torch.<span class="hljs-built_in">add</span>(w, x)     # retain_grad()<br>b = torch.<span class="hljs-built_in">add</span>(w, 1)<br>y = torch.mul(a, b)<br><span class="hljs-comment"># y 求导</span><br>y.backward()<br><span class="hljs-comment"># 打印 w 的梯度，就是 y 对 w 的导数</span><br><span class="hljs-built_in">print</span>(w.grad)<br></code></pre></td></tr></table></figure><p>结果为tensor([5.])。</p><p>我们回顾前面说过的 Tensor 中有一个属性is_leaf标记是否为叶子节点。<br>在上面的例子中，$x$ 和 $w$ 是叶子节点，其他所有节点都依赖于叶子节点。叶子节点的概念主要是为了节省内存，在计算图中的一轮反向传播结束之后，非叶子节点的梯度是会被释放的。</p><p>代码示例（接上文）：</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs routeros"><span class="hljs-comment"># 查看叶子结点</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;is_leaf:\n&quot;</span>, w.is_leaf, x.is_leaf, a.is_leaf, b.is_leaf, y.is_leaf)<br><br><span class="hljs-comment"># 查看梯度</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;gradient:\n&quot;</span>, w.grad, x.grad, a.grad, b.grad, y.grad)<br></code></pre></td></tr></table></figure><p>结果为：</p><figure class="highlight arcade"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs arcade">is_leaf:<br> <span class="hljs-literal">True</span> <span class="hljs-literal">True</span> <span class="hljs-literal">False</span> <span class="hljs-literal">False</span> <span class="hljs-literal">False</span><br>gradient:<br> tensor([<span class="hljs-number">5.</span>]) tensor([<span class="hljs-number">2.</span>]) <span class="hljs-built_in">None</span> <span class="hljs-built_in">None</span> <span class="hljs-built_in">None</span><br></code></pre></td></tr></table></figure><p>非叶子节点的梯度为空，如果在反向传播结束之后仍然需要保留非叶子节点的梯度，可以对节点使用retain_grad()方法。</p><p>而 Tensor 中的 grad_fn 属性记录的是创建该张量时所用的方法 (函数)。而在反向传播求导梯度时需要用到该属性。</p><p>示例代码：</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs routeros"><span class="hljs-comment"># 查看梯度</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;w.grad_fn = &quot;</span>, w.grad_fn)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;x.grad_fn = &quot;</span>, x.grad_fn)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;a.grad_fn = &quot;</span>, a.grad_fn)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;b.grad_fn = &quot;</span>, b.grad_fn)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;y.grad_fn = &quot;</span>, y.grad_fn)<br></code></pre></td></tr></table></figure><p>结果为</p><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs ini"><span class="hljs-attr">w.grad_fn</span> =  None<br><span class="hljs-attr">x.grad_fn</span> =  None<br><span class="hljs-attr">a.grad_fn</span> =  &lt;AddBackward0 object at <span class="hljs-number">0</span>x000001D8DDD20588&gt;<br><span class="hljs-attr">b.grad_fn</span> =  &lt;AddBackward0 object at <span class="hljs-number">0</span>x000001D8DDD20588&gt;<br><span class="hljs-attr">y.grad_fn</span> =  &lt;MulBackward0 object at <span class="hljs-number">0</span>x000001D8DDD20588&gt;<br></code></pre></td></tr></table></figure><h3 id="PyTorch-的动态图机制"><a href="#PyTorch-的动态图机制" class="headerlink" title="PyTorch 的动态图机制"></a>PyTorch 的动态图机制</h3><p>PyTorch 采用的是动态图机制 (Dynamic Computational Graph)，而 Tensorflow 采用的是静态图机制 (Static Computational Graph)。</p><p>动态图是运算和搭建同时进行，也就是可以先计算前面的节点的值，再根据这些值搭建后面的计算图。优点是灵活，易调节，易调试。PyTorch 里的很多写法跟其他 Python 库的代码的使用方法是完全一致的，没有任何额外的学习成本。</p><p>静态图是先搭建图，然后再输入数据进行运算。优点是高效，因为静态计算是通过先定义后运行的方式，之后再次运行的时候就不再需要重新构建计算图，所以速度会比动态图更快。但是不灵活。TensorFlow 每次运行的时候图都是一样的，是不能够改变的，所以不能直接使用 Python 的 while 循环语句，需要使用辅助函数 tf.while_loop 写成 TensorFlow 内部的形式。</p><h2 id="autograd"><a href="#autograd" class="headerlink" title="autograd"></a>autograd</h2><h3 id="自动求导-autograd"><a href="#自动求导-autograd" class="headerlink" title="自动求导 (autograd)"></a>自动求导 (autograd)</h3><p>在深度学习中，权值的更新是依赖于梯度的计算，因此梯度的计算是至关重要的。在 PyTorch 中，只需要搭建好前向计算图，然后利用<code>torch.autograd</code>自动求导得到所有张量的梯度。</p><p><strong>torch.autograd.backward()（一次求出所有参数的导数）</strong></p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs routeros">torch.autograd.backward(tensors, <span class="hljs-attribute">grad_tensors</span>=None, <span class="hljs-attribute">retain_graph</span>=None, <span class="hljs-attribute">create_graph</span>=<span class="hljs-literal">False</span>, <span class="hljs-attribute">grad_variables</span>=None)<br></code></pre></td></tr></table></figure><p>功能：自动求取梯度</p><ul><li>tensors: 用于求导的张量，如 loss</li><li>retain_graph: 保存计算图。PyTorch 采用动态图机制，默认每次反向传播之后都会释放计算图。这里设置为 True 可以不释放计算图。</li><li>create_graph: 创建导数计算图，用于高阶求导</li><li>grad_tensors: 多梯度权重。当有多个 loss 混合需要计算梯度时，设置每个 loss 的权重。</li></ul><p>代码示例:</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs routeros">w = torch.tensor([1.], <span class="hljs-attribute">requires_grad</span>=<span class="hljs-literal">True</span>)<br>x = torch.tensor([2.], <span class="hljs-attribute">requires_grad</span>=<span class="hljs-literal">True</span>)<br><span class="hljs-comment"># y=(x+w)*(w+1)</span><br>a = torch.<span class="hljs-built_in">add</span>(w, x)<br>b = torch.<span class="hljs-built_in">add</span>(w, 1)<br>y = torch.mul(a, b)<br><br><span class="hljs-comment"># 第一次执行梯度求导</span><br>y.backward()<br><span class="hljs-built_in">print</span>(w.grad)<br><span class="hljs-comment"># 第二次执行梯度求导，出错</span><br>y.backward()<br></code></pre></td></tr></table></figure><p>其中y.backward()方法调用的是torch.autograd.backward(self, gradient, retain_graph, create_graph)。但是在第二次执行y.backward()时会出错。因为 PyTorch 默认是每次求取梯度之后不保存计算图的，因此第二次求导梯度时，计算图已经不存在了。在第一次求梯度时使用y.backward(retain_graph&#x3D;True)即可。如下代码所示：</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs routeros">w = torch.tensor([1.], <span class="hljs-attribute">requires_grad</span>=<span class="hljs-literal">True</span>)<br>x = torch.tensor([2.], <span class="hljs-attribute">requires_grad</span>=<span class="hljs-literal">True</span>)<br><span class="hljs-comment"># y=(x+w)*(w+1)</span><br>a = torch.<span class="hljs-built_in">add</span>(w, x)<br>b = torch.<span class="hljs-built_in">add</span>(w, 1)<br>y = torch.mul(a, b)<br><br><span class="hljs-comment"># 第一次求导，设置 retain_graph=True，保留计算图</span><br>y.backward(<span class="hljs-attribute">retain_graph</span>=<span class="hljs-literal">True</span>)<br><span class="hljs-built_in">print</span>(w.grad)<br><span class="hljs-comment"># 第二次求导成功</span><br>y.backward()<br></code></pre></td></tr></table></figure><p>代码示例：</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs routeros">w = torch.tensor([1.], <span class="hljs-attribute">requires_grad</span>=<span class="hljs-literal">True</span>)<br>x = torch.tensor([2.], <span class="hljs-attribute">requires_grad</span>=<span class="hljs-literal">True</span>)<br><br>a = torch.<span class="hljs-built_in">add</span>(w, x)<br>b = torch.<span class="hljs-built_in">add</span>(w, 1)<br><br>y0 = torch.mul(a, b)    # y0 = (x+w) * (w+1)<br>y1 = torch.<span class="hljs-built_in">add</span>(a, b)    # y1 = (x+w) + (w+1)    dy1/dw = 2<br><br><span class="hljs-comment"># 把两个 loss 拼接都到一起</span><br>loss = torch.cat([y0, y1], <span class="hljs-attribute">dim</span>=0)       # [y0, y1]<br><span class="hljs-comment"># 设置两个 loss 的权重: y0 的权重是 1，y1 的权重是 2</span><br>grad_tensors = torch.tensor([1., 2.])<br><br>loss.backward(<span class="hljs-attribute">gradient</span>=grad_tensors)    # gradient 传入 torch.autograd.backward()中的grad_tensors<br><span class="hljs-comment"># 最终的 w 的导数由两部分组成。∂y0/∂w * 1 + ∂y1/∂w * 2</span><br><span class="hljs-built_in">print</span>(w.grad)<br></code></pre></td></tr></table></figure><p>结果为：</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs stylus"><span class="hljs-function"><span class="hljs-title">tensor</span><span class="hljs-params">([<span class="hljs-number">9</span>.])</span></span><br></code></pre></td></tr></table></figure><p>该 loss 由两部分组成：$y{0}$ 和 $y{1}$。其中 $\frac{\partial y{0}}{\partial w}&#x3D;5$，$\frac{\partial y{1}}{\partial w}&#x3D;2$。而 gradtensors 设置两个 loss 对 w 的权重分别为 1 和 2。因此最终 w 的梯度为：$\frac{\partial y{0}}{\partial w} \times 1+ \frac{\partial y_{1}}{\partial w} \times 2&#x3D;9$。</p><p><strong>torch.autograd.grad()（只能对指定的参数求导）</strong></p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs routeros">torch.autograd.grad(outputs, inputs, <span class="hljs-attribute">grad_outputs</span>=None, <span class="hljs-attribute">retain_graph</span>=None, <span class="hljs-attribute">create_graph</span>=<span class="hljs-literal">False</span>, <span class="hljs-attribute">only_inputs</span>=<span class="hljs-literal">True</span>, <span class="hljs-attribute">allow_unused</span>=<span class="hljs-literal">False</span>)<br></code></pre></td></tr></table></figure><p>功能：求取梯度。</p><ul><li>outputs: 用于求导的张量，如 loss</li><li>inputs: 需要梯度的张量</li><li>create_graph: 创建导数计算图，用于高阶求导</li><li>retain_graph:保存计算图</li><li>grad_outputs: 多梯度权重计算</li></ul><p>torch.autograd.grad()的返回结果是一个 tunple，需要取出第 0 个元素才是真正的梯度。</p><p>下面使用torch.autograd.grad()求二阶导，在求一阶导时，需要设置 create_graph&#x3D;True，让一阶导数 grad_1 也拥有计算图，然后再使用一阶导求取二阶导：</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs routeros">x = torch.tensor([3.], <span class="hljs-attribute">requires_grad</span>=<span class="hljs-literal">True</span>)<br>y = torch.pow(x, 2)     # y = x*<span class="hljs-number">*2</span><br><span class="hljs-comment"># 如果需要求 2 阶导，需要设置 create_graph=True，让一阶导数 grad_1 也拥有计算图</span><br>grad_1 = torch.autograd.grad(y, x, <span class="hljs-attribute">create_graph</span>=<span class="hljs-literal">True</span>)   # grad_1 = dy/dx = 2x = 2 * 3 = 6<br><span class="hljs-built_in">print</span>(grad_1)<br><span class="hljs-comment"># 这里求 2 阶导</span><br>grad_2 = torch.autograd.grad(grad_1[0], x)              # grad_2 = d(dy/dx)/dx = d(2x)/dx = 2<br><span class="hljs-built_in">print</span>(grad_2)<br></code></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight scheme"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs scheme">(<span class="hljs-name">tensor</span>([<span class="hljs-name">6.</span>], grad_fn=&lt;MulBackward0&gt;),)<br>(<span class="hljs-name">tensor</span>([<span class="hljs-name">2.</span>]),)<br></code></pre></td></tr></table></figure><p>关于自动求导需要注意的 3 个点：<br>在每次反向传播求导时，计算的梯度不会自动清零。如果进行多次迭代计算梯度而没有清零，那么梯度会在前一次的基础上叠加。</p><p>代码示例：</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs routeros">w = torch.tensor([1.], <span class="hljs-attribute">requires_grad</span>=<span class="hljs-literal">True</span>)<br>x = torch.tensor([2.], <span class="hljs-attribute">requires_grad</span>=<span class="hljs-literal">True</span>)<br><span class="hljs-comment"># 进行 4 次反向传播求导，每次最后都没有清零</span><br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(4):<br>    a = torch.<span class="hljs-built_in">add</span>(w, x)<br>    b = torch.<span class="hljs-built_in">add</span>(w, 1)<br>    y = torch.mul(a, b)<br>    y.backward()<br>    <span class="hljs-built_in">print</span>(w.grad)<br></code></pre></td></tr></table></figure><p>结构如下：</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs stylus"><span class="hljs-function"><span class="hljs-title">tensor</span><span class="hljs-params">([<span class="hljs-number">5</span>.])</span></span><br><span class="hljs-function"><span class="hljs-title">tensor</span><span class="hljs-params">([<span class="hljs-number">10</span>.])</span></span><br><span class="hljs-function"><span class="hljs-title">tensor</span><span class="hljs-params">([<span class="hljs-number">15</span>.])</span></span><br><span class="hljs-function"><span class="hljs-title">tensor</span><span class="hljs-params">([<span class="hljs-number">20</span>.])</span></span><br></code></pre></td></tr></table></figure><p>每一次的梯度都比上一次的梯度多 5，这是由于梯度不会自动清零。使用w.grad.zero_()将梯度清零。</p><figure class="highlight mipsasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs mipsasm">for i in range(<span class="hljs-number">4</span>):<br>    a = torch.<span class="hljs-keyword">add(w, </span>x)<br>    <span class="hljs-keyword">b </span>= torch.<span class="hljs-keyword">add(w, </span><span class="hljs-number">1</span>)<br>    y = torch.<span class="hljs-keyword">mul(a, </span><span class="hljs-keyword">b)</span><br><span class="hljs-keyword"></span>    y.<span class="hljs-keyword">backward()</span><br><span class="hljs-keyword"></span>    print(w.grad)<br>    <span class="hljs-comment"># 每次都把梯度清零</span><br>    <span class="hljs-comment"># w.grad.zero_()</span><br></code></pre></td></tr></table></figure><ul><li>依赖于叶子节点的节点，requires_grad 属性默认为 True。</li><li>叶子节点不可执行 inplace 操作。</li></ul><p>以加法来说，inplace 操作有a +&#x3D; x，a.add_(x)，改变后的值和原来的值内存地址是同一个，非inplace 操作有a &#x3D; a + x，a.add(x)，改变后的值和原来的值内存地址不是同一个。</p><p>代码示例：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;非 inplace 操作&quot;</span>)<br>a = torch.ones((<span class="hljs-number">1</span>, ))<br><span class="hljs-built_in">print</span>(<span class="hljs-built_in">id</span>(a), a)<br><span class="hljs-comment"># 非 inplace 操作，内存地址不一样</span><br>a = a + torch.ones((<span class="hljs-number">1</span>, ))<br><span class="hljs-built_in">print</span>(<span class="hljs-built_in">id</span>(a), a)<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;inplace 操作&quot;</span>)<br>a = torch.ones((<span class="hljs-number">1</span>, ))<br><span class="hljs-built_in">print</span>(<span class="hljs-built_in">id</span>(a), a)<br><span class="hljs-comment"># inplace 操作，内存地址一样</span><br>a += torch.ones((<span class="hljs-number">1</span>, ))<br><span class="hljs-built_in">print</span>(<span class="hljs-built_in">id</span>(a), a)<br></code></pre></td></tr></table></figure><p>结果为：</p><figure class="highlight scss"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs scss">非 inplace 操作<br><span class="hljs-number">2404827089512</span> <span class="hljs-built_in">tensor</span>([<span class="hljs-number">1</span>.])<br><span class="hljs-number">2404893170712</span> <span class="hljs-built_in">tensor</span>([<span class="hljs-number">2</span>.])<br>inplace 操作<br><span class="hljs-number">2404827089512</span> <span class="hljs-built_in">tensor</span>([<span class="hljs-number">1</span>.])<br><span class="hljs-number">2404827089512</span> <span class="hljs-built_in">tensor</span>([<span class="hljs-number">2</span>.])<br></code></pre></td></tr></table></figure><p>如果在反向传播之前 inplace 改变了叶子 的值，再执行 backward() 会报错</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs routeros">w = torch.tensor([1.], <span class="hljs-attribute">requires_grad</span>=<span class="hljs-literal">True</span>)<br>x = torch.tensor([2.], <span class="hljs-attribute">requires_grad</span>=<span class="hljs-literal">True</span>)<br><span class="hljs-comment"># y = (x + w) * (w + 1)</span><br>a = torch.<span class="hljs-built_in">add</span>(w, x)<br>b = torch.<span class="hljs-built_in">add</span>(w, 1)<br>y = torch.mul(a, b)<br><span class="hljs-comment"># 在反向传播之前 inplace 改变了 w 的值，再执行 backward() 会报错</span><br>w.add_(1)<br>y.backward()<br></code></pre></td></tr></table></figure><p>这是因为在进行前向传播时，计算图中依赖于叶子节点的那些节点，会记录叶子节点的地址，在反向传播时就会利用叶子节点的地址所记录的值来计算梯度。比如在 $y&#x3D;a \times b$ ，其中 $a&#x3D;x+w$，$b&#x3D;w+1$，$x$ 和 $w$ 是叶子节点。当求导 $\frac{\partial y}{\partial a} &#x3D; b &#x3D; w+1$，需要用到叶子节点 $w$。</p><h2 id="逻辑回归"><a href="#逻辑回归" class="headerlink" title="逻辑回归"></a>逻辑回归</h2><p>逻辑回归是线性的二分类模型。模型表达式 $y&#x3D;f(z)&#x3D;\frac{1}{1+e^{-z}}$，其中 $z&#x3D;WX+b$。$f(z)$ 称为 sigmoid 函数，也被称为 Logistic 函数。函数曲线如下：(横坐标是 $z$，而 $z&#x3D;WX+b$，纵坐标是 $y$)<br><img src="/img/pytorchtrain/2.png"></p><p>分类原则如下：<br>$0.5&gt;y$，class&#x3D;0；<br>$0.5\leq y$，class&#x3D;1。</p><p>当 $y&lt;0.5$ 时，类别为 0；当 $0.5 \leq y$ 时，类别为 1。</p><p>其中 $z&#x3D;WX+b$ 就是原来的线性回归的模型。从横坐标来看，当 $z&lt;0$ 时，类别为 0；当 $0 \leq z$ 时，类别为 1，直接使用线性回归也可以进行分类。逻辑回归是在线性回归的基础上加入了一个 sigmoid 函数，这是为了更好地描述置信度，把输入映射到 (0,1) 区间中，符合概率取值。</p><p>逻辑回归也被称为对数几率回归 $\ln \frac{y}{1-y}&#x3D;W X+b$，几率的表达式为：$\frac{y}{1-y}$，$y$ 表示正类别的概率，$1-y$ 表示另一个类别的概率。根据对数几率回归可以推导出逻辑回归表达式：<br>$\ln \frac{y}{1-y}&#x3D;W X+b$ $\frac{y}{1-y}&#x3D;e^{W X+b}$ $y&#x3D;e^{W X+b}-y * e^{W X+b}$ $y\left(1+e^{W X+b}\right)&#x3D;e^{W X+b}$ $y&#x3D;\frac{e^{W X+b}}{1+e^{W X+b}}&#x3D;\frac{1}{1+e^{-(W X+b)}}$</p><p>回顾pytorch构建模型的步骤：</p><ol><li>数据处理（包括数据读取，数据清洗，进行数据划分和数据预处理，比如读取图片如何预处理及数据增强，使数据变成网络能够处理的tensor类型，输入数据）</li><li>模型构建（包括构建模型模块，组织复杂网络，初始化网络参数，定义网络层）</li><li>损失函数（包括创建损失函数，设置损失函数超参数，根据不同任务选择合适的损失函数）</li><li>优化器（包括根据梯度使用某种优化器更新参数，管理模型参数，管理多个参数组实现不同学习率，调整学习率。）</li><li>迭代训练（组织上面 4 个模块进行反复训练。还包括观察训练效果，绘制 Loss&#x2F; Accuracy 曲线，用 TensorBoard 进行可视化分析）</li></ol><p>代码示例：</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">import</span> torch<br><span class="hljs-attribute">import</span> torch.nn as nn<br><span class="hljs-attribute">import</span> matplotlib.pyplot as plt<br><span class="hljs-attribute">import</span> numpy as np<br><span class="hljs-attribute">torch</span>.manual_seed(<span class="hljs-number">10</span>)<br><br><span class="hljs-comment"># ============================ step 1/5 生成数据 ============================</span><br><span class="hljs-attribute">sample_nums</span> = <span class="hljs-number">100</span><br><span class="hljs-attribute">mean_value</span> = <span class="hljs-number">1</span>.<span class="hljs-number">7</span><br><span class="hljs-attribute">bias</span> = <span class="hljs-number">1</span><br><span class="hljs-attribute">n_data</span> = torch.ones(sample_nums, <span class="hljs-number">2</span>)<br><span class="hljs-comment"># 使用正态分布随机生成样本，均值为张量，方差为标量</span><br><span class="hljs-attribute">x0</span> = torch.normal(mean_value * n_data, <span class="hljs-number">1</span>) + bias      # 类别<span class="hljs-number">0</span> 数据 shape=(<span class="hljs-number">100</span>, <span class="hljs-number">2</span>)<br><span class="hljs-comment"># 生成对应标签</span><br><span class="hljs-attribute">y0</span> = torch.zeros(sample_nums)                         # 类别<span class="hljs-number">0</span> 标签 shape=(<span class="hljs-number">100</span>, <span class="hljs-number">1</span>)<br><span class="hljs-comment"># 使用正态分布随机生成样本，均值为张量，方差为标量</span><br><span class="hljs-attribute">x1</span> = torch.normal(-mean_value * n_data, <span class="hljs-number">1</span>) + bias     # 类别<span class="hljs-number">1</span> 数据 shape=(<span class="hljs-number">100</span>, <span class="hljs-number">2</span>)<br><span class="hljs-comment"># 生成对应标签</span><br><span class="hljs-attribute">y1</span> = torch.ones(sample_nums)                          # 类别<span class="hljs-number">1</span> 标签 shape=(<span class="hljs-number">100</span>, <span class="hljs-number">1</span>)<br><span class="hljs-attribute">train_x</span> = torch.cat((x0, x1), <span class="hljs-number">0</span>)<br><span class="hljs-attribute">train_y</span> = torch.cat((y0, y1), <span class="hljs-number">0</span>)<br><br><span class="hljs-comment"># ============================ step 2/5 选择模型 ============================</span><br><span class="hljs-attribute">class</span> LR(nn.Module):<br>    <span class="hljs-attribute">def</span> __init__(self):<br>        <span class="hljs-attribute">super</span>(LR, self).__init__()<br>        <span class="hljs-attribute">self</span>.features = nn.Linear(<span class="hljs-number">2</span>, <span class="hljs-number">1</span>)<br>        <span class="hljs-attribute">self</span>.sigmoid = nn.Sigmoid()<br><br>    <span class="hljs-attribute">def</span> forward(self, x):<br>        <span class="hljs-attribute">x</span> = self.features(x)<br>        <span class="hljs-attribute">x</span> = self.sigmoid(x)<br>        <span class="hljs-attribute">return</span> x<br><br><span class="hljs-attribute">lr_net</span> = LR()   # 实例化逻辑回归模型<br><br><span class="hljs-comment"># ============================ step 3/5 选择损失函数 ============================</span><br><span class="hljs-attribute">loss_fn</span> = nn.BCELoss()<br><br><span class="hljs-comment"># ============================ step 4/5 选择优化器   ============================</span><br><span class="hljs-attribute">lr</span> = <span class="hljs-number">0</span>.<span class="hljs-number">01</span>  # 学习率<br><span class="hljs-attribute">optimizer</span> = torch.optim.SGD(lr_net.parameters(), lr=lr, momentum=<span class="hljs-number">0</span>.<span class="hljs-number">9</span>)<br><br><span class="hljs-comment"># ============================ step 5/5 模型训练 ============================</span><br><span class="hljs-attribute">for</span> iteration in range(<span class="hljs-number">1000</span>):<br><br>    <span class="hljs-comment"># 前向传播</span><br>    <span class="hljs-attribute">y_pred</span> = lr_net(train_x)<br>    <span class="hljs-comment"># 计算 loss</span><br>    <span class="hljs-attribute">loss</span> = loss_fn(y_pred.squeeze(), train_y)<br>    <span class="hljs-comment"># 反向传播</span><br>    <span class="hljs-attribute">loss</span>.backward()<br>    <span class="hljs-comment"># 更新参数</span><br>    <span class="hljs-attribute">optimizer</span>.step()<br>    <span class="hljs-comment"># 清空梯度</span><br>    <span class="hljs-attribute">optimizer</span>.zero_grad()<br>    <span class="hljs-comment"># 绘图</span><br>    <span class="hljs-attribute">if</span> iteration % <span class="hljs-number">20</span> == <span class="hljs-number">0</span>:<br>        <span class="hljs-attribute">mask</span> = y_pred.ge(<span class="hljs-number">0</span>.<span class="hljs-number">5</span>).float().squeeze()  # 以<span class="hljs-number">0</span>.<span class="hljs-number">5</span>为阈值进行分类<br>        <span class="hljs-attribute">correct</span> = (mask == train_y).sum()  # 计算正确预测的样本个数<br>        <span class="hljs-attribute">acc</span> = correct.item() / train_y.size(<span class="hljs-number">0</span>)  # 计算分类准确率<br><br>        <span class="hljs-attribute">plt</span>.scatter(x0.data.numpy()[:, <span class="hljs-number">0</span>], x0.data.numpy()[:, <span class="hljs-number">1</span>], c=&#x27;r&#x27;, label=&#x27;class <span class="hljs-number">0</span>&#x27;)<br>        <span class="hljs-attribute">plt</span>.scatter(x1.data.numpy()[:, <span class="hljs-number">0</span>], x1.data.numpy()[:, <span class="hljs-number">1</span>], c=&#x27;b&#x27;, label=&#x27;class <span class="hljs-number">1</span>&#x27;)<br><br>        <span class="hljs-attribute">w0</span>, w1 = lr_net.features.weight[<span class="hljs-number">0</span>]<br>        <span class="hljs-attribute">w0</span>, w1 = float(w0.item()), float(w1.item())<br>        <span class="hljs-attribute">plot_b</span> = float(lr_net.features.bias[<span class="hljs-number">0</span>].item())<br>        <span class="hljs-attribute">plot_x</span> = np.arange(-<span class="hljs-number">6</span>, <span class="hljs-number">6</span>, <span class="hljs-number">0</span>.<span class="hljs-number">1</span>)<br>        <span class="hljs-attribute">plot_y</span> = (-w0 * plot_x - plot_b) / w1<br><br>        <span class="hljs-attribute">plt</span>.xlim(-<span class="hljs-number">5</span>, <span class="hljs-number">7</span>)<br>        <span class="hljs-attribute">plt</span>.ylim(-<span class="hljs-number">7</span>, <span class="hljs-number">7</span>)<br>        <span class="hljs-attribute">plt</span>.plot(plot_x, plot_y)<br><br>        <span class="hljs-attribute">plt</span>.text(-<span class="hljs-number">5</span>, <span class="hljs-number">5</span>, &#x27;Loss=%.<span class="hljs-number">4</span>f&#x27; % loss.data.numpy(), fontdict=&#123;&#x27;size&#x27;: <span class="hljs-number">20</span>, &#x27;color&#x27;: &#x27;red&#x27;&#125;)<br>        <span class="hljs-attribute">plt</span>.title(<span class="hljs-string">&quot;Iteration: &#123;&#125;\nw0:&#123;:.2f&#125; w1:&#123;:.2f&#125; b: &#123;:.2f&#125; accuracy:&#123;:.2%&#125;&quot;</span>.format(iteration, w0, w1, plot_b, acc))<br>        <span class="hljs-attribute">plt</span>.legend()<br>        <span class="hljs-comment"># plt.savefig(str(iteration / 20)+&quot;.png&quot;)</span><br>        <span class="hljs-attribute">plt</span>.show()<br>        <span class="hljs-attribute">plt</span>.pause(<span class="hljs-number">0</span>.<span class="hljs-number">5</span>)<br>        <span class="hljs-comment"># 如果准确率大于 99%，则停止训练</span><br>        <span class="hljs-attribute">if</span> acc &gt; <span class="hljs-number">0</span>.<span class="hljs-number">99</span>:<br>            <span class="hljs-attribute">break</span><br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>深度学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>pytorch</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>深度学习-8-Transformer</title>
    <link href="/2021/08/03/transformer/"/>
    <url>/2021/08/03/transformer/</url>
    
    <content type="html"><![CDATA[<blockquote><p>创作声明：主要为李宏毅老师的听课笔记，附视频链接：<a href="https://www.bilibili.com/video/BV1Wv411h7kN?p=34">https://www.bilibili.com/video/BV1Wv411h7kN?p=34</a></p></blockquote><h2 id="Sequence-to-sequence-Seq2seq"><a href="#Sequence-to-sequence-Seq2seq" class="headerlink" title="Sequence-to-sequence (Seq2seq)"></a>Sequence-to-sequence (Seq2seq)</h2><p>Transformer就是一个Sequence-to-sequence的model，它的缩写我们会写做Seq2seq，那Sequence-to-sequence的model，又是什么呢？<br>我们之前在提到input a sequence的时候，我们说input是一个sequence，那output有三种可能，由机器自己决定output的长度就是Seq2seq。</p><p>机器翻译是一个典型的seq2seq任务，</p><p><img src="/img/transformer/1.png"><br>让机器读一个语言的句子，输出另外一个语言的句子，在做机器翻译的时候，输入的文字的长度是N，输出的句子的长度是N’，N’的长度要由机器自己来决定。<br>除此之外，语音识别，语音合成，聊天机器人甚至文法剖析，multi-label classification都可以用Seq2Seq model的问题解决。</p><p>一般的seq2seq’s model，它里面会分成两块，<strong>一块是Encoder，另外一块是Decoder</strong>。  </p><p>input一个sequence有Encoder，负责处理这个sequence，再把处理好的结果丢给Decoder，由Decoder决定，它要输出什么样的sequence，等一下还会详细阐述Encoder跟Decoder内部的架构。<br><img src="/img/transformer/2.png"><br>我们要学习的transformer就是一个典型Seq2seq的model。</p><h2 id="Encoder"><a href="#Encoder" class="headerlink" title="Encoder"></a>Encoder</h2><h3 id="功能"><a href="#功能" class="headerlink" title="功能"></a>功能</h3><p>Encoder要做的事情，就是给一排向量，输出另外一排向量。</p><p>给一排向量、输出一排向量这件事情，很多模型都可以做到，可能第一个想到的是，我们刚刚讲完的self-attention，其实不只self-attention，RNN CNN其实也都能够做到input一排向量，output另外一个同样长度的向量<br><img src="/img/transformer/3.png"><br>Transformer的Encoder，用的就是self-attention。</p><h3 id="结构"><a href="#结构" class="headerlink" title="结构"></a>结构</h3><p>现在的Encoder里面，会分成很多的block,<br><img src="/img/transformer/4.png"><br>每一个block都是输入一排向量，输出一排向量，输入一排向量给第一个block，第一个block输出另外一排向量，再输给另外一个block，到最后一个block，会输出最终的vector sequence，但<strong>每一个block其实并不是neural network的一层，每一个block里面做的事情，是好几个layer在做的事情</strong>，在transformer的Encoder里面，每一个block做的事情大致如图所示，<br><img src="/img/transformer/5.png"></p><ul><li>先做一个self-attention，input一排vector以后，做self-attention，考虑整个sequence的信息，Output另外一排vector。</li><li>接下来这一排vector，会再丢到fully connected的feed forward network里面，再output另外一排vector，这一排vector就是block的输出<br>事实上在原来的transformer里面，<strong>它做的事情是更复杂的</strong></li></ul><p>在之前self-attention的时候，我们说输入一排vector，就输出一排vector，这边的每一个vector，它是考虑了所有的input以后，所得到的结果，在transformer里面，它加入了一个设计，我们不只是输出这个vector，我们还要把这个vector加上它的input，它要把input拿来直接加给输出，得到新的output。<br><img src="/img/transformer/6.png"><br>这样的network架构，叫做<strong>residual connection（残差连接）</strong>，其实这种residual connection，在deep learning的领域用的是非常的广泛，关于residual connection的详细介绍，日后的笔记会有阐述，我们目前需要了解的是有一种network设计的架构，叫做residual connection，它会把input直接跟output加起来，得到新的vector</p><p>得到residual的结果以后，再做normalization，这边用的不是batch normalization，这边用的叫layer normalization，<br><img src="/img/transformer/7.png"><br>layer normalization做的事情，比bacth normalization更简单一点。<br>输入一个向量输出另外一个向量，不需要考虑batch，它会把输入的这个向量，计算它的mean跟standard deviation。</p><p>但是要注意batch normalization是对不同example，不同feature的同一个dimension，去计算mean跟standard deviation、</p><p>但layer normalization，它是对同一个feature，同一个example里面，不同的dimension，去计算mean跟standard deviation。</p><p>得到layer normalization的输出以后，它的这个输出才是FC network的输入<br><img src="/img/transformer/8.png"><br>FC network这边也有residual的架构，所以我们会把FC network的input，跟它的output加起来做一下residual，得到新的输出，然后把residual的结果再做一次layer normalization，得到的输出，才是residual network里面一个block的输出，所以这个是挺复杂的。</p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p><img src="/img/transformer/2.png"></p><ul><li>首先要有self-attention，并在input的地方加上positional encoding，如果你只用self-attention，你没有位置的信息，所以你需要加上positional的information，然后在这个图上特别强调positional的information。</li><li>Multi-Head Attention，这个就是self-attention的block，这边特别强调它是Multi-Head的self-attention。</li><li>Add&amp;norm，就是residual加layer normalization，我们刚才说self-attention有加上residual的connection，加下来还要过layer normalization，图上的Add&amp;norm，就是residual加layer norm的意思。</li><li>接下来，过完fc的feed forward network以后再做一次Add&amp;norm才是一个block的输出，</li></ul><p>然后这个block会重复n次，就是transformer的encoder。</p><p>这个encoder的network架构是按照原始的论文来设计，但原始论文的设计不代表它是最optimal的设计<br><img src="/img/transformer/9.png"><br>有一篇文章叫<a href="https://arxiv.org/abs/2002.04745">on layer normalization in the transformer architecture</a>，它问的问题就是为什么我们是先做residual再做layer normalization，能不能做residual以后再做layer normalization，你可以看到左边这个图是原始的transformer，右边这个图是把block更换一下顺序以后的transformer，更换一下顺序以后结果是会比较好的，这代表原始的transformer的架构，并不是一个最optimal的设计，你永远可以思考有没有更好的设计方式。</p><h2 id="Decoder"><a href="#Decoder" class="headerlink" title="Decoder"></a>Decoder</h2><h3 id="功能-1"><a href="#功能-1" class="headerlink" title="功能"></a>功能</h3><p>Decoder其实有两种，我们主要介绍比较常见的Autoregressive Decoder。</p><p>Decoder 做的事情就是把 Encoder 的输出先读进去，最后产生最后的输出。</p><p>以语音识别为例，Decoder要产生一段文字，<br><img src="/img/transformer/10.png"><br>首先，你要先给它一个特殊的符号，这个特殊的符号代表开始，这个是一个Special的Token，你需要在本来Decoder可能产生的文字里面，多加一个特殊的字，这个字代表了BEGIN，代表了开始这个事情。</p><p>在机器学习里面，假设你要处理NLP的问题，每一个Token，可以把它用一个One-Hot的Vector来表示，BEGIN也是用One-Hot Vector来表示，其中一维是1，其他是0。</p><p>接下来Decoder会输出一个向量，这个Vector的长度跟你的<strong>Vocabulary Size</strong>是一样的<br><img src="/img/transformer/11.png"><br>Vocabulary Size是什么意思呢？<br>假设我们今天做的是中文的语音识别，我们Decoder输出的是中文，Vocabulary Size，可能就是中文的方块字的数目，就是输出的最小单位的量。</p><p>不同的语言，它输出的单位取决于你对语言的理解，比如说英文，你可以选择输出字母的A到Z，但你可能会觉得字母这个单位太小了，有人可能会选择输出英文的词汇，英文的词汇是用空白作为间隔的，但如果都用词汇当作输出，又太多了，所以有一些方法，可以把英文的前缀字根切出来，拿前缀字根当作单位。</p><p>图中每一个中文的字都会对应到一个数值，因为在产生这个向量之前会先跑一个Softmax，就跟做分类一样，这一个向量里面的分数是一个<strong>Distribution（分布）</strong>，也就是，它这个向量里面的值全部加起来总和会是1，<strong>分数最高的一个中文字，它就是最终的输出</strong>。</p><p>在这个例子里面，“机”的分数最高，所以“机”就是Decoder第一个输出，然后接下来把“机”当做是Decoder新的Input。<br><img src="/img/transformer/12.png"><br>原来Decoder的Input，只有BEGIN这个特别的符号，现在它除了BEGIN以外，它还有“机”作为它的Input，所以Decoder现在它有两个输入，一个是BEGIN这个符号，一个是“机”，根据这两个输入，它输出一个蓝色的向量，根据这个蓝色的向量里面，给每一个中文的字的分数，我们会决定第二个输出，哪一个字的分数最高，它就是输出，假设“器”的分数最高，“器”就是输出，依此类推。<br>Encoder这边也有给decoder的输入，等一下再说对于Encoder的输入Decoder是怎么处理的。</p><p>这边有一个关键的地方，我们特别用红色的虚线把它标出来，也就是说Decoder看到的输入，有它在前一个时间点自己的输出，Decoder会把自己的输出，当做接下来的输入一部分。</p><p>如果Decoder看到错误的输入会不会造成**Error Propagation（误差传播）**的问题呢？</p><p>Error Propagation就是，一步错步步错，比如不小心把机器的“器”，不小心写成天气的“气”，会不会接下来就整个句子都没有办法再产生正确的词汇了？</p><p>有可能，我们最后会提到这个问题要怎么处理，我们暂时先无视这个问题。</p><h3 id="结构-1"><a href="#结构-1" class="headerlink" title="结构"></a>结构</h3><p>我们先来看一下这个Decoder内部的结构，<br><img src="/img/transformer/13.png"><br>我们把Encoder跟Decoder放在一起，你会发现如果我们把Decoder中间这一块把它遮住，其实Encoder跟Decoder，并没有那么大的差别。<br><img src="/img/transformer/14.png"><br>当我们把中间这一块遮起来以后，我们之后再讲遮起来这一块里面做了什么事，Encoder这边Multi-Head Attention，然后Add&amp;Norm，Feed Forward，Add&amp;Norm，重复N次，Decoder其实也是一样，只是最后，我们会再做一个Softmax，使得它的输出变成一个概率。</p><p>这边有一个不一样的地方是，在Decoder这边，Multi-Head Attention这一个Block上面，还加了一个Masked。<br>我们原来的Self-Attention，Input一排Vector，Output另外一排Vector，这一排Vector每一个输出，都要看过完整的Input以后，才做决定，所以输出$b^1$的时候，其实是根据$b^1$到$b^4$所有的信息去输出$b^1$。<br>Masked Attention的不同点是，现在我们不能再看右边的部分，也就是产生$b^1$的时候，我们只能考虑$a^1$的信息，不能够再考虑$a^2a^3a^4$；产生$b^2$的时候，我们只能考虑$a^1a^2$的信息，不能够再考虑$a^3a^4$；产生$b^3$的时候，我们只能考虑$a^1a^2a^3$的信息，不能够再考虑$a^4$；产生$b^4$的时候，我们可以考虑所有的信息。<br>细化到操作如下，<br><img src="/img/transformer/15.png"><br><strong>为什么需要加Masked？</strong><br>对Decoder而言，先有$a^1$才有$a^2$，才有$a^3$$a^4$，所以实际上，当你有$a^2$，要计算$b^2$的时候，你是没有$a^3$和$a^4$的，所以就没有办法把$a^3$$a^4$ 考虑进来。</p><p>还有一个非常关键的问题，<strong>Decoder必须自己决定，输出的Sequence的长度</strong>。<br>可是到底输出的Sequence的长度应该是多少，我们不知道，<br><img src="/img/transformer/16.png"><br>所以我们要让Decoder自己可以输出一个结束，所要准备一个特别的符号叫做“断”，用END来表示这个特殊的符号。<br><img src="/img/transformer/17.png"><br>我们现在把“习”当作输入以后，此时Decoder看到了Encoder输出的这个Embedding，看到了“BEGIN”，看到了“机”“器”“学”“习”以后，看到这些信息以后它要知道语音识别的结果已经结束了，不需要再产生更多的词汇了。</p><p>此时向量END，就是断的那个符号，它的概率必须要是最大的，然后你就输出断这个符号，之后整个Decoder产生Sequence的过程，就结束了。<br>这个就是Autoregressive Decoder的机制。</p><h3 id="Decoder–Non-autoregressive（NAT）"><a href="#Decoder–Non-autoregressive（NAT）" class="headerlink" title="Decoder–Non-autoregressive（NAT）"></a>Decoder–Non-autoregressive（NAT）</h3><p><img src="/img/transformer/18.png"><br>NAT的Decoder输入的是一整排的BEGIN的Token，把一排BEGIN的Token都丢给它，让它一次产生一排Token就结束了。  </p><p>举例来说，如果你丢给它4个BEGIN的Token，它就产生4个中文的字，变成一个句子结束，所以它只要一个步骤，就可以完成句子的生成。</p><p>这边有一个问题：刚才不是说不知道输出的长度应该是多少吗，那我们这边怎么知道BEGIN要放多少个，当做NAT Decoder的收入？<br><img src="/img/transformer/19.png"><br>这件事没有办法很直接的知道，所以有两个做法：</p><ul><li>一个做法是，你另外learn一个Classifier，这个Classifier，它输入Encoder的Input，然后输出是一个数字，这个数字代表Decoder应该要输出的长度，这是一种可能的做法。</li><li>另一种可能做法就是，你就假设一个句子长度的上限，然后B给它一堆BEGIN的Token，假设现在输出的句子的长度绝对不会超过300个字，你就给它300个BEGIN，然后就会输出300个字，然后看看什么地方输出END，END右边的就当做它没有输出，就结束了。</li></ul><p><strong>NAT的Decoder有什么样的好处呢？</strong></p><ul><li>并行化，AT的Decoder在输出它的句子的时候，是逐字产生的，假设要输出长度一百个字的句子，那你就需要做一百次的Decode但是NAT的Decoder不是这样，不管句子的长度如何，都是一个步骤就产生出完整的句子，所以在速度上NAT的Decoder会跑得比AT的Decoder要快，NAT Decoder的想法是有这个Transformer以后，有这种Self-Attention的Decoder以后才有的，因为以前如果你是用LSTM，RNN的话，你就算给它一排BEGIN也没有办法同时产生全部的输出，它的输出还是一个一个产生的，所以在没有这个Self-Attention之前，不会有人想要做什么NAT的Decoder，不过自从有了Self-Attention以后，那NAT的Decoder，现在就算是一个热门的研究的主题了。</li><li>NAT的Decoder有一个好处就是可以控制输出的长度，你比如有一个Classifier决定NAT的Decoder应该输出的长度，那如果在做语音合成的时候，假设你现在突然想要让你的系统讲快一点，就把那个Classifier的Output除以二，它讲话速度就变两倍快，如果你想要这个讲话放慢速度，那你就把那个Classifier输出的那个长度，它Predict出来的长度乘两倍，那你的这个Decoder，说话的速度就变两倍慢。</li></ul><p><strong>NAT的Decoder，它的Performance，往往都不如AT的Decoder</strong>，要让NAT的Decoder，跟AT的Decoder Performance一样好，你必须要用非常多的Trick才能够办到，AT的Decoder随便Train一下，NAT的Decoder你要花很多力气，才有可能跟AT的Performance差不多。</p><h2 id="Encoder-Decoder"><a href="#Encoder-Decoder" class="headerlink" title="Encoder-Decoder"></a>Encoder-Decoder</h2><p>接下来要讲Encoder跟Decoder它们中间是怎么传递信息的了，也就是刚才我们刻意把它遮起来的那一块。<br><img src="/img/transformer/20.png"><br>这块叫做Cross Attention，它是连接Encoder跟Decoder之间的桥梁，Encoder提供两个箭头，然后Decoder提供了一个箭头，所以从左边这两个箭头，Decoder可以读到Encoder的输出。<br>运作的过程如下，<br><img src="/img/transformer/21.png"></p><ul><li>Encoder输入一排向量，接着输出一排向量$a^1a^2a^3$。</li><li>接下来Decoder会先读BEGIN这个Special的Token，BEGIN这个Special的Token读进来以后会经过Mask Self-Attention得到一个向量。</li><li>接下来<strong>把这个向量乘上一个矩阵做一个Transform</strong>，得到一个Query叫做q。</li><li>encoder输出的$a^1a^2a^3$会产生$k^1k^2k^3$和$v^1v^2v^3$。</li><li>q和$k^1k^2k^3$计算Attention的分数，得到$\alpha^1\alpha^2\alpha^3$，做一下Normalization，这边加一个’，代表它做过Normalization。</li><li>把$\alpha^1\alpha^2\alpha^3$分别乘上$v^1v^2v^3$，再把它Weighted Sum加起来会得到v。</li></ul><p>V接下来会输入到Fully-Connected Network做接下来的处理，而以上的五步就叫做Cross Attention。</p><p>Decoder就是通过产生q去Encoder这边综合信息产生V，当做接下来的Fully-Connected Network的Input。</p><p>假设第一个字产生一个“机”，现在产生第二个字，接下来的运作也是一模一样的，输入BEGIN和“机”产生一个向量，这个向量一样乘上一个Linear的Transform……<br><img src="/img/transformer/22.png"></p><h2 id="Training-tips"><a href="#Training-tips" class="headerlink" title="Training tips"></a>Training tips</h2><p>以语音识别为例，说明一个transformer模型的训练技巧。</p><p>首先要有训练资料，要收集一大堆的声音讯号，每一句声音讯号都要有人来辨识它的对应的词汇是什么。</p><p>比如我们已经知道说输入这段声音讯号，第一个应该要输出的中文字是“机”，所以当我们把BEGIN，丢给这个model的时候，它第一个输出应该要跟“机”越接近越好，“机”这个字会被表示成一个One-Hot的Vector，在这个Vector里面，只有“机”对应的维度是1，其他都是0，这是正确答案，Decoder的输出是一个Distribution，是一个概率的分布，我们会希望这一个概率的分布，跟这个One-Hot的Vector越接近越好，<br><img src="/img/transformer/23.png"><br>所以我们会去计算Ground Truth跟Distribution之间的Cross Entropy，我们希望Cross Entropy的值越小越好。</p><p>实际上训练的时候，我们已经知道输出应该是“机器学习”这四个字，就告诉你的Decoder，现在你第一次的输出第二次的输出，第三次的输出第四次输出，应该分别就是“机”“器”“学”跟“习”，这四个中文字的One-Hot Vector，我们希望我们的输出跟这四个字的One-Hot Vector越接近越好。</p><p>在训练的时候，每一个输出都会有一个Cross Entropy，每一个输出跟它对应的正确答案都有一个Cross Entropy，我们要希望所有的Cross Entropy的总和最小，越小越好。<br><img src="/img/transformer/24.png"><br><strong>Decoder的训练：把Ground Truth，正确答案给它，希望Decoder的输出跟正确答案越接近越好。</strong><br>这边有一件值得我们注意的事情，在训练的时候我们会给Decoder看正确答案，也就是我们会告诉它：</p><ul><li>在已经有“BEGIN”，在有“机”的情况下你就要输出“器”</li><li>有“BEGIN”有“机”有“器”的情况下输出“学”</li><li>有“BEGIN”有“机”有“器”有“学”的情况下输出“习”</li><li>有“BEGIN”有“机”有“器”有“学”有“习”的情况下，你就要输出“断”<br>我们会在输入的时候给它正确的答案，这件事情叫做<strong>Teacher Forcing</strong></li></ul><h3 id="Scheduled-Sampling"><a href="#Scheduled-Sampling" class="headerlink" title="Scheduled Sampling"></a>Scheduled Sampling</h3><p>这个时候会有一个问题:</p><p>训练的时候，Decoder有看到正确答案，但是测试的时候没有正确答案可以给Decoder看<br>在真正使用这个模型Inference的时候，Decoder看到的是自己的输入，这中间显然有一个Mismatch，这个不一致的现象叫做<strong>Exposure Bias（曝光偏差）</strong>，<strong>如何解决这个问题</strong>呢？</p><p>有一个可思考的方向是，给Decoder的输入加一些错误的东西，不要给Decoder都是正确的答案，偶尔给它一些错的东西，它反而会学得更好，这一招叫做<strong>Scheduled Sampling</strong>，但是Scheduled Sampling会伤害到Transformer平行化的能力，具体的细节可以看以下的论文<br><img src="/img/transformer/25.png"></p><h3 id="Copy-Mechanism"><a href="#Copy-Mechanism" class="headerlink" title="Copy Mechanism"></a>Copy Mechanism</h3><p>Decoder没有必要自己创造输出出来，它需要做的事情，也许是从输入的东西里面复制一些东西，比如做聊天机器人。<br><img src="/img/transformer/26.png"><br>对机器来说，它没有必要创造库洛洛这个词汇，这对机器来说一定会是一个非常怪异的词汇，它在训练资料里面可能一次也没有出现过，所以它不太可能正确地产生这段词汇出来。</p><p>但是假设今天机器它在学的时候，它学到的是看到输入的时候说我是某某某，就直接把某某某复制出来说某某某你好。</p><p>这样的机器的训练显然会比较容易，它比较有可能得到正确的结果，所以复制对于对话来说是一个非常需要的能力。</p><h3 id="Guided-Attention"><a href="#Guided-Attention" class="headerlink" title="Guided Attention"></a>Guided Attention</h3><p>Guiding Attention要做的事情就是，要求机器在做Attention的时候，是有固定的方式的，举例来说，对语音合成或者是语音识别来说，我们认为Attention应该就是由左向右。<br><img src="/img/transformer/26.png"><br>在这个例子里面，我们用红色的这个曲线，来代表Attention的分数，越高就代表Attention的值越大。</p><p>我们以语音合成为例，输入是一串文字，在合成声音的时候，显然是由左念到右，所以机器应该是，先看最左边输入的词汇产生声音，再看中间的词汇产生声音，再看右边的词汇产生声音。</p><p>如果在做语音合成的时候，机器的Attention是颠三倒四的，它先看最后面，接下来再看前面，那再胡乱看整个句子，那显然有些是做错了。</p><p>所以Guiding Attention要做的事情就是，强迫Attention有一个固定的样貌，那如果你对这个问题本身就已经有正确的理解，比如知道语音合成TTS，Attention的分数，Attention的位置都应该由左向右，那就直接把这个限制，放进你的Training里面，要求机器学到Attention，就应该要由左向右。</p><p>如果想要深入了解Guiding Attention，大家可以自行Google，比如说Mnotonic Attention，或Location-Aware Attention等关键词。</p><h3 id="Beam-Search"><a href="#Beam-Search" class="headerlink" title="Beam Search"></a>Beam Search</h3><p>举一个例子，假设我们现在的这个Decoder就只能产生两个字，一个叫做A一个叫做B，对Decoder而言，它做的事情就是，每一次在A B里面决定一个，比如决定了A以后，再把A当做输入，然后再决定A B要选哪一个。<br><img src="/img/transformer/29.png"><br>我们刚才讲的Process里面，每一次Decoder都是选分数最高的那一个，我们每次都是选Max的那一个，所以假设A的分数0.6，B的分数0.4，Decoder的第一次就会输出A，然后接下来假设B的分数0.6，A的分数0.4，Decoder就会输出B，像这样每次找分数最高的那个Token，每次找分数最高的那个字，来当做输出这件事情叫做Greedy Decoding。</p><p>Greedy Decoding一定是最好的方法吗，有没有可能我们在某一步的时候，选分低的反而会更好，<br><img src="/img/transformer/28.png"><br>比如说第一步虽然B是0.4，但我们就先选0.4这个B，然后接下来我们选了B以后，也许接下来的B的可能性就大增，就变成0.9，然后接下来第三个步骤，B的可能性也是0.9。</p><p>如果你比较红色的这一条路跟绿色这条路的话，你会发现说绿色这一条路，虽然一开始第一个步骤，你选了一个比较差的输出，但是接下来的结果是好的。</p><p>所以我们要怎么找到，这个最好的绿色这一条路呢，也许一个可能是爆搜所有可能的路径，但问题是我们并没有办法爆搜所有可能的路径，如果是在输出中文，日常使用的中文有至少4000个字，所以这个树每一个地方分叉，都是至少4000个可能的路径，你走两三步以后，你就无法穷举。</p><p>而有一个算法叫做<strong>Beam Search（束搜索）</strong>，它用比较有效的方法找一个不是完全精准的的路，这个技术叫做Beam Search，有兴趣的可以自行搜索。</p><h2 id="本文尚待回答的问题（后续文章更新）"><a href="#本文尚待回答的问题（后续文章更新）" class="headerlink" title="本文尚待回答的问题（后续文章更新）"></a>本文尚待回答的问题（后续文章更新）</h2><ul><li>Resnet的相关知识</li></ul>]]></content>
    
    
    <categories>
      
      <category>深度学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>Transformer</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>深度学习-7-Self attention</title>
    <link href="/2021/07/30/selfattention/"/>
    <url>/2021/07/30/selfattention/</url>
    
    <content type="html"><![CDATA[<blockquote><p>创作声明：主要为李宏毅老师的听课笔记，附视频链接：<a href="https://www.bilibili.com/video/BV1Wv411h7kN?p=23">https://www.bilibili.com/video/BV1Wv411h7kN?p=23</a></p></blockquote><h2 id="Sophisticated-Input"><a href="#Sophisticated-Input" class="headerlink" title="Sophisticated Input"></a>Sophisticated Input</h2><p>之前的笔记提到的Network的Input都是一个向量，不管是在预测YouTube观看人数的问题上还是影像处理上，我们的输入都可以看作是一个向量，我们的输出，可能是一个数值，这个是Regression，可能是一个类别，这是Classification。</p><p>但假设我们遇到更复杂的问题，比如我们说输入是多个向量，而且这个输入的向量的数目是会改变的，或者我们刚才在讲图像识别的时候，还特别强调我们假设输入的图像大小都是一样的，那现在每次我们Model输入的Sequence的数目，Sequence的长度都不一样呢，那这个时候应该要怎么处理？这个问题就是<strong>Sophisticated Input（复杂信息输入）</strong>。</p><p>而Sophisticated Input的目的是什么呢？</p><p>假设我们今天要Network的输入是一个句子，每一个句子的长度都不一样，每个句子里面词汇的数目都不一样，<br><img src="/img/selfattention/1.png"><br>如果我们把一个句子里面的每一个词汇，都描述成一个向量，那我们的Model的输入，就会是一个Vector Set，而且这个Vector Set的大小，每次都不一样，句子的长度不一样，那你的Vector Set的大小就不一样。</p><p>把一个词汇表示成一个向量，最简单的做法是One-Hot的Encoding，<br><img src="/img/selfattention/2.png"><br>开一个很长很长的向量，这个向量的长度跟世界上存在的词汇的数目是一样多的，每一个维度对应到一个词汇，Apple就是100，Bag就是010，Cat就是001，以此类推。</p><p>但是这样的表示方法有一个非常严重的问题，它假设所有的词汇彼此之间都是没有关系的，从这个向量里面你看不到：Cat跟Dog都是动物所以他们比较接近，Cat跟Apple一个动物一个植物，所以他们比较不相像。这个向量里面，没有任何这些关系的信息。</p><p>有另外一个方法叫做<strong>Word Embedding（单词嵌入）</strong>，如果将word看作文本的最小单元，可以将Word Embedding理解为一种映射，其过程是：将文本空间中的某个word，通过一定的方法，映射或者说嵌入（embedding）到另一个数值向量空间。<br><img src="/img/selfattention/3.png"><br>这个向量具有语义的信息，如果把Word Embedding画出来的话，你会发现，所有的动物可能聚集成一团，所有的植物可能聚集成一团，所有的动词可能聚集成一团。</p><h2 id="What-is-the-output？"><a href="#What-is-the-output？" class="headerlink" title="What is the output？"></a>What is the output？</h2><p>输入可以是一堆向量，那这个时候，我们有可能有什么样的输出呢，有三种可能性：</p><ol><li><strong>每一个向量都有一个对应的Label</strong><br>当你的模型，看到输入是四个向量的时候，它就要输出四个Label，而每一个Label，它可能是一个数值，那就是Regression的问题，如果每个Label是一个Class，那就是一个Classification的问题。</li></ol><p>举例来说在文字处理上，假设你今天要做的是<strong>POS Tagging（词性标注）</strong>，你要让机器自动决定每一个词汇它是什么样的词性，它是名词还是动词还是形容词等等。</p><p>这个任务其实并不容易，举例来说，你现在看到一个句子，I saw a saw.并不是“我看一个看”，而是“我看到一个锯子”，这个第二个saw当名词用的时候，它是锯子，所以机器要知道第一个saw是个动词，第二个saw虽然它也是个saw，但它是名词。</p><p>这个任务输入跟输出的长度是一样的，这个就是属于第一个类型的输出。<br>2. <strong>一整个Sequence，只需要输出一个Label</strong><br>举例来说，<strong>Sentiment Analysis（情感分析）</strong>，Sentiment Analysis就是给机器看一段话，它要决定这段话是正面的还是负面的。  </p><p>假设你的公司开发了一个产品，这个产品上线了，你想要知道网友的评价怎么样，但是你又不可能一则一则网友的留言都去分析，那可以用Sentiment Analysis的技术，让机器自动去判断当一则贴文里面有提到某个产品的时候，它是正面的还是负面的，这个是Sentiment Analysis，给一整个句子，只需要一个Label，Positive或Negative，这个就是第二类的输出<br>。<br>3. <strong>计算机自己决定应该要输出多少个Label</strong><br>我们不知道应该输出多少个Label，机器要自己决定，应该要输出多少个Label，可能你输入是N个向量，输出可能是N’个Label，为什么是N’，机器自己决定，这种任务又叫做sequence to sequence的任务。翻译就是sequence to sequence的任务，因为输入输出是不同的语言，它们的词汇的数目本来就不会一样多。</p><p>今天先拿第一种情况举例。</p><h2 id="Sequence-Labeling"><a href="#Sequence-Labeling" class="headerlink" title="Sequence Labeling"></a>Sequence Labeling</h2><p>输入跟输出数目一样多的状况又叫做Sequence Labeling，你要给Sequence里面的每一个向量，都给它一个Label，那要如何处理Sequence Labeling的问题呢？</p><p>直觉的想法是我们就用Fully-Connected Network，虽然这个输入是一个Sequence，但我们就各个击破，不要管它是不是一个Sequence，把每一个向量，分别输入到Fully-Connected的Network里面，然后Fully-Connected的Network就会给我们输出。</p><p>那这么做显然有非常大的瑕疵，假设今天是词性标记的问题，给机器一个句子，I saw a saw，对Fully-Connected Network来说，后面这一个saw跟前面这个saw完全一模一样，它们是同一个词汇，既然Fully-Connected的Network输入同一个词汇，它没有理由输出不同的东西，但实际上，你期待第一个saw要输出动词，第二个saw要输出名词，但对Network来说它不可能做到，因为这两个saw明明是一模一样的，你叫它一个要输出动词，一个要输出名词，它完全不知道要怎么处理。</p><p>所以有没有可能让Fully-Connected的Network，考虑更多的信息，比如说上下文这样的信息呢？</p><p>这是有可能的，把前后几个向量都串起来，一起丢到Fully-Connected的Network就结束了，让它可以考虑一些跟我现在要考虑的这个向量相邻的其他向量的新鞋，但是真正的问题，但是如果今天我们有某一个任务，不是考虑一部分就可以解决的，而是要考虑一整个Sequence才能够解决的话，那要怎么办呢？</p><p>有人可能会说这个很容易，把Window开大一点然后把整个Sequence盖住就结束了，但是今天Sequence的长度是有长有短的，我们输入给我们的Model的Sequence的长度，每次可能都不一样，那你可能要统计一下你的训练数据，然后看看你的训练数据里面，最长的Sequence有多长，然后开一个Window比最长的Sequence还要长，你才有可能把整个Sequence盖住。</p><p>但是你开一个这么大的Window，意味着Fully-Connected Network，它需要非常多的参数，不只运算量很大，可能还容易Overfitting</p><p>所以有没有更好的方法，来考虑整个Input Sequence的信息呢，这就要用到**Self-Attention（自注意力）**这个机制。</p><h2 id="Self-Attention"><a href="#Self-Attention" class="headerlink" title="Self-Attention"></a>Self-Attention</h2><h3 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h3><p>Self-Attention的运作方式就是，Self-Attention会接收一整个Sequence的信息，然后你Input几个Vector，它就输出几个Vector，这边输入4个Vector，它就Output 4个Vector<br>，这4个Vector，他们都是考虑一整个Sequence以后才得到的。<br><img src="/img/selfattention/4.png"><br>如此一来，Fully-Connected Network就不是只考虑一个非常小的范围，而是考虑整个Sequence的信息，再来决定现在应该要输出什么样的结果，这个就是Self-Attention。</p><p>Self-Attention可以叠加使用。  </p><p>Fully-Connected Network，跟Self-Attention交替使用，Self-Attention处理整个Sequence的信息，Fully-Connected Network专注于处理某一个位置的信息，<br>再用Self-Attention，再把整个Sequence信息再处理一次，然后交替使用Self-Attention跟Fully-Connected。</p><h3 id="Self-Attention过程"><a href="#Self-Attention过程" class="headerlink" title="Self-Attention过程"></a>Self-Attention过程</h3><p>Self-Attention的Input，它就是一串的Vector，那这串Vector可能是你整个Network的Input，它也可能是某个Hidden Layer的Output，所以我们这边不用$x$来表示它，我们用$a$来表示它，代表它有可能是前面已经做过一些处理，它是某个Hidden Layer的Output，那Input一排a这个向量以后，Self-Attention要Output另外一排$b$这个向量。<br><img src="/img/selfattention/5.png"><br>每一个b都是考虑了所有的a以后才生成出来的，所以这边刻意画了非常非常多的箭头，告诉你$b^1$考虑了$a^1$到$a^4$产生的，$b^2$考虑了$a^1$到$a^4$产生的，b3b4也都是考虑整个input的sequence才产生的。<br>以$b^1$为例，阐述b产生过程。<br>这里有一个特别的机制，这个机制是根据$a^1$这个向量找出整个很长的sequence里面，到底哪些部分是重要的，哪些部分跟判断$a^1$是哪一个label有关系的，哪些部分是我们要决定$a^1$的class，决定$a^1$的regression数值的时候，所需要用到的信息。<br><img src="/img/selfattention/6.png"><br>每一个向量跟$a^1$的关联的程度，用一个数值叫α来表示。</p><p>self-attention的module，怎么自动决定两个向量之间的关联性呢，你给它两个向量$a^1$跟$a^4$，它怎么决定$a^1$跟$a^4$有多相关，然后给它一个数值α，这边需要一个计算attention的模组。<br><img src="/img/selfattention/7.png"><br>计算attention的模组拿两个向量作为输入，然后直接输出α那个数值，<strong>计算这个α的数值有各种不同的做法</strong>，我们提一种方法，这是今日最常用的方法，也是用在Transformer里面的方法:    </p><ul><li>用<strong>dot product（数量积）</strong>，输入的这两个向量分别乘上两个不同的矩阵，左边这个向量乘上$W^q$矩阵得到矩阵q，右边这个向量乘上$W^k$矩阵得到矩阵k。</li><li>q与k做dot product（把他们做element-wise的相乘再全部加起来）以后就得到一个scalar，这个scalar就是α。</li></ul><p>我们需要把$a^1$去跟$a^2a^3a^4$，分别都去计算他们之间的关联性，也就是计算他们之间的α，<br><img src="/img/selfattention/8.png"></p><ul><li><p>你把$a^1$乘上$W^q$得到$q^1$，q我们叫做Query，它就像是你搜寻相关文章的关键字，所以这边叫做Query，然后$a^2a^3a^4$都要乘上$W^k$，得到$k$这个Vector，这个Vector叫做Key。</p></li><li><p>把Query q1，跟这个Key k2，算dot product就得到$\alpha_{1,2}$（往往除以维度的平方根，使得梯度更加稳定）。</p></li><li><p>$\alpha_{1,2}$代表Query是1提供的，Key是2提供的时候，这个1跟2他们之间的关联性，α的这个关联性也叫做Attention的Score，以此类推$\alpha_{1,3}$，$\alpha_{1,4}$ 。</p></li></ul><p><strong>实际中，$a^1$也会跟自己算关联性。</strong></p><p>计算出，a1跟每一个向量的关联性以后，接下来这边会接入一个<strong>Soft-Max</strong><br><img src="/img/selfattention/9.png"><br>这个Soft-Max跟分类的时候的那个Soft-Max是一模一样的，所以Soft-Max的输出就是一排α，这一排α通过Soft-Max就得到$\alpha^\prime$。</p><p>这边不一定要用Soft-Max，用别的替代也没问题，比如说有人尝试过做ReLU，结果发现还比Soft-Max好一点，其实不一定要用Soft-Max，这边你要用什么Activation Function都行，Soft-Max是最常见的，可以自己尝试，看能不能试出比Soft-Max更好的结果。</p><p>得到$\alpha^\prime$以后，我们要根据这个$\alpha^\prime$去抽取出这个Sequence里面重要的信息，根据这个α我们已经知道哪些向量跟$a^1$是最有关系的，怎么抽取重要的信息呢？<br><img src="/img/selfattention/10.png"></p><ul><li>首先把$a^1$到$a^4$这边每一个向量乘上$W^v$得到新的向量，这边分别用$v^1v^2v^3v^4$来表示。</li><li>接下来把这边的$v^1$到$v^4$，每一个向量都去乘上Attention的分数（$\alpha^\prime$），</li><li>然后再把它加起来，得到b1</li></ul><p>如果某一个向量它得到的分数越高，比如说$a^1$跟$a^2$的关联性很强，$\alpha^\prime$得到的值很大，那我们今天在做Weighted Sum以后得到的的值，就可能会比较接近$v^2$。</p><p>所以谁的Attention的分数最大，谁的$v$就会Dominant你抽出来的结果，这边就讲了怎么从一整个Sequence得到$b^1$。</p><p>从这一排vector得到$b^2$，跟从这一排vector得到$b^1$的操作是一模一样的。要强调一点是$b^1$到$b^4$，它们并不需要依序产生，它们是一次同时被计算出来的。</p><h3 id="从矩阵的角度Self-Attention过程"><a href="#从矩阵的角度Self-Attention过程" class="headerlink" title="从矩阵的角度Self-Attention过程"></a>从矩阵的角度Self-Attention过程</h3><p>我们已知每一个a都产生q k v，<br><img src="/img/selfattention/11.png"><br>用矩阵运算表示这个操作，<br><img src="/img/selfattention/12.png"><br>每一个a都要乘上$W^q$，得到$q^i$，这些不同的a你可以把它合起来，当作一个矩阵来看待，这个矩阵我们用I来表示，这个矩阵的四个column就是$a^1$到$a^4$，I乘上$W^q$就得到另外一个矩阵$Q$，$Q$就是把$q^1$到$q^4$这四个vector拼起来，就是$Q$的四个column，$W^q$其实是network的参数，它是会被learn出来的。</p><p>接下来产生k跟v的操作跟q是一模一样的，vector sequence乘上三个不同的矩阵，你就得到了q，k，v。</p><p>下一步是，每一个q都会去跟每一个k，去计算attention的分数，得到attention分数这一件事情，如果从矩阵操作的角度来看，它在做什么样的事情呢？<br><img src="/img/selfattention/13.png"><br>$q^1$跟$k^1$做inner product得到$α_{1,1}$，实际上是$k^1$的<strong>transpose</strong>与$q^1$向量乘法，以此类推四次，那四次的操作，其实可以把它拼起来，看作是矩阵跟向量相乘。<br><img src="/img/selfattention/14.png"><br>那从$q^1$以此类推四次，那四次的操作，其实可以把它拼起来，看作是矩阵跟矩阵相乘。<br><img src="/img/selfattention/15.png"><br>我们会把attention的分数，做一下normalization，比如softmax，你会对这边的每一个column，每一个column做softmax，让每一个column里面的值相加是1，我们用$A^\prime$来表示通过softmax以后的结果。</p><p>把$v^1$到$v^4$当成是V这个矩阵的四个column，然后接下来你把v乘上$A^\prime$的第一个column以后，你得到的结果就是$b^1$（线性代数），以此类推，最后如图得到<br><img src="/img/selfattention/16.png"><br>所以我们等于就是把$A^\prime$这个矩阵，乘上V这个矩阵，得到O这个矩阵，O这个矩阵里面的每一个column，就是Self-attention的输出，也就是$b^1$到$b^4$。</p><p>总结如下图：<br><img src="/img/selfattention/17.png"></p><h2 id="Multi-head-Self-attention"><a href="#Multi-head-Self-attention" class="headerlink" title="Multi-head Self-attention"></a>Multi-head Self-attention</h2><p>Self-attention 进阶的版本 Multi-head Self-attention今天的使用非常广泛。</p><p>我们在做这个Self-attention的时候，我们就是用q去找相关的k，但是相关这件事情有很多种不同的形式，有很多种不同的定义，<strong>所以我们不能只有一个q，我们应该要有多个q，不同的q负责不同种类的相关性</strong>。</p><p>需要用多少的head，这个又是另外一个hyperparameter。</p><p>以两个head为例:<br><img src="/img/selfattention/18.png"><br>总之，各忙各的。<br>i是指$a^i$的上角标，$b^{i,1}$是$a^i$在第一个head下的输出，$b^{i,2}$是$a^i$在第二个head下的输出。<br>接下来你可能会把$b^{i,1}$与$b^{i,2}$接起来，然后再通过一个 transform也就是再乘上一个矩阵，然后得到 bi，再送到下一层去，这个就是 Multi-head attention，一个 Self-attention 的变形。</p><h2 id="Positional-Encoding"><a href="#Positional-Encoding" class="headerlink" title="Positional Encoding"></a>Positional Encoding</h2><p>思考到现在，你会发现Self-attention的layer少了一个也许很重要的信息，这个信息是位置的信息</p><p>对一个Self-attention layer而言，每一个input，它是出现在sequence的最前面，还是最后面，它是完全没有这个信息的。</p><p>所以对Self-attention而言，位置1跟位置2跟位置3跟位置4，完全没有任何差别，这四个位置的操作其实是一模一样，对它来说q1到跟q4的距离，并没有特别远，2跟3的距离也没有特别近。</p><p>但是这样设计可能会有一些问题，因为有时候位置的信息也许很重要，举例来说，我们在做这个POS tagging，就是词性标记的时候，比如你知道说动词比较不容易出现在句首，所以如果我们知道某一个词汇它是放在句首的，那它是动词的可能性可能就比较低，这样子的位置的信息往往也是有用的。</p><p>我们采用**positional encoding（位置编码）**的方法去添加位置信息。<br><img src="/img/selfattention/19.png"><br>我们为每一个位置设定一个vector，叫做positional vector，这边用$e^i$来表示，上标i代表是位置，每一个不同的位置，就有不同的vector，比如$e^1$是一个vector，$e^2$是一个vector，$e^112$是一个vector，不同的位置都有一个它专属的e，然后把这个e加到$a^i$上面作为新的$a^i$参与后面的运算过程，就结束了。</p><p>positional encoding的具体方法仍然是一个尚待研究的问题，你可以创造自己新的方法，或甚至positional encoding，是可以根据数据学出来的。<br>在 Transformer 中采用了手工设计的公式，计算公式如下：<br><img src="/img/selfattention/20.png"><br>pos表示token在sequence中的位置，i或者准确以上是2i和2i+1便是了positionnal encoding的维度，i的取值范围是：[0,……，$d_{model&#x2F;2}$)。底数是10000，为什么要使用10000呢？这个就类似于玄学了，原论文中完全没有提，这里不得不说说论文的readability的问题，即便是很多高引的文章，最基本的内容都讨论不清楚，所以才出现像上面提问里的讨论，说实话这些论文还远远没有做到easy to follow。<br>关于positional encoding和三角函数有什么关系？<a href="https://www.zhihu.com/question/347678607/answer/864217252">可以参照这位的理解</a></p><h2 id="Applications-for-Image"><a href="#Applications-for-Image" class="headerlink" title="Applications  for Image"></a>Applications  for Image</h2><p>其实一张图片，我们也可以换一个观点，把它看作是一个vector的set，一个分辨率5乘以10的图片可以看作是一个tensor，这个tensor的大小是5乘以10乘以3，3代表RGB这3个channel。（笔者研究图像处理，所以对于attention在并行化上甩开RNN等自然语言处理方向问题不再赘述）</p><p>你可以把每一个位置的pixel，看作是一个三维的向量，所以每一个pixel，其实就是一个三维的向量，那整张图片，其实就是5乘以10个向量的set。</p><p>所以我们其实可以换一个角度，图像这个东西，其实也是一个vector set，它既然也是一个vector set的话，你完全可以用Self-attention来处理一张图片。</p><p>我们可以来比较一下，<strong>Self-attention跟CNN之间，有什么样的差异或者是关联性</strong>。</p><p>如果我们今天用Self-attention来处理一张图片，假设这个是你要考虑的pixel，那它产生query，其他pixel产生key，</p><p><img src="/img/selfattention/21.png"></p><p>我们在做inner product的时候，你考虑的不是一个小的receptive field的信息，而是整张图像的信息</p><p>但是在做CNN的时候，会画出一个receptive field，每一个filter，每一个neural，只考虑receptive field范围里面的信息。</p><p>所以比较CNN跟Self-attention的话，<strong>CNN可以看作是一种简化版的Self-attention，因为在做CNN的时候，我们只考虑receptive field里面的信息，而在做Self-attention的时候，我们是考虑整张图片的信息，或者是你可以反过来说，Self-attention是一个复杂化的CNN</strong>。</p><p>在CNN里面，我们要划定receptive field，每一个neural，只考虑receptive field里面的信息，而receptive field的范围跟大小，是人决定的，而对Self-attention而言，我们用attention，去找出相关的pixel，就好像是receptive field是自动被学出的，network自己决定receptive field的形状长什么样子，network自己决定说，以这个pixel为中心，哪些pixel是我们真正需要考虑的，哪些pixel是相关的。<br>所以<strong>receptive field的范围，不再是人工划定，而是让机器自己学出来</strong>。</p><p>在这篇paper里面，会用数学的方式严谨的告诉你说，其实CNN就是Self-attention的特例，Self-attention只要设定合适的参数，它可以做到跟CNN一模一样的事情<br><img src="/img/selfattention/22.png"><br>所以self attention，是更flexible的CNN，而CNN是有受限制的Self-attention，Self-attention只要通过某些设计和限制，它就会变成CNN。</p><p>所以Self-attention它弹性比较大，所以需要比较多的训练数据，训练数据少的时候，就会overfitting<br>而CNN它弹性比较小，在训练数据少的时候，结果比较好，但训练数据多的时候，它没有办法从更大量的训练数据得到好处，如下图实验结果：<br><img src="/img/selfattention/23.png"></p><h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h2><p>Self-attention有非常非常多的变形，你可以看一篇paper叫做Long Range Arena，里面比较了各种不同的Self-attention的变形:<br><img src="/img/selfattention/24.png"></p><p>Self-attention它最大的问题就是，它的运算量非常地大，所以怎么样减少Self-attention的运算量，是一个未来的重点，可以看到这边有各种Self-attention的变形。</p><p>可以看到，横轴代表它运算的速度，纵轴代表是performance，所以有很多各式各样新的xxformer，它们的速度会比原来的Transformer快，但是快的速度带来的就是performance变差，到底什么样的Self-attention，才能够又快又好，这仍然是一个<strong>尚待研究的问题</strong>。</p>]]></content>
    
    
    <categories>
      
      <category>深度学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>Self-attention</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>深度学习-6-CNN</title>
    <link href="/2021/07/29/cnn/"/>
    <url>/2021/07/29/cnn/</url>
    
    <content type="html"><![CDATA[<blockquote><p>创作声明：主要为李宏毅老师的听课笔记，附视频链接：<a href="https://www.bilibili.com/video/BV1Wv411h7kN?p=22">https://www.bilibili.com/video/BV1Wv411h7kN?p=22</a></p></blockquote><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>CNN是<strong>Convolutional Neural Network（卷积神经网络）</strong> 的缩写，是专用于影像的神经网络。接下来的笔记将通过CNN的例子介绍Network的架构，它的设计有什么样的想法，为什么设计Network的架构可以让我们的Network结果做得更好。</p><h2 id="从图像分类说起"><a href="#从图像分类说起" class="headerlink" title="从图像分类说起"></a>从图像分类说起</h2><p>我们要做图像的分类，也就是给机器一张图片，它要去决定这张图片里面有什么样的东西，那怎么做呢？</p><p>之前的笔记已经讲过怎么做分类这件事情，在以下的讨论里面，我们都假设我们的模型输入的图片大小是固定的，举例来说它固定输入的图片大小，都是100× 100的分辨率，就算是今天Deep Learning已经这么的Popular，我们往往都还是需要假设说，一个模型输入的图像大小都是一样的。</p><p>对于不同大小的图像，今天常见的处理方式是把所有图片都先Rescale成大小一样，再输入到图像辨识系统。</p><p>我们模型的目标是分类，所以我们会把每一个类别，表示成一个One-Hot的Vector，我的目标就叫做$\hat{y}$,<br><img src="/img/cnn/1.png"><br>在这个One-Hot的Vector里面，假设我们现在类别是一个猫的话，那猫所对应的Dimension，它的数值就是1，其他的东西所对的Dimension的数值就是0，Dimension的长度决定了模型可以辨识出多少不同种类的东西，今天比较强的图像辨识系统，往往可以辨识出1000种以上的东西，甚至到上万种不同的Object，那如果你今天希望你的图像辨识系统，它可以辨识上万种Object，那你的Label就会是一个上万维，维度是上万的One-Hot Vector。<br><img src="/img/cnn/2.png"><br>我们的模型的输出通过Softmax以后输出是$y\prime$，我们希望$y\prime$和$\hat{y}$的Cross Entropy越小越好。</p><h2 id="图像的输入"><a href="#图像的输入" class="headerlink" title="图像的输入"></a>图像的输入</h2><p><strong>接下来的问题是怎么把图像输入？</strong></p><p>其实对于计算机来说，一张图片其实是一个三维的<strong>Tensor（张量）</strong>，如果不知道Tensor是什么的话，你就认为它是维度大于2的矩阵就是Tensor，矩阵是二维，那超过二维的矩阵，你就叫它Tensor。<br><img src="/img/cnn/3.png"><br>一张图片是一个三维的Tensor，其中一维代表图片的宽，另外一维代表图片的高，还有一维代表图片的**Channel（通道）**的数目。</p><p>一张彩色的图片每一个Pixel，都是由R G B三个颜色所组成的，所以这三个Channel就代表了R G B三个颜色，那长跟宽就代表了今天这张图片的分辨率，可以表示这张图片里面像素的数目。</p><p>到目前为止我们所说的Network的输入其实都是一个向量，所以我们只要能够把一张图片变成一个向量，我们就可以把它当做是Network的输入，但是怎么把这个三维的Tensor变成一个向量呢，最直觉的方法就是直接拉直它。</p><p>一个三维的Tensor里面有几个数字呢？</p><p>在这个例子里面有100×100×3个数字，把这些数字拿出来排成一排，就是一个巨大的向量，这个向量可以作为Network的输入，而这个向量里面，每一维它里面存的数值，其实就是某一个Pixel某一个颜色的强度，每一个Pixel由R G B三个颜色所组成。</p><p>之前的笔记的网络都是<strong>Fully Connected Network（全连接神经网络）</strong>，如果我们把向量当做这种Network的输入，它的长度就是100×100×3，假设第一层的Neuron的数目有1000个，<br>每一个Neuron跟输入的向量的每一个数值都会有一个Weight，所以如果输入的向量长度是100×100×3，有1000个Neuron，第一层的Weight就有1000×100×100×3，也就是$3×10^7$，是一个非常巨大的数目。<br><img src="/img/cnn/4.png"><br>虽然随着参数的增加，我们可以增加模型的弹性，但是我们也增加了Overfitting的风险，有关模型的弹性，overfitting怎么产生的，这是相关的<a href="https://speech.ee.ntu.edu.tw/~hylee/ml/ml2021-course-data/W14_PAC-introduction.pdf">数学证明</a>，总之概念上，如果模型的弹性越大越容易Overfitting。</p><p>在做图像辨识的时候，怎么避免使用这么多的参数，考虑到图像辨识这个问题本身的特性，其实我们并不一定需要Fully Connected，我们其实不需要每一个Neuron，跟Input的每一个Dimension都有一个Weight。</p><p>我们需要的是对图像辨识这个问题，对图像本身的特性的一些观察。</p><h2 id="简化"><a href="#简化" class="headerlink" title="简化"></a>简化</h2><h3 id="第一个简化"><a href="#第一个简化" class="headerlink" title="第一个简化"></a>第一个简化</h3><h4 id="观察"><a href="#观察" class="headerlink" title="观察"></a>观察</h4><p>假设我们想要知道这张图片里面有一个鸟要怎么做呢？</p><p><strong>对一个图像辨识的类神经网络里面的神经元而言，要做的就是侦测现在这张图片里面有没有出现一些特别重要的Pattern。</strong><br><img src="/img/cnn/5.png"><br>举例来说，有某一个Neuron，它看到鸟嘴这个Pattern，又有某个Neuron说，它看到眼睛这个Pattern<br>，又有某个Neuron说，它看到鸟爪这个Pattern。</p><p>类神经网络可以告诉你，因为看到了这些Pattern，所以它看到了一只鸟。<br>人类其实也是用同样的方法，来看一张图片中有没有一只鸟。</p><p>假设我们现在用Neuron做的事情就是判断说现在有没有某种Pattern出现，那我们并不需要每一个Neuron都去看一张完整的图片，因为这一些重要的Pattern，比如说鸟嘴比如说眼睛比如说鸟爪，并不需要看整张完整的图片，才能够得到这些信息，所以这些Neuron不需要把整张图片当作输入，它们只需要把图片的一小部分当作输入，就足以让它们侦测某些特别关键的Pattern有没有出现了。</p><p>根据这个观察，我们就可以做第一个简化。</p><h4 id="简化-1"><a href="#简化-1" class="headerlink" title="简化"></a>简化</h4><p>在CNN里面我们会设定一个区域叫做<strong>Receptive Field（感受野）</strong>，每一个Neuron都只关心自己的Receptive Field里面发生的事情就可以</p><p>举例来说，你会先定义说这个蓝色的Neuron，它的守备范围就是这一个Receptive Field，那这个Receptive Field里面有3×3×3个数值，那对蓝色的Neuron来说，它只需要关心这一个小范围就好，不需要在意整张图片里面有什么东西。<br><img src="/img/cnn/6.png"><br>它要做的事情就是</p><ol><li>把这3×3×3的数值拉直，变成一个长度是3×3×3也就是27维的向量，再把这27维的向量作为这个Neuron的输入。</li><li>这个Neuron会给27维的向量的，每一个Dimension一个Weight，所以这个Neuron有3×3×3 27个Weight。</li><li>加上Bias得到的输出，这个输出再送给下一层的Neuron当作输入。<br><img src="/img/cnn/7.png"><br>每一个Neuron，它只考虑自己的Receptive Field，而Receptive Field要怎么决定要问你自己<br><img src="/img/cnn/8.png"><br>你可以这么设定，这边有个蓝色的Neuron，它就看左上角这个范围，这是它的Receptive Field，另外又有另外一个黄色的Neuron，它是看右下角这个3×3×3的范围，Receptive Field彼此之间也可以是重叠的，比如说我现在画一个Receptive Field，那这个地方它是绿色的Neuron的守备范围，它跟蓝色的跟黄色的都有一些重叠的空间。</li></ol><p>甚至可以两个不同的Neuron守备看到的范围是一样的，一个范围使用一个Neuron来守备可能没有办法侦测所有的Pattern，所以同个Receptive Field可以有多个不同的Neuron。</p><p>Receptive Field可以有大有小，可以只考虑某些channel，形状也可以是正方形长方形，如果说你要设计很奇怪的Receptive Field，去解决很特别的问题，那完全是可以的，这都是你自己决定的。</p><p>虽然Receptive Field可以任意设计，但也有<strong>最经典的Receptive Field的安排方式</strong></p><ol><li>看所有的Channel</li></ol><p><img src="/img/cnn/9.png"></p><p>一般在做图像辨识的时候，可能不会觉得有些Pattern只出现某一个Channel里面，所以会看全部的Channel，所以既然会看全部的Channel</p><p>我们在描述一个Receptive Field的时候，只需要考虑他的宽和高，反正深度一定是考虑全部的Channel，而这个高跟宽合起来叫做<strong>Kernel Size（内核大小）</strong>。</p><p>举例来说，在这个例子里面，Kernel Size就是3×3，一般Kernel Size其实不会设太大，在图像辨识里面，往往做个3×3的Kernel Size就足够了，如果设个7×7和9×9，那已经算是大的Kernel Size。</p><p>如果Kernel Size都是3×3，意味着说我们觉得在做图像辨识的时候，重要的Pattern在3×3这么小的范围内，就可以被侦测出来了，可能会疑惑的是有些Pattern也许很大3×3的范围没办法侦测出来。</p><p>接下来的笔记会回答这个问题，现在需要先明白的是，常见的Receptive Field设定方式，就是Kernel Size 3×3，然后同一个Receptive Field，不会只有一个Neuron去关照它，往往会有一组一排Neuron去守备它，比如说64个或者是128个Neuron去守备一个Receptive Field的范围。<br>2. Stride</p><p>到目前为止，我们讲的都是一个Receptive Field，那不同Receptive Field之间的关系是怎么样呢？</p><p>举例：最左上角的这个Receptive Field，往右移一点就可以然后制造另外一个Receptive Field，这个移动的量叫做<strong>Stride（步）</strong><br><img src="/img/cnn/10.png"><br>在这个例子里面Stride等于2，Stride是一个Hyperparameter，Stride你往往不会设太大，往往设1或2，因为你希望这些Receptive Field之间是有重叠的，因为假设Receptive Field完全没有重叠，那有一个Pattern就正好出现，在两个Receptive Field的交界上面，那就会变成没有任何Neuron去侦测它，那可能就会Miss掉这个Pattern，所以我们希望Receptive Field彼此之间有高度的重叠。</p><p>这边遇到一个问题，<strong>超出了图像的范围</strong>怎么办？</p><p>有人可能会说不要在这边摆Receptive Field，但这样就漏掉了图像的边缘，如果有个Pattern在图像的边缘，你就没有Neuron去关照那些Pattern，所以其实我们实际上中引入的方法是<strong>Padding（补值）</strong>。</p><p>比如今天Receptive Field有一部分，超出图像的范围之外了，那就当做那个里面的值都是0，其实也有别的补值的方法，Padding就是补值的意思，比如说有人会说我补整张图片里面所有Value的平均，或者你说，我把边缘的这些数字拿出来补，有各种不同的Padding的方法。<br>除了横着移动也有垂直方向上的移动，在这边我们一样垂直方向Stride也是设2，所以你有一个Receptive Field在这边，垂直方向移动两格，就有一个Receptive Field在这个地方，你就按照这个方式，扫过整张图片，所以整张图片里面，每个地方都被某一个Receptive Field覆盖，也就是图片里面每一个位置，都有一群Neuron在侦测那个地方有没有出现某些Pattern。</p><p>这是第一个简化Fully Connected Network的方式。<br><strong>有人会疑问，如果我们只看一个感受野确实参数会少，但整个图片都扫一遍，考虑重叠现象，岂不是算起来参数更多？这就要提到第二个简化。</strong></p><h3 id="第二个简化"><a href="#第二个简化" class="headerlink" title="第二个简化"></a>第二个简化</h3><h4 id="观察-1"><a href="#观察-1" class="headerlink" title="观察"></a>观察</h4><p>同样的Pattern，它可能会出现在图片的不同区域里面，比如说鸟嘴这个Pattern，它可能出现在图片的左上角，也可能出现在图片的中间，虽然它们的形状都是一样的都是鸟嘴，但是它们可能出现在图片里面的不同的位置。</p><p>按照我们刚才的讨论，同样的Pattern出现在图片的不同的位置，也不是问题，因为出现在左上角的鸟嘴，它一定落在某一个Receptive Field里面，因为Receptive Field是移动完之后会复盖满整个图片的，所以图片里面没有任何地方不是在某个Neuron的守备范围内。</p><p>假设在Receptive Field里面，有一个Neuron的工作是侦测鸟嘴的话，那鸟嘴就会被侦测出来，所以就算鸟嘴出现在中间也没有关系，鸟嘴一定是在某一个Receptive Field的范围里面，那个Receptive Field一定有一组Neuron在照顾，如果其中有一个Neuron它可以侦测鸟嘴的话，那鸟嘴出现在图片的中间，也会被侦测出来。</p><p>问题是侦测鸟嘴的Neuron，它们做的事情其实是一样的，只是它们守备的范围是不一样，所以只要每个守备范围都放一个侦测鸟嘴的Neuron就不会遗漏了，问题是如果不同的守备范围，都要有一个侦测鸟嘴的Neuron，那参数量不会太多了吗？<br><img src="/img/cnn/11.png"></p><h4 id="简化-2"><a href="#简化-2" class="headerlink" title="简化"></a>简化</h4><p>我们实际的操作是让不同Receptive Field的Neuron共享参数，来削减参数量，也就是做<strong>Parameter Sharing（参数共享）</strong>，<br><img src="/img/cnn/12.png"><br>所谓共享参数就是这两个Neuron它们的weights完全是一样的，图中特别用颜色来标注它们的weights完全是一样的。<br><img src="/img/cnn/13.png"><br>上面这个Neuron的第一个weight，叫做w1，下面这个Neuron的第一个weight也是w1，它们是同一个weight，用红色来表示。</p><p>上面这个Neuron的第二个weight是w2，下面这个Neuron的第二个weight也是w2，它们都用黄色来表示，以此类推……</p><p>总之上面这个Neuron跟下面这个Neuron，它们守备的Receptive Field是不一样的，但是它们的参数是一模一样的。</p><p>两个Neuron的参数一模一样，但是它们照顾的范围是不一样的，输入不一样，结果也不一样。</p><p>上面这个Neuron，我们说它的输入是，下面这个Neuron它的输入是，<br>上面这个Neuron的输出就是，x1×w1 + x2×w2，全部加加加再加Bias，然后透过Activation Function得到输出</p><p>下面这个Neuron虽然也有w1 w2，但w1跟w2是乘以x1’ x2’，所以它的输出不会跟上面这个Neuron一样。</p><p>我们让一些Neuron可以共享参数，至于要怎么共享，完全可以自己决定，而这个是你可以自己决定的事情，这里介绍的是常见的在图像辨识上面的共享的方法。<br><img src="/img/cnn/14.png"><br>刚才提到每一个Receptive Field都有一组Neuron在负责守备，比如说有64个Neuron，所以左上这个Receptive Field有64个Neuron，右下这个Receptive Field也有64个Neuron，我们用一样的颜色，代表这两个Neuron共享一样的参数</p><p>左上边这个Receptive Field的第一个红色Neuron，会跟右下边这个Receptive Field的第一个红色Neuron共享参数，它的第二个橙色Neuron，跟右下边的第二个橙色Neuron共享参数，它的第三个绿色Neuron，跟右下边的第三个绿色Neuron共享参数，所以不同Receptive Field的同一neuron都只有一组参数而已，综合起来每个Receptive Field参数全都一致。</p><p>这些参数有一个名字，叫做<strong>Filter（过滤器）</strong>，所以这两个红色Neuron，它们共享同一组参数，这组参数就叫Filter1，橙色这两个Neuron它们共同一组参数，这组参数就叫Filter2叫Filter3叫Filter4，以此类推。</p><h2 id="Convolutional-Layer"><a href="#Convolutional-Layer" class="headerlink" title="Convolutional Layer"></a>Convolutional Layer</h2><p>目前已经说了两个简化的方法，那我们来整理一下我们学到了什么，<br><img src="/img/cnn/15.png"><br>Fully Connected的Network是弹性最大的，但有时候不需要看整张图片，也许只要看图片的一小部分就可以侦测出重要的Pattern，所以我们有了Receptive Field的概念。<br>当我们强制一个Neuron只能看一张图片里面的一个范围的时候，它的弹性是变小的，如果是Fully Connected的Network，它可以决定看整张图片，还是只看一个范围，就如果它只想看一个范围，就把很多Weight设成0，就只看一个范围，所以加入Receptive Field以后，你的Network的弹性是变小的。</p><p>接下来是参数共享，参数共享进一步限制了Network的弹性，本来在Learning的时候，不同Receptive Field的同种Neuron可以各自有不同的参数，它们可以正好学出一模一样的参数，也可以有不一样的参数，但是加入参数共享以后，就意味着说某一些Neuron参数要一模一样，所以这又更增加了对Neuron的限制，而Receptive Field加上Parameter Sharing，就是<strong>Convolutional Layer（卷积层）</strong>。</p><p>有用到Convolutional Layer的Network，就叫<strong>Convolutional Neural Network（卷积神经网络）</strong>，就是CNN，从这个图上你可以很明显地看出，其实CNN的的Model Bias比较大。</p><p>但Model Bias大，不一定是坏事，因为当Model Bias小，Model的Flexibility很高的时候，它比较容易Overfitting，Fully Connected Layer可以做各式各样的事情，它可以有各式各样的变化，但是它可能没有办法在任何特定的任务上做好。</p><p>而Convolutional Layer，它是专门为图像设计的，刚才讲的Receptive Field参数共享，这些观察都是为图像设计的，所以它在图像上仍然可以做得好，虽然它的Model Bias很大，但这个在图像上不是问题，但是如果它用在图像之外的任务，你就要仔细想想那些任务有没有我们刚才提到的图像任务中用的特性。</p><h2 id="另外一种诠释方式"><a href="#另外一种诠释方式" class="headerlink" title="另外一种诠释方式"></a>另外一种诠释方式</h2><h3 id="Filter"><a href="#Filter" class="headerlink" title="Filter"></a>Filter</h3><p>Convolutional的Layer就是里面有很多的<strong>Filter</strong>，这些Filter的大小是，3×3×Channel的Size，如果今天是彩色图片的话，那就是RGB三个Channel，如果是黑白的图片的话，它的Channel就等于1。<br><img src="/img/cnn/16.png"><br>一个Convolutional的Layer里面就是有一排的Filter，每一个Filter都是一个3×3×Channel的Tensor。</p><p>Filter的作用就是要去图片里面探测某一个Pattern，当然这些Pattern，要在3×3×Channel那么小的范围内才能够被这些Filter抓出来</p><p>那这些Filter，怎么去图片里面抓Pattern的呢，我们现在举一个实际的例子<br><img src="/img/cnn/17.png"><br>我们假设Channel是1，也就是说我们图片是黑白的图片</p><p>那我们假设这些Filter的参数是已知的，Filter就是一个一个的Tensor，这个Tensor里面的数值我们都已经知道了，（实际上这些Tensor里面的数值，其实就是Model里面的Parameter，这些Filter里面的数值其实是未知的，它是要通过gradient decent去找出来的）</p><p>不过我们现在已经假设这些Filter里面的数值已经找出来了，我们来看看这些Filter，是怎么跟一张图片进行运作，怎么去图片上面把Pattern侦测出来的。<br><img src="/img/cnn/18.png"><br>这是一个6×6的大小的图片，这些Filter的做法就是，先把Filter放在图片的左上角，然后把Filter里面所有的值，跟左上角这个范围内的9个值做相乘，也就是把这个Filter里面的9个值，跟这个范围里面的9个值呢，做<strong>Inner Product（内积）</strong>，结果是3。（三通道无非体积对应内积，原理一样）</p><p><strong>注意！在此处卷积运算中，内积不要理解为线性代数中矩阵的乘法，而是filter跟图片对应位置的数值直接相乘，所有的都乘完以后再相加</strong><br><img src="/img/cnn/1.gif"><br>Filter本来放在左上角，接下来就往右移一点，那这个移动的距离叫做<strong>Stride</strong>。</p><p>在这里，我们Stride设为1，那往右移一点，然后再把这个Filter，跟这个范围里面的数值，算Inner Product算出来是-1，以此类推，再往右移一点再算一下，然后这边全部扫完以后，就往下移一点再算一下，一直到把这个Filter放在右下角。<br><img src="/img/cnn/19.png"></p><h3 id="Feature-Map"><a href="#Feature-Map" class="headerlink" title="Feature Map"></a>Feature Map</h3><p>这个Filter怎么说它在侦测Pattern呢？<br><img src="/img/cnn/20.png"><br>这个Filter里面，它<strong>对角线的地方都是1</strong>，所以它看到Image里面也出现连三个1的时候，它的数值会最大。<br>所以你会发现左上角和左下角的地方的值最大，就告诉我们说这个图片里面左上角和左下角有出现这个三个1连在一起的Pattern，这个是第一个Filter</p><p>接下来我们把每一个Filter，都做重复的Process，比如说这边有第二个Filter，我们就把第二个Filter，先从左上角开始扫起，得到一个数值，往右移一点再得到一个数值，再往右移一点再得到一个数值，反复同样的Process，反复同样的操作，直到把整张图片都扫完，我们又得到另外一群数值<br><img src="/img/cnn/21.png"></p><p>每一个Filter，都会给我们一群数字，红色的Filter给我们一群数字，蓝色的Filter给我们一群数字，如果我们有64个Filter，我们就得到64组的数字，而每一组数字都是**Feature Map（特征图）**的一个通道。</p><p>所以当我们把一张图片，通过一个Convolutional Layer，里面有64个Filter，产生出来64组数字，每一组在这个例子里面是4×4，他们叠在一起产生了<strong>Feature Map</strong>，你可以看成是另外一张新的图片，只是这个图片的Channel它有64个，这并不是RGB这个图片的Channel，在这里每一个Channel就对应到一个Filter，本来一张图片它三个Channel，通过一个Convolution，它变成一张新的图片有64个Channel。</p><p>Convolutional Layer可以叠很多层的，刚才是叠了第一层，那如果叠第二层的话，第二层的Convolution里面，也有一堆Filter，那每一个Filter呢，它的大小我们这边也设3×3，那它的高度必须设为64。</p><p>Filter的这个高度就是它要处理的图像的Channel，第一层的Convolution假设输入的图像是黑白的Channel是1，那我们的Filter的高度就是1，输入的图像如果是彩色的Channel是3，那Filter的高度就是3，那在第二层里面，我们也会得到一张图像，对第二个Convolutional Layer来说，它的输入也是一张图片，这个图片的Channel是64。</p><p>64是前一个Convolutional Layer的Filter数目，前一个Convolutional LayerFilter数目64，那输出以后就是64个Channel。</p><p>如果我们的Filter的大小一直设3×3，会不会让我们的Network，没有办法看比较大范围的Pattern呢？</p><p>其实不会的，如果我们在第二层Convolutional Layer，我们的Filter的大小一样设3×3的话，当我们看最左上角这个数值的时候，最左上角这个数值在原始图像上，其实是对应到这个范围，<br><img src="/img/cnn/22.png"><br>右下角的数值在原始图像上，其实是对应到这个范围，<br><img src="/img/cnn/23.png"><br>综合起来，我们在原来的图像上，其实是考虑了一个5×5的范围，<br><img src="/img/cnn/24.png"><br>所以虽然我们的Filter只有3×3，但它在图像上考虑的范围是比较大的5×5，而今天你的Network叠得越深，同样是3×3的大小的Filter，它看的范围就会越来越大，所以Network够深，就不用怕侦测不到比较大的Pattern。</p><h2 id="两个版本的总结"><a href="#两个版本的总结" class="headerlink" title="两个版本的总结"></a>两个版本的总结</h2><p>这两个版本的故事，是一模一样的<br><img src="/img/cnn/25.png"><br>在第一个版本的故事里面，说到了Neuron会共享参数，这些共享的参数，就是第二个版本的故事里面的Filter</p><p>这个Filter里面有3×3×3个数字，这边特别用颜色把这些数字圈起来，意思是说这个Weight就是这个数字。</p><p>以此类推，这边把Bias去掉了，Neuron这个是有Bias的，这个Filter其实也有Bias，只是刚才没有提到，在实际操作中，CNN的这些Filter，其实都有Bias的数值。</p><p>把Filter扫过一张图片这件事其实就是Convolution。<br><img src="/img/cnn/26.png"></p><h2 id="Pooling"><a href="#Pooling" class="headerlink" title="Pooling"></a>Pooling</h2><p>Convolutional Layer，在做图像辨识时还有第三个常用的东西叫做<strong>Pooling（池化）</strong>，<br><img src="/img/cnn/27.png"><br>Pooling来自于另外一个观察，我们把一张比较大的图片做<strong>Subsampling（下采样）</strong>，举例来说你把偶数的Column都拿掉，奇数的Row都拿掉，图片变成为原来的1&#x2F;4，但是不会影响里面是什么东西，把一张大的图片缩小，这是一只鸟，这张小的图片看起来还是一只鸟</p><p>那所以有了Pooling这样的设计，Pooling本身没有参数，它里面没有Weight，它没有要Learn的东西，所以有人会说Pooling比较像是一个Activation Function，比较像是Sigmoid，ReLU那些，因为它里面是没有要Learn的东西的，它就是一个Operator，行为都是固定好的，没有要根据Data学任何东西。</p><p>Pooling也有很多不同的版本，我们这边讲的是<strong>Max Pooling</strong><br><img src="/img/cnn/28.png"><br>每一个Filter都产生一些数字，要做Pooling的时候，我们就把这些数字几个几个一组，比如说在这个例子里面就是2×2个一组，每一组里面选一个代表，在Max Pooling里面，我们选的代表就是最大的那一个<br><img src="/img/cnn/29.png"><br>你不一定要选最大的那一个，这个自己可以决定的，Max Pooling这一个方法是选最大的那一个，但是也有average Pooling，还有选几何平均的，所以有各式各样的Pooling的方法<br>也不一定要2×2个一组，这个也是你自己决定的，你要3×3 4×4也可以。</p><p>所以我们做完Convolution以后，往往后面还会搭配Pooling，Pooling做的事情就是把图片变小，做完Convolution以后我们会得到一张图片，这一张图片里面有很多的Channel，那做完Pooling以后，我们就是把这张图片的Channel不变，本来64个Channel还是64个Channel，但是我们会把图片变得小一些<br><img src="/img/cnn/30.png"></p><p>在刚才的例子里面，本来4×4的图片，如果我们把这个Output的数值2×2个一组，那4×4的图片就会变成2×2的图片，这个就是Pooling所做的事情</p><p>在实际操作中，往往是Convolution跟Pooling交替使用，就是可能做几次Convolution，做一次Pooling，比如两次Convolution一次Pooling。</p><p>不过预见的是Pooling对于Performance，还是可能会带来一点伤害的，因为假设你今天要侦测的是非常微细的东西，那做Subsampling，Performance可能会差一点</p><p>所以近年来你会发现，很多Network的设计，往往开始把Pooling丢掉，他会做Full Convolution的Neural Network，整个Network里面统统都是Convolution，完全不用Pooling</p><p>那是因为近年来运算能力越来越强，Pooling最主要的理由是为了减少运算量，做Subsampling，把图像变少减少运算量，如果你今天运算资源，足够支撑不做Pooling的话，很多Network的架构的设计往往就不做Pooling，，Convolution从头到尾，然后看看做不做得起来，看看能不能做得更好。</p><h2 id="全连接"><a href="#全连接" class="headerlink" title="全连接"></a>全连接</h2><p>最后我们要做<strong>Flatten</strong>，Flatten的意思就是把这个图像里面本来排成矩阵的样子的数值拉直变成一个向量，再把这个向量放进进Fully Connected的Layer里面，最终你可能还要过个Softmax，然后得到图像辨识的结果，这就是一个经典的图像辨识的Network。如下图里面有Convolution，有Pooling有Flatten，最后再通过几个Fully Connected的Layer和Softmax，得到图像辨识的结果。</p><p><img src="/img/cnn/31.png"></p><blockquote><p>Flatten层用来将输入“压平”，即把多维的输入一维化，常用在从卷积层到全连接层的过渡。Flatten不影响batch的大小,有了全卷积网络之后 flatten已经不是必要操作。</p></blockquote><p><strong>为什么要放到全连接层呢？</strong></p><p><strong>前面的卷积本质还是在提取特征，而对特征图进行判断是全连接的事情。</strong></p><p>全连接层（fully connected layers，FC）<strong>在整个卷积神经网络中起到“分类器”的作用</strong>。如果说卷积层、池化层和激活函数层等操作是将原始数据映射到隐层特征空间的话，全连接层则起到将学到的“分布式特征表示”映射到样本标记空间的作用。在实际使用中，全连接层可由卷积操作实现：对前层是全连接的全连接层可以转化为卷积核为1x1的卷积；而<strong>前层是卷积层的全连接层可以转化为卷积核为hxw的全局卷积，h和w分别为前层卷积结果的高和宽（flatten的具体操作）</strong>。</p><p>但是大部分是两层以上，弹性更大，可以拟合更复杂的非线性函数。</p><p><strong>目前由于全连接层参数冗余（仅全连接层参数就可占整个网络参数80%左右），一些一些性能优异的网络模型如ResNet和GoogLeNet等均用全局平均池化（global average pooling，GAP）取代FC来融合学到的深度特征，最后仍用softmax等损失函数作为网络目标函数来指导学习过程。需要指出的是，用GAP替代FC的网络通常有较好的预测性能。</strong></p>]]></content>
    
    
    <categories>
      
      <category>深度学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>CNN</tag>
      
      <tag>卷积神经网络</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>深度学习-5-Classification</title>
    <link href="/2021/07/28/classfication/"/>
    <url>/2021/07/28/classfication/</url>
    
    <content type="html"><![CDATA[<h2 id="Classification-as-Regression？"><a href="#Classification-as-Regression？" class="headerlink" title="Classification as Regression？"></a>Classification as Regression？</h2><blockquote><p>创作声明：主要为李宏毅老师的听课笔记。<br>正确的答案有加Hat，Model的输出没有加Hat</p></blockquote><p>我们之前的笔记提到Regression就是输入一个向量，然后输出一个数值，那我们先思考一个问题，能不能用回归问题的解法去求解分类问题呢？</p><p><img src="/img/dlclassfication/1.png"></p><p>输入一个东西以后，我们的输出仍然是一个数值，它叫做y，然后y我们要让它跟正确答案，那个Class越接近越好，y是一个数字，我们怎么让它跟Class越接近越好呢，我们必须把Class也变成数字<br>举例来说Class1就是编号1，Class2就是编号2，Class3就是编号3，接下来呢我们要做的事情，就是希望y可以跟Class的编号，越接近越好。<br>但是这会是一个好方法吗，如果你仔细想想的话，这个方法也许在某些状况下，是会有瑕疵的。<br>因为当你用数字表示类别的时候，也就意味着数字上的联系也会代入到类别上的关系，</p><p>比如1和2的距离比1和3的距离近，但是类别上类别1和2就更相似吗？</p><p>显然不是。</p><h2 id="Class-as-one-hot-vector？"><a href="#Class-as-one-hot-vector？" class="headerlink" title="Class as one-hot vector？"></a>Class as one-hot vector？</h2><p>比较常见的做法是把Class用<strong>One-hot vector</strong>来表示，</p><p><img src="/img/dlclassfication/2.png"></p><p>用One-hot vector来表示，就没有说Class1跟Class2比较接近，Class1跟Class3比较远这样的问题，如果用这个One-hot vector算距离的话，Class之间两两它们的距离都是一样。</p><p>如果我们今天的目标ŷ是一个向量，比如ŷ是有三个element的向量，那network也应该要Output三个维度。</p><p>我们过去做的都是Regression的问题，所以只Output一个数字，其实从一个数值改到三个数值，它是没有什么不同的，你可以Output一个数值，你就可以Output三个数值，所以把本来Output一个数值的方法，重复三次</p><p><img src="/img/dlclassfication/3.png"></p><h2 id="Classification-with-softmax"><a href="#Classification-with-softmax" class="headerlink" title="Classification with softmax"></a>Classification with softmax</h2><p>在做Classification的时候，我们往往会把y再通过一个叫做Soft-max的function得到$y^\prime$，然后我们才去计算$y^\prime$跟ŷ之间的距离。</p><p>为什么要加上Soft-max呢，一个比较简单的解释，一个骗小孩的解释就是，这个ŷ 它里面的值，都是0跟1，它是One-hot vector，所以里面的值只有0跟1，但是y里面有任何值。</p><p>既然我们的目标只有0跟1，但是y有任何值，我们就先把它Normalize到0到1之间，这样才好跟label的计算相似度，这是一个比较简单的讲法。</p><p>如果你真的想要知道，为什么要用Soft-max的话，你可以参考其他资料，如果你不想知道的话，你就记得这个Soft-max要做的事情，就是把本来y里面可以放任何值，改成挪到0到1之间。</p><p><strong>Softmax</strong><br>运行模式：</p><p><img src="/img/dlclassfication/4.png"></p><p>我们会先把所有的y取一个exponential，就算是负数，取exponential以后也变成正的，然后你再对它做Normalize，除掉所有y的exponential值的和，然后你就得到$y^\prime$。</p><p>或者是用图示化的方法是这个样子：</p><p><img src="/img/dlclassfication/5.png"></p><p>y₁取exp y₂取exp y₃取exp，把它全部加起来，得到一个Summation，接下来再把exp y₁’除掉Summation，exp y₂’除掉Summation，exp y₃’除掉Summation，就得到y₁’ y₂’ y₃’<br>有了这个式子以后，你就会发现</p><ul><li>y₁’ y₂’ y₃’，它们都是介于0到1之间</li><li>y₁’ y₂’ y₃’，它们的和是1</li></ul><p>举一个例子，本来y₁等于3，y₂等于1，y₃等于负3，exp3就是20，exp1就是2.7，exp -3就是0.05，做完Normalization以后，这边就变成0.88，0.12和0。<br>所以这个Soft-max它要做的事情，除了Normalized，让y₁’ y₂’ y₃’，变成0到1之间，还有和为1以外，它还有一个附带的效果是，它会让大的值跟小的值的<strong>相对差距</strong>更大，Soft-max的输入，往往就叫它logit。</p><h2 id="Loss-of-Classification"><a href="#Loss-of-Classification" class="headerlink" title="Loss of Classification"></a>Loss of Classification</h2><p><img src="/img/dlclassfication/6.png"><br>我们把x丢到一个Network里面产生y以后，我们会通过soft-max得到y’，再去计算y’跟ŷ之间的距离，这个写作е。</p><p>计算y’跟ŷ之间的距离不只一种做法，举例来说，如果我可以让这个距离是Mean Square Error，但是更常用的做法，叫做<strong>Cross-entropy（交叉熵）</strong></p><p><img src="/img/dlclassfication/7.png"></p><p>Cross-entropy是覆盖每一个值，把ŷ的第i位拿出来，乘上y’的第i位取Natural log,然后再全部加起来。</p><p>当ŷ跟y’一模一样的时候，MSE会是最小的，Cross-entropy也会是最小的。</p><p>Make Minimize Cross-entropy其实就是maximize likelihood。</p><p>实际上在pytorch里面，Cross-entropy跟Soft-max，他们是被绑在一起的，他们是一个Set，你只要Copy Cross-entropy，里面就自动内建了Soft-max。</p><p>我们从optimization的角度，来说明相较于Mean Square Error，Cross-entropy为什么被更常用在分类上（存在数学证明，有兴趣可以自行查阅）</p><p>对于一个3个Class的分类<br>Network先输出y₁y₂y₃，在通过soft-max以后，产生y₁’ y₂’跟y₃’<br>那接下来假设我们的正确答案是(1,0,0)，我们要去计算(1,0,0)这个向量，跟y₁’ y₂’跟y₃’他们之间的距离，这个距离我们用е来表示，е可以是Mean square error，也可以是Cross-entropy.<br>我们现在假设y₁是从-10到10，y₂是从-10到10，y₃我们就固定设成-1000。</p><p>因为y₃设很小，所以过soft-max以后y₃’就非常趋近于0，它跟正确答案非常接近，且它对我们的结果影响很少。<br>总之我们y₃设一个定值，我们只看y₁y₂有变化的时候，对我们的loss有什么样的影响。</p><p>进一步我们的目的是看损失函数设定为Mean Square Error，跟Cross-entropy的时候，算出来的Error surface会有什么不一样的地方，如下图：</p><p><img src="/img/dlclassfication/8.png"></p><p>红色代表Loss大，蓝色代表Loss小，如果今天y₁很大y₂很小，就代表y₁’会很接近1，y₂’会很接近0，所以不管是对Mean Square Error，或是Cross-entropy而言，y₁大y₂小的时候Loss都是小的。</p><p>如果y₁小y₂大的话，这边y₁’就是0 y₂’就是1，所以这个时候Loss会比较大。</p><p>所以这两个图都是左上角Loss大，右下角Loss小，所以我们就期待最后在Training的时候，我们的参数可以走到右下角的地方。</p><p><img src="/img/dlclassfication/9.png"></p><p>假设我们开始的地方都是左上角，如果我们选择Cross-Entropy，左上角有斜率，所以有办法通过gradient，一路往右下的地方走，如果选Mean square error的话就卡住了，Mean square error在这种Loss很大的地方，它是非常平坦的，它的gradient是非常小趋近于0的，如果你初始的时候在这个地方，离你的目标非常远，那它gradient又很小，你就会没有办法用gradient descent顺利的走到右下角的地方去。</p><p>所以如果在做classification，你选Mean square error的时候，你有非常大的可能性会train不起来，当然这个是在你没有好的optimizer的情况下，今天如果你用Adam，这个地方gradient很小，那gradient很小之后，它learning rate之后会自动帮你调大，也许你还是有机会走到右下角，不过这会让你的training，比较困难一点，让training的起步比较慢一点。</p><p>所以这也是一个很好的例子，告诉我们说，就算是Loss function的定义，都可能影响Training是不是容易这件事情，之前说BN把error surface炸平，这边也是一个好的例子告诉我们你可以改Loss function，同样可以改变optimization的难度。</p><h2 id="本文尚待回答的问题"><a href="#本文尚待回答的问题" class="headerlink" title="本文尚待回答的问题"></a>本文尚待回答的问题</h2><ul><li>softmax原理</li></ul>]]></content>
    
    
    <categories>
      
      <category>深度学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>Classification</tag>
      
      <tag>Softmax</tag>
      
      <tag>Cross-entropy</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>深度学习-4-Batch Normalization</title>
    <link href="/2021/07/28/batchnormlization/"/>
    <url>/2021/07/28/batchnormlization/</url>
    
    <content type="html"><![CDATA[<blockquote><p>创作声明：主要为李宏毅老师的听课笔记，附视频链接：<a href="https://www.bilibili.com/video/BV1Wv411h7kN?p=15">https://www.bilibili.com/video/BV1Wv411h7kN?p=15</a></p></blockquote><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>回答在optimization那篇笔记的问题，<strong>Batch Normalization（批归一化）</strong> 就是其中一个把山推平的想法。</p><p>假设有两个参数，它们对loss的斜率差别非常大，在w1这个方向上你的斜率变化很小，在w2这个方向上的斜率变化很大。</p><p><img src="/img/dlbn/1.png"></p><p>如果是固定的learning rate，可能很难得到好的结果，所以才需要adaptive learning rate、Adam等等比较进阶的optimization的方法，才能够得到好的结果。</p><p>现在从另外一个方向想，直接把难做的error surface改掉，看能不能够改得好做一点。</p><p>在做这件事之前，我们需要思考这种状况是怎么产生的？</p><p>假设现在有一个linear model，没有activation function，它的输入是x1跟x2 ，对应的参数是w1和w2。</p><p>当我们对w1有一个小小的改变，比如说加上delta w1的时候，L也会有一个改变，这是通过改变w1的时候改变了y，y改变的时候就改变了e，然后接下来就改变了L。<br><img src="/img/dlbn/2.png"><br>那什么时候w1的改变会对L的影响很小呢，也就是它在error surface上的斜率会很小呢？</p><p>当你的<strong>input很小</strong>的时候，假设x1的值在不同的training example里面它的值都很小，因为x1是直接乘上w1，如果x1的值很小，w1有变化的时候，它对y的影响也是小的，对e的影响也是小的，它对L的影响就会是小的。</p><p>反之，如果是x2，<br><img src="/img/dlbn/3.png"><br>假设x2值偏大，它对L的影响也会是大的。</p><p>所以在这个linear model里面，当我们输入的feature的每一个dimension的值，它的scale差距很大的时候，我们就可能产生像这样子的error surface，就可能产生不同方向斜率非常不同，坡度非常不同的error surface。</p><p>所以我们有没有可能给feature里面不同的dimension，让它有同样的数值的范围？<br><img src="/img/dlbn/4.png"><br>如果我们可以给不同的dimension同样的数值范围的话，那我们可能就可以制造比较好的error surface，让training变得比较容易一点。</p><p>其实有很多不同的方法，这些不同的方法，往往就合起来统称为<strong>Feature Normalization（特征归一化）</strong></p><h2 id="Feature-Normalization"><a href="#Feature-Normalization" class="headerlink" title="Feature Normalization"></a>Feature Normalization</h2><p>我们探讨Feature Normalization的一种可能性，它并不是Feature Normalization的全部。<br><img src="/img/dlbn/5.png"><br>假设$x^1$到$x^R$，是我们所有的训练资料的<strong>feature vector（特征向量）</strong>，<br>那我们把不同数据的feature vector，同一个dimension里面的数值取出来，然后去计算某一个dimension的<strong>mean（平均）</strong>，它的mean就是$m_i$，我们计算第i个dimension的，<strong>standard deviation（标准差）</strong>，我们用$\sigma_i$表示它,接下来我们就可以做一种normalization，那这种normalization叫做<strong>standardization(标准化）</strong>，这边为了行文方便，暂时先都统称normalization。</p><p>我们是把某一个数值$x_i^r$，减掉这一个dimension算出来的mean，再除掉这个dimension，算出来的standard deviation，得到新的数值叫做$\widetilde{x}_i^r$<br>然后得到新的数值以后，再把新的数值代替原先的数值。</p><p>做完normalize以后有什么好处呢？</p><p><img src="/img/dlbn/6.png"></p><ul><li>做完normalize以后，这个dimension上面的数值平均是0，然后它的**variance（方差）**就会是1，所以这一排数值的分布就都会在0上下。</li><li>对每一个dimension都做一样的normalization，就会发现所有feature不同dimension的数值都在0上下，那你可能就可以制造一个比较好的error surface。</li></ul><p>所以Feature Normalization往往对你的training有帮助，它可以让你在做gradient descent的时候，这个gradient descent，它的Loss收敛更快一点，训练更顺利一点。</p><h2 id="Considering-Deep-Learning"><a href="#Considering-Deep-Learning" class="headerlink" title="Considering Deep Learning"></a>Considering Deep Learning</h2><p><img src="/img/dlbn/7.png"></p><p>深度学习的神经网络会有很多层，如上图：</p><p>以$\widetilde{x}^1$为例，虽然它已经做 normalize 了,但是通过w1以后它就没有做 normalize，如果$\widetilde{x}^1$ 通过 w1得到是$z^1$，而 $z^1$不同的 dimension 间，它的数值的分布仍然有很大的差异的话，那我们要 train 第二层的参数，会不会也有困难呢？</p><p>对w2来说，这边的a或这边的z其实也是一种feature，我们应该要对这些feature也做normalization。</p><p>如果选择的是Sigmoid，那比较推荐对z做Feature Normalization，因为Sigmoid是一个s的形状，那它在0附近斜率比较大，所以如果你对z做Feature Normalization，把所有的值都挪到0附近，那你到时候算gradient的时候，算出来的值会比较大。</p><p>因为不一定是用sigmoid，所以不一定要把Feature Normalization放在z这个地方，如果是选别的，也许你选a也会有好的结果，在实际操作上，可能没有太大的差别、</p><p>这边对z做一下Feature Normalization。</p><p><img src="/img/dlbn/8.png"></p><p>方法也是一样的，但是现在需要注意的是：</p><p>本来，如果我们没有做Feature Normalization的时候，改变$z^1$的值，只会改变这边$a^1$值，但是现在当你改变$z^1$的值的时候，$\mu$跟$\sigma$也会跟着改变，接着$z^2$的值$a^2$的值，$z^3$的值$a^3$的值，也会跟着改变。</p><p>所以之前，$\widetilde{x}^1$$\widetilde{x}^2$$\widetilde{x}^3$，它是独立分开处理的，但是在做Feature Normalization以后，这三个example，它们变得彼此关联了。</p><p>所以当你做Feature Normalization的时候，要把这一整个流程（就是收集一堆feature，把这堆feature算出$\mu$和$\sigma$这件事情）当做是network的一部分。</p><p>也就是说之前的network，每次都只有一个input，得到一个output，现在你有一个比较大的network，这个大的network，它是头一堆input，用这堆input在这个network里面，要算出$\mu$和$\sigma$，然后接下来产生一堆output。</p><p><strong>那这边就会有一个问题</strong>，因为你的训练资料里面的data非常多，现在一个data set，benchmark corpus都上百万数据，GPU的内存没有办法，把所有data set的data都load进去。</p><p>在实际操作的时候，不会network考虑整个training data里面的所有example，只会考虑一个batch里面的example，举例来说，batch size设64，那network就是把64个data读进去，算这64个data的，算这64个data的，对这64个data都去做normalization。</p><p>因为我们在实际操作的时候我们只对一个batch里面的data，做normalization，所以这招叫做Batch Normalization。</p><p>这种Batch Normalization有一个问题就是，<strong>你一定要有一个够大的batch</strong>，你才算得出$\mu$和$\sigma$，假设batch size设1，那你就没有什么$\mu$和$\sigma$可以算。</p><p>所以这个Batch Normalization，是适用于batch size比较大的时候，因为batch size如果比较大，也许这个batch size里面的data就足以表示整个data set的分布，这个时候就可以，把这个本来要对整个data set，做Feature Normalization这件事情，改成只在一个batch上做Feature Normalization，作为approximation，</p><p>在做Batch Normalization的时候，往往还会有这样的设计：</p><p>算出$\widetilde{z}$以后，把这个$\widetilde{z}$与另外一个向量$\gamma$做<strong>element wise multiplication（直译为元素智能乘积，也就是两两做相乘）</strong>。<br>再加上向量$\beta$，得到$\hat{z}$，$\gamma$和$\beta$是network的参数，也是另外被learn出来的，<br><img src="/img/dlbn/9.png"></p><p>那为什么要加上$\gamma$和$\beta$的设计呢？</p><p>如果我们做normalization以后，那这边的$\widetilde{z}$的平均就一定是0，如果平均是0的话，就是给那network一些限制，那也许这个限制会带来什么负面的影响，所以我们利用$\gamma$和$\beta$让network自己去learn这个$\gamma$和$\beta$，来调整一下输出的分布。</p><p>可能有疑问是刚才说做Batch Normalization就是，为了要让每一个不同的dimension的range都一样，现在有$\gamma$和$\beta$的影响，会不会造成不同的dimension的range又不一样了？</p><p>有可能，但是你实际上在训练的时候，这个$\gamma$和$\beta$的初始的element分别是全是1和全是0，one vector和zero vector。</p><p>所以network在一开始训练的时候，每一个dimension的分布，是比较接近的，而当已经训练够长的一段时间，也许已经找到一个比较好的error surface，那再把$\gamma$和$\beta$调整好（来自实验经验的考虑确实会效果更好）。</p><h2 id="Batch-Normalization-testing"><a href="#Batch-Normalization-testing" class="headerlink" title="Batch Normalization  testing"></a>Batch Normalization  testing</h2><p>在这里，testing有时候又叫inference。</p><p><strong>为什么testing也要 normalization呢，不是直接test就好了嘛？</strong><br>个人认为：神经网络在训练时候数据进行了归一化处理，而且引入相关学习的参数，那么本身归一化就是神经网络的一部分，他的参数也被学习到了。在testing的时候，自然也需要归一化。</p><p>在做Batch Normalization的时候，一个$\widetilde{z}$，也就是一个normalization过的feature进来，然后得到一个z，z要减掉$\mu$然后除$\sigma$，而$\mu$和$\sigma$是用一整个batch的资料算出来的。</p><p>如果今天是在做作业，一次有所有的testing的资料，确实也可以在testing的资料上面，制造一个一个batch。</p><p>但实际应用时可能是个在线问题，是一个真正的线上的app，要求尽快出结果，此时比如说你的batch size设64，我一定要等64个数据都进来，我才做运算吗，这显然是不行的，但是没有成batch，又没办法去算$\mu$和$\sigma$。</p><p><img src="/img/dlbn/10.png"></p><p>实际上的解法是这个样子的，如果看那个PyTorch的话，Batch Normalization在testing的时候，你并不需要做什么特别的处理，PyTorch帮你处理好了，处理的方法如下:</p><p>在training的时候，如果有做Batch Normalization的话，那么你每一个batch计算出来的$\mu$和$\sigma$都会拿出来算<strong>moving average（流动平均数）</strong>。</p><p>取第一个batch出来的时候，你就会算一个$\mu^1$，取第二个batch出来的时候，你就会算一个$\mu^1$，一直到取第t个batch出来的时候，你会算一个$\mu^t$，你会把你现在算出来的的一个平均值，叫做$\bar\mu$，乘上一个常数，这也是一个hyper parameter，也是需要调的。</p><p>在PyTorch里面，它设为0.1，然后加上(1-p)*$\mu^t$，然后来更新$\mu$的平均值，然后最后在testing的时候，你就不用算batch里面的$\mu$和$\sigma$。</p><p>因为testing的时候，在真正的application上，也没有batch这个东西，我们是直接拿$\bar\mu$和$\bar\sigma$来取代这边的$\mu$和$\sigma$，以上就是Batch Normalization在testing的时候的处理方法。</p><h2 id="Batch-Normalization的实际效果和原因猜想"><a href="#Batch-Normalization的实际效果和原因猜想" class="headerlink" title="Batch Normalization的实际效果和原因猜想"></a>Batch Normalization的实际效果和原因猜想</h2><h3 id="实际效果"><a href="#实际效果" class="headerlink" title="实际效果"></a>实际效果</h3><p>以下是从介绍Batch Normalization原始的paper上面截出来的一个实验结果，在原始的文件上还讲了很多其他的东西，举例来说有Batch Normalization用在CNN上要怎么用，更多的内容需要自己去读一下原始的文献：</p><p><img src="/img/dlbn/11.png"></p><p>横轴代表的是训练的过程，纵轴代表的是validation set上面的accuracy。</p><p>黑色的虚线是没有做Batch Normalization的结果。</p><p>然后如果有做Batch Normalization，会得到红色的这一条虚线，红色这一条虚线，它训练的速度显然比黑色的虚线还要快很多，虽然最后收敛的结果只要给它足够的训练的时间，可能都跑到差不多的accuracy，但是红色这一条虚线，可以在比较短的时间内，就跑到一样的accuracy。这个蓝色的菱形，代表说这几个点的accuracy是一样的。</p><p>粉红色的线是sigmoid function，实际上我们一般都会选择ReLu，而不是用sigmoid function，因为sigmoid function它的training是比较困难的，但是这边想要强调的是，就算是sigmoid function它的training是比较困难的，加上Batch Normalization，还可以train的起来，图中没有sigmoid不做Batch Normalization的结果，因为在这个实验上，作者有说，sigmoid不加Batch Normalization，根本都train不起来。</p><p>蓝色的实线跟这个蓝色的虚线，是把learning rate设比较大一点，乘5和乘30，因为如果你做Batch Normalization的话，那error surface会比较平滑比较容易训练，所以可以把learning rate设大一点，这边不好解释的地方是不知道为什么learning rate设30倍的时候比5倍差，作者也没有解释，大家也都知道做deep learning就是有时候会产生不知道怎么解释的现象就是了，作者就是照实把他做出来的实验结果呈现在这个图上面。</p><h3 id="原因猜想"><a href="#原因猜想" class="headerlink" title="原因猜想"></a>原因猜想</h3><p>接下来的问题就是，Batch Normalization为什么会有帮助呢？</p><p>在原始的Batch Normalization那篇paper里面，作者提出来一个概念，叫做<strong>internal covariate shift（内部协变量偏移）</strong>，covariate shift（训练集和预测集样本分布不一致的问题就叫做“covariate shift”现象）这个词汇是原来就有的，internal covariate shift是Batch Normalization的作者自己发明的。</p><p>他认为说今天在train network的时候，会有以下这个问题：</p><p><img src="/img/dlbn/12.png"></p><p>network有很多层，<br>x通过第一层以后得到a，<br>a通过第二层以后得到b，<br>计算出gradient以后，把A update成A′，把B这一层的参数update成B′<br>但是作者认为说，我们在计算B update到B′的gradient的时候，这个时候前一层的output是小a，那当前一层从A变成A′的时候，它的output就从a变成a′。</p><p>但是我们计算gradient的时候，我们是根据这个a算出来的啊，所以这个update的方向，也许它适合用在a上，但不适合用在a′上面，如果说Batch Normalization的话，我们会让a跟a′的分布比较接近，也许这样就会对训练有帮助。</p><p>但是有一篇paper叫做How Does Batch Normalization Help Optimization，就反驳了internal covariate shift的这一个观点<br>在这篇paper里面，他告诉你 internal covariate shift，首先它不一定是training network的时候的一个问题，然后Batch Normalization，它会比较好，也不见得是因为它解决了internal covariate shift。<br>在这篇paper里面呢，他做了很多的实验，比如说他比较了训练的时候a的分布的变化发现不管有没有做Batch Normalization，它的变化都不大。</p><p>然后他又说，就算是变化很大，对training也没有太大的伤害，接着他说，不管你是根据a算出来的gradient，还是根据a′算出来的gradient，方向都差不多。</p><p>所以他告诉你说，internal covariate shift，可能不是training network的一个主要问题，它可能也不是Batch Normalization会好的一个的关键，那有关更多的实验，读者可以自己参见这篇文章。</p><p>那为什么Batch Normalization会比较好呢，那在这篇How Does Batch Normalization Help Optimization这篇论文里面，他从实验上，也从理论上，至少支持了Batch Normalization，可以改变error surface，让error surface比较不崎岖这个观点。</p><p><img src="/img/dlbn/13.png"></p><p>所以这个观点是有理论的支持，也有实验的佐证的，在这篇文章里面，他还说如果我们要让network，这个error surface变得比较不崎岖，其实不见得要做Batch Normalization，感觉有很多其他的方法，都可以让error surface变得不崎岖，那他就试了一些其他的方法，发现说，跟Batch Normalization performance也差不多，甚至还稍微好一点，所以他就感叹<br>说，positive impact of batchnorm on training，可能是somewhat，serendipitous，什么是serendipitous呢，这个字眼可能可以翻译成偶然的，但偶然并没有完全表达这个词汇的意思，这个词汇的意思是说，你发现了一个什么意料之外的东西。</p><p>举例来说，盘尼西林就是意料之外的发现，盘尼西林的由来就是，弗莱明本来想要培养一些葡萄球菌，然后但是因为他实验没有做好，他的葡萄球菌被感染了，有一些霉菌掉到他的培养皿里面，然后他发现那些霉菌会杀死葡萄球菌，所以他就发现了盘尼西林，所以这是一种偶然的发现。</p><p>那这篇文章的作者也觉得，Batch Normalization也像是盘尼西林一样，是一种偶然的发现，但无论如何，它是一个有用的方法。</p><h2 id="To-learn-more"><a href="#To-learn-more" class="headerlink" title="To learn more"></a>To learn more</h2><p>其实Batch Normalization，不是唯一的normalization，normalization的方法有很多，那这边列举几个比较知名的方法：</p><p>Batch Renormalization<br><a href="https://arxiv.org/abs/1702.03275">https://arxiv.org/abs/1702.03275</a></p><p>Layer Normalization<br><a href="https://arxiv.org/abs/1607.06450">https://arxiv.org/abs/1607.06450</a></p><p>Instance Normalization<br><a href="https://arxiv.org/abs/1607.08022">https://arxiv.org/abs/1607.08022</a></p><p>Group Normalization<br><a href="https://arxiv.org/abs/1803.08494">https://arxiv.org/abs/1803.08494</a></p><p>Weight Normalization<br><a href="https://arxiv.org/abs/1602.07868">https://arxiv.org/abs/1602.07868</a></p><p>Spectrum Normalization<br><a href="https://arxiv.org/abs/1705.10941">https://arxiv.org/abs/1705.10941</a></p>]]></content>
    
    
    <categories>
      
      <category>深度学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>Batch Normalization</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>深度学习-3-降低Loss下篇-Optimization</title>
    <link href="/2021/07/27/opti/"/>
    <url>/2021/07/27/opti/</url>
    
    <content type="html"><![CDATA[<blockquote><p>创作声明：主要为李宏毅老师的听课笔记，附视频链接：<a href="https://www.bilibili.com/video/BV1Wv411h7kN?p=11">https://www.bilibili.com/video/BV1Wv411h7kN?p=11</a></p></blockquote><h2 id="Critical-Point"><a href="#Critical-Point" class="headerlink" title="Critical Point"></a>Critical Point</h2><h3 id="Critical-Point的相关概念"><a href="#Critical-Point的相关概念" class="headerlink" title="Critical Point的相关概念"></a>Critical Point的相关概念</h3><p>在做Optimization的时候，你会发现，随着你的参数不断的update，你的training的loss不会再下降，但是你对这个loss仍然不满意，你可以把deep的network，跟linear的model，或比较浅的network比较，发现说它没有做得更好，所以 deepnetwork，没有发挥它完整的力量，有时候你会甚至发现，一开始你的model就train不起来，一开始你不管怎么update你的参数，你的loss降不下来，那这个时候到底发生了什么事情呢？  </p><p>过去常见的一个估计，是因为我们现在走到了一个地方，这个地方参数对loss的微分为零，当你的参数对loss微分为零的时候，gradient descent就没有办法再update参数了，这个时候training就停下来了，loss当然就不会再下降了。  </p><p><img src="/img/opti/1.png"></p><p>这个时候gradient为0，而且此时loss还是很高，没有发挥模型能力，显然不是global minima，那其实除了马上会想到的local minima，还有其他可能，比如说<strong>saddle point（鞍点）</strong>，所谓saddle point，其实就是gradient是零，但不是local minima，也不是local maxima的地方，像在右边这个例子里面红色的这个点，它在左右这个方向是比较高的，前后这个方向是比较低的，它就像是一个马鞍的形状，所以叫做saddle point，中文就翻成鞍点。<br>gradient为零的点，统称为<strong>critical point（驻点）</strong>，所以loss没有办法再下降，可以说是因为卡在了critical point，但不能说是卡在local minima，因为saddle point也是微分为零的点。</p><p><strong>新的问题：gradient卡在了某个critical point，我们有没有办法知道，到底是local minima，还是saddle point？</strong></p><p>回答这个问题很有必要，因为如果是卡在local minima，那可能就没有路可以走了，因为四周都比较高，你现在所在的位置已经是最低的点，loss最低的点了，往四周走loss都会比较高，你会不知道怎么走到其他的地方去，但如果你今天是卡在saddle point的话，saddle point旁边还是有路可以走的，还是有路可以让你的loss更低的。</p><h3 id="区分local-minima和saddle-point"><a href="#区分local-minima和saddle-point" class="headerlink" title="区分local minima和saddle point"></a>区分local minima和saddle point</h3><p>需要用到简单的数学知识，<br>判断说一个点到底是local minima，还是saddle point，需要知道我们loss function的形状，network本身很复杂，用复杂network算出来的loss function，显然也很复杂，虽然我们没有办法完整知道整个loss function的样子，但是如果给定某一组参数，比如说蓝色的这个在附近的loss function，是有办法被写出来的，它写出来如下：</p><p><img src="/img/opti/2.png"></p><p>实际上是<strong>Tayler Series Approximation（泰勒级数展开）</strong></p><p><strong>g</strong>是一个向量，就是我们的gradient，我们用绿色的这个g来代表gradient，它的第i个component，就是θ的第i个component对L的微分。</p><p>H是一个矩阵，H里面放的是L的二次微分，它第i个row，第j个column的值，就是把θ的第i个component，对L作微分，再把θ的第j个component，对L作微分，再把θ的第i个component，对L作微分，做两次微分以后的结果就是这个$H_{ij}$</p><p><img src="/img/opti/3.png"></p><p>如果我们今天走到了一个critical point，意味着gradient为零，也就是绿色的这一项完全都不见了，只剩下红色的这一项，所以当在critical point的时候，这个loss function，它可以被近似为$L（\theta^{\prime}）$加上红色的这一项<br>我们可以根据红色的这一项来判断，在$\theta^{\prime}$附近的error surface，到底长什么样子，而知道Error surface长什么样子，我就可以判断$\theta^{\prime}$是一个local minima,是一个local maxima,还是一个saddle point。</p><p>方便起见，我们把$（\theta-\theta^{\prime}）$用$v$x向量来表示，判断方法如下：</p><p><img src="/img/opti/4.png"></p><h3 id="逃离saddle-point"><a href="#逃离saddle-point" class="headerlink" title="逃离saddle point"></a>逃离saddle point</h3><p><img src="/img/opti/5.png"></p><p>只要$（\theta-\theta^{\prime}）$等于$u$，loss就会变小，所以你今天只要让$θ&#x3D;u+\theta^{\prime}$，你就可以让loss变小只要沿着u，也就是eigen vector的方向，去更新你的参数去改变你的参数，你就可以让loss变小了。</p><p>所以虽然在critical point没有gradient，如果我们今天是在一个saddle point，你也不一定要惊慌，你只要找出负的eigen value，再找出它对应的eigen vector，用这个eigen vector去加$\theta^{\prime}$,就可以找到一个新的点，这个点的loss比原来还要低。</p><p>但是在实际的操作里，你几乎不会真的把Hessian算出来，这个要是二次微分，要计算这个矩阵的computation需要的运算量非常非常的大，更遑论还要把它的eigen value，跟eigen vector找出来，所以在实际的操作上，几乎没有人用这一个方法来逃离saddle point，之后的笔记会详细阐述其他有机会逃离saddle point的方法，他们的运算量都比要算这个H要小很多，这个方法是想说，如果是卡在saddle point，也许没有那么可怕，最糟的状况下你还有这一招，可以告诉你要往哪一个方向走。</p><p>从经验上看起来，其实local minima并没有那么常见，多数的时候往往是因为你卡在了一个saddle point。至于为什么，粗浅的解释是维度论，三维中封闭的东西，从四维轻松穿过，而向量的维度很多，那么是不是可以走的路就越多了呢，我们在训练一个network的时候，我们的参数往往动辄百万千万以上，所以我们的Error surface，其实是在一个非常高的维度中，我们参数有多少就代表我们的Error surface的维度有多少，参数是一千万就代表error surface，它的维度是一千万，既然维度这么高，会不会其实就有非常多的路可以走呢，那既然有非常多的路可以走，会不会其实local minima根本就很少呢，这是个尚待研究的问题。</p><h2 id="Batch"><a href="#Batch" class="headerlink" title="Batch"></a>Batch</h2><p>之前说过实际上在算微分的时候，并不是真的对所有Data算出来的L作微分，而是把所有的Data分成一个一个的Batch</p><p><img src="/img/opti/batch1.png"></p><p>所有的Batch看过一遍，叫做一个Epoch，事实上，在做这些Batch的时候，你会做一件事情叫做<strong>Shuffle（洗牌）</strong>，Shuffle有很多不同的做法，但一个常见的做法就是，在每一个Epoch开始之前，会分一次Batch，然后每一个Epoch的Batch都不一样。</p><h3 id="回答之前的问题，为什么要用batch？"><a href="#回答之前的问题，为什么要用batch？" class="headerlink" title="回答之前的问题，为什么要用batch？"></a><strong>回答之前的问题，为什么要用batch？</strong></h3><p><img src="/img/opti/batch2.png"></p><p>假设有20组数据，左边所有的数据都看过一遍才能够 Update 一次参数，右边所有资料看过一遍，你已经更新了20次的参数，但是左边这样子的方法有一个优点，就是它这一步走的是稳的，那右边这个方法它的缺点，就是它每一步走的是不稳的。<br>看起来左边的方法跟右边的方法，他们各自都有擅长跟不擅长的东西，左边是蓄力时间长，但是威力比较大，右边技能冷却时间短，但是它是比较不准的，看起来各自有各自的优缺点，但是你会觉得说，左边的方法技能冷却时间长，右边的方法技能冷却时间短，那只是你没有考虑并行运算的问题。</p><h4 id="Larger-batch-size-does-not-require-longer-time-to-compute-gradient"><a href="#Larger-batch-size-does-not-require-longer-time-to-compute-gradient" class="headerlink" title="Larger batch size does not require longer time to compute gradient"></a>Larger batch size does not require longer time to compute gradient</h4><p>事实上，比较大的Batch Size，你要算Loss，再进而算Gradient，所需要的时间，不一定比小的Batch Size要花的时间长。<br><img src="/img/opti/batch3.png"><br>这边我们就是做了一个实验，我们想要知道说，给机器一个Batch，它要计算出Gradient，进而Update参数，到底需要花多少的时间。<br>这边列出了Batch Size等于1等于10，等于100等于1000所需要耗费的时间。<br>你会发现Batch Size从1到1000，需要耗费的时间几乎是一样的，你可能直觉上认为有1000组数据，那需要计算Loss，然后计算Gradient，花的时间不会是一组数据的1000倍吗，但是实际上并不是这样的<br>因为在实际上做运算的时候，我们有GPU，可以做并行运算，是因为你可以做并行计算的关系，这1000组数据是平行处理的，所以1000组数据所花的时间，并不是一组数据的1000倍，当然GPU并行计算的能力还是有它的极限，当你的Batch Size真的非常非常巨大的时候，GPU在跑完一个Batch，计算出Gradient所花费的时间，还是会随着Batch Size的增加，而逐渐增长。</p><p>所以如果Batch Size是从1到1000，所需要的时间几乎是一样的，但是当你的Batch Size增加到10000，乃至增加到60000的时候，你就会发现GPU要算完一个Batch，把这个Batch里面的资料都拿出来算Loss，再进而算Gradient，所要耗费的时间，确实有随着Batch Size的增加而逐渐增长，但你会发现这边用的是V100，所以它挺厉害的，给它60000组数据，一个Batch里面，塞了60000组数据，它在10秒钟之内也把Gradient就算出来了。</p><h4 id="Smaller-batch-requires-longer-time-for-one-epoch"><a href="#Smaller-batch-requires-longer-time-for-one-epoch" class="headerlink" title="Smaller batch requires longer time for one epoch"></a>Smaller batch requires longer time for one epoch</h4><p>GPU虽然有并行计算的能力，但它并行计算能力终究是有个极限，所以你Batch Size真的很大的时候，时间还是会增加的，但是因为有并行计算的能力，因此实际上，当你的Batch Size小的时候，你要跑完一个Epoch，花的时间是比大的Batch Size还要多的。<br><img src="/img/opti/batch4.png"><br>假设我们的训练数据只有6000组，那Batch Size设1，那你要60000个Update才能跑完一个Epoch，如果今天是Batch Size等于1000，你要60个Update才能跑完一个Epoch，而一个Batch Size等于1000和Batch Size等于1算Gradient的时间根本差不多，那60000次Update，跟60次Update比起来，它的时间的差距量就非常可观了。  </p><p>在没有考虑并行计算的时候，你觉得大的Batch比较慢，但实际上，在有考虑并行计算的时候，一个Epoch大的Batch花的时间反而是比较少的。</p><p>那这样看起来大的Batch应该比较好？<br>大的BatchUpdate比较稳定，小的Batch，它的Gradient的方向比较Noisy，那这样看起来，大的Batch应该比较好，小的Batch应该比较差，因为现在大的Batch的劣势已经因为并行计算被拿掉了，它好像只剩下优势而已。</p><p><strong>但神奇的地方是Noisy的Gradient，反而可以帮助Training，这个也是跟直觉正好相反的，大的Batch Size，往往在Training的时候，会给你带来比较差的结果，这个实际是Optimization的问题，当你用大的Batch Size的时候，你的Optimization可能会有问题，小的Batch Size，Optimization的结果反而是比较好的，为什么会这样子呢？</strong></p><h4 id="“Noisy”-update-is-better-for-training"><a href="#“Noisy”-update-is-better-for-training" class="headerlink" title="“Noisy” update is better for training"></a>“Noisy” update is better for training</h4><p>一个可能的解释是这样子的： </p><p><img src="/img/opti/batch5.png"></p><p>假设你是Full Batch，那你今天在Update你的参数的时候，你就是沿着一个Loss Function来Update参数，今天Update参数的时候走到一个Local Minima，走到一个Saddle Point，显然就停下来了，Gradient是零，如果你不特别去看Hession的话，那你用Gradient Descent的方法，你就没有办法再更新你的参数了，但是假如是Small Batch的话，因为我们每次是挑一个Batch出来，算它的Loss，Loss Function 定义不会变，但会因为数据的不同产生的 Error surface，你选到第一个Batch的时候，你是用L1来算你的Gradient，你选到第二个Batch的时候，你是用L2来算你的Gradient，假设你用L1算Gradient的时候，发现Gradient是零，卡住了，但L2跟L1又不一样，L2就不一定会卡住，所以L1卡住了没关系，换下一个Batch来，L2再算Gradient，所以今天这种Noisy的Update的方式，结果反而对Training，其实是有帮助的。</p><h4 id="“Noisy”-update-is-better-for-generalization"><a href="#“Noisy”-update-is-better-for-generalization" class="headerlink" title="“Noisy” update is better for generalization"></a>“Noisy” update is better for generalization</h4><p>小的 Batch 也对 Testing 有帮助，这个实验结果是引用自<a href="https://arxiv.org/abs/1609.04836">On Large-Batch Training For Deep Learning，Generalization Gap And Sharp Minima</a>，</p><p><img src="/img/opti/batch6.png"></p><p>一个解释如图：</p><p><img src="/img/opti/batch7.png"></p><p>假设这个是我们的Training Loss，在这个Training Loss上面可能有很多个Local Minima，有不只一个Local Minima，那这些Local Minima它们的Loss都很低，它们Loss可能都趋近于0，但是这个Local Minima，还是有好Minima跟坏Minima之分。</p><p>如果一个Local Minima它在一个峡谷里面，它是坏的Minima，然后它在一个平原上，它是好的Minima，为什么会有这样的差异呢？</p><p>假设现在Training跟Testing中间，有一个Mismatch，Training的Loss跟Testing的Loss，它们那个Function不一样，有可能是本来你Training跟Testing的<strong>Distribution（分布）</strong> 就不一样。</p><p>那也有可能是因为Training跟Testing，你都是从Sample的Data算出来的，也许Training跟Testing，Sample到的Data不一样，那所以它们算出来的Loss，当然是有一点差距。</p><p>那我们就假设说这个Training跟Testing，它的差距就是把Training的Loss，这个Function往右平移一点，这时候你会发现，对左边这个在一个盆地里面的Minima来说，它的在Training跟Testing上面的结果，不会差太多，只差了一点点，但是对右边这个在峡谷里面的Minima来说，一差就可以天差地远。</p><p>它在这个Training Set上，算出来的Loss很低，但是因为Training跟Testing之间的不一样，所以Testing的时候，这个Error Surface一变，它算出来的Loss就变得很大，而很多人相信这个大的Batch Size，会让我们倾向于走到峡谷里面，而小的Batch Size，倾向于让我们走到盆地里面<br>他直觉上的想法是这样，就是小的Batch，它有很多的Loss，它每次Update的方向都不太一样，所以如果今天这个峡谷非常地窄，它可能一个不小心就跳出去了，因为每次Update的方向都不太一样，它的Update的方向也就随机性，所以一个很小的峡谷，没有办法困住小的Batch。</p><p>如果峡谷很小，它可能动一下就跳出去，之后停下来如果有一个非常宽的盆地，它才会停下来，那对于大的Batch Size，反正它就是顺着规定Update，然后它就很有可能，走到一个比较小的峡谷里面<br>但这只是一个解释，那也不是每个人都相信这个解释，那这个其实还是一个<strong>尚待研究的问题</strong>。</p><h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><p>在有平行运算的情况下，小的Batch跟大的Batch，其实运算的时间并没有太大的差距，除非你的大的Batch那个大是真的非常大，才会显示出差距来。但是一个Epoch需要的时间，小的Batch比较长，大的Batch反而是比较快的，所以从一个Epoch需要的时间来看，大的Batch其实是占到优势的。</p><p>而小的Batch，你会Update的方向比较Noisy，大的Batch Update的方向比较稳定，但是Noisy的Update的方向，反而在Optimization的时候会占到优势，而且在Testing的时候也会占到优势，所以大的Batch跟小的Batch，它们各自有它们擅长的地方。</p><p><strong>所以Batch Size，变成另外一个你需要去调整的Hyperparameter。</strong></p><p>那我们能不能够鱼与熊掌兼得呢，我们能不能够截取大的Batch的优点，跟小的Batch的优点，我们用大的Batch Size来做训练，用平行运算的能力来增加训练的效率，但是训练出来的结果同时又得到好的结果呢，又得到好的训练结果呢。</p><p><img src="/img/opti/batch8.png"></p><h2 id="Momentum"><a href="#Momentum" class="headerlink" title="Momentum"></a>Momentum</h2><p><strong>Momentum（动量）</strong>，这是另外一个，有可能可以对抗Saddle Point或Local Minima的技术，Momentum的运作如图：</p><p><img src="/img/opti/mo1.png"></p><p>它的概念，可以想像成在物理的世界里面，假设Error Surface就是真正的斜坡，而我们的参数是一个球，你把球从斜坡上滚下来，如果今天是Gradient Descent，它走到Local Minima就停住了，走到Saddle Point就停住了,但是在物理的世界里，一个球如果从高处滚下来，从高处滚下来就算滚到Saddle Point，如果有惯性，它从左边滚下来，因为惯性的关系它还是会继续往右走，甚至它走到一个Local Minima，如果今天它的动量够大的话，它还是会继续往右走，甚至翻过这个小坡然后继续往右走<br>那所以今天在物理的世界里面，一个球从高处滚下来的时候，它并不会被Saddle Point，或Local Minima卡住，不一定会被Saddle Point，或Local Minima卡住，我们有没有办法运用这样子的概念，到Gradient Descent里面呢，这个就是Momentum技术。</p><h4 id="Vanilla-Gradient-Descent"><a href="#Vanilla-Gradient-Descent" class="headerlink" title="Vanilla Gradient Descent"></a>Vanilla Gradient Descent</h4><p><strong>Vanilla Gradient Descent（一般梯度下降）</strong> 如图：</p><p><img src="/img/opti/mo2.png"></p><h4 id="Gradient-Descent-Momentum"><a href="#Gradient-Descent-Momentum" class="headerlink" title="Gradient Descent + Momentum"></a>Gradient Descent + Momentum</h4><p>加上Momentum以后，每一次我们在移动我们的参数的时候，我们不是只往Gradient Descent，我们不是只往Gradient的反方向来移动参数，而是Gradient的反方向，加上前一步移动的方向，两者加起来的结果，去调整参数，如图：</p><p><img src="/img/opti/mo3.png"></p><p><img src="/img/opti/mo4.png"><br><strong>所谓的Momentum，当加上Momentum的时候，我们Update的方向，不是只考虑现在的Gradient，而是考虑过去所有Gradient的总和。</strong></p><h2 id="Adaptive-Learning-Rate"><a href="#Adaptive-Learning-Rate" class="headerlink" title="Adaptive Learning Rate"></a>Adaptive Learning Rate</h2><h3 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h3><p>critical point其实不一定是在训练一个Network的时候会遇到的最大的障碍，此话怎讲？</p><p>我们在训练一个network的时候，会把它的loss记录下来，所以会看到loss原来很大，随着你参数不断的update，横轴代表参数update的次数，随着你参数不断的update，这个loss会越来越小，最后就卡住了，你的loss不再下降。<br>多数这个时候，就会猜测是不是走到了critical point，因为gradient等于零的关系，所以我们没有办法再更新参数。当我们说走到critical point的时候，意味着gradient非常的小，但是你有确认过，当你的loss不再下降的时候，gradient真的很小吗？其实很多人可能并没有确认过这件事，而事实上在下面这个例子里面，当我们的loss不再下降的时候，gradient并没有真的变得很小。</p><p><img src="/img/opti/lr1.png"></p><p>gradient是一个向量，下面是gradient的norm，即gradient这个向量的长度，随着参数更新的时候的变化，你会发现虽然loss不再下降，但是这个gradient的norm，gradient的大小并没有真的变得很小<br>这样的结果其实也不难估计，也许你遇到的是这样子的状况：</p><p><img src="/img/opti/lr2.png"></p><p>这个是我们的error surface，然后你现在的gradient，在error surface山谷的两个谷壁间，不断的来回的震荡,这个时候你的loss不会再下降，所以你会觉得它真的卡到了critical point，卡到了saddle point，卡到了local minima吗？不是的，它的gradient仍然很大，只是loss不再减小了。</p><p>举一个非常简单的例子，这边有一个非常简单的error surface：</p><p><img src="/img/opti/lr3.png"></p><p>我们只有两个参数，这两个参数值不一样的时候，Loss的值不一样，我们就画出了一个error surface，这个error surface的最低点在黄色X这个地方，事实上，这个error surface是<strong>convex（凸面的）</strong> 的形状，<strong>convex optimization（凸优化）</strong>。</p><p>它的这个等高线是椭圆形的，只是它在横轴的地方，它的gradient非常的小，它的坡度的变化非常的小，非常的平滑，所以这个椭圆的长轴非常的长，短轴相对之下比较短，在纵轴的地方gradient的变化很大，error surface的坡度非常的陡峭，我们把黑点当作初始的点，然后来做gradient descend<br>你可能觉得这个convex的error surface，做gradient descend，有什么难的吗？不就是一路滑下来，然后再走过去吗，应该是非常容易。你实际上自己试一下，你会发现说，就连这种convex的error surface，形状这么简单的error surface，你用gradient descend，都不见得能把它做好，举例来说这个是实际的结果：</p><p><img src="/img/opti/lr4.png"></p><p>learning rate设10⁻²的时候，参数在山壁的两端不断的震荡，loss掉不下去，但是gradient其实仍然是很大的。</p><p>可能是因为你learning rate设太大了，learning rate决定了我们update参数的时候步伐有多大，learning rate显然步伐太大，你没有办法慢慢地滑到山谷里面，是不是只要把learning rate设小一点，就可以解决这个问题了。</p><p>事实不然，试着调整这个learning rate就会发现，仅仅要train这种convex的optimization的问题，就会很麻烦，调这个learning rate，从10⁻²一直调到10⁻⁷，调到10⁻⁷以后，终于不再震荡了。</p><p><img src="/img/opti/lr5.png"></p><p>终于从这个地方滑到山低谷终于左转，但是会发现这个训练永远走不到终点，因为learning rate已经太小了，竖直往上这一段这个很斜的地方，因为这个坡度很陡，gradient的值很大，所以还能够前进一点，左拐以后这个地方坡度已经非常的平滑了，这么小的learning rate，根本没有办法再让我们的训练前进。</p><p>我们需要更好的gradient descend的版本，在之前我们的gradient descend里面，所有的参数都是设同样的learning rate，这显然是不够的，learning rate它应该要为每一个参数客制化。</p><h3 id="客制化"><a href="#客制化" class="headerlink" title="客制化"></a>客制化</h3><p>其实我们可以看到一个大原则，如果在某一个方向上，gradient的值很小，非常的平坦，那我们会希望learning rate调大一点，如果在某一个方向上，gradient的值很小，非常的陡峭，那我们其实期待learning rate可以设得小一点。<br><img src="/img/opti/lr6.png"></p><p>我们要改一下gradient descend原来的式子，我们只放某一个参数update的式子，我们之前在讲gradient descend，我们往往是讲所有参数update的式子，为了简化这个问题，我们只看一个参数，但是你完全可以把这个方法，推广到所有参数的状况。</p><p>$\theta_i^{t}$，在第t次迭代时第i个参数。</p><p>$g_i^{t}$代表θ等于θᵗ的时候，参数θᵢ对loss的微分，我们把这个θᵢᵗ减掉learning rate乘gᵢᵗ会得到θᵢᵗ⁺¹,这是我们原来的gradient descend，我们的learning rate是固定的。<br>现在我们要有一个随着参数客制化的learning rate，我们把原来learning rate这一项呢，改写成<br>$\frac{\eta}{\sigma_i^{t}}$，$\sigma_i^{t}$有一个上标t，有一个下标i，这代表说这个σ这个参数，首先它是depend on i的，不同的参数我们要给它不同的σ,同时它也是iteration dependent的，不同的iteration我们也会有不同的σ，此时我们就有一个parameter dependent的learning rate。<br>最后公式如下：</p><p>$\theta_i^{t+1}\gets \theta_i^{t}-\frac{\eta}{\sigma_i^{t}}g_i^{t}$</p><h4 id="Adagrad"><a href="#Adagrad" class="headerlink" title="Adagrad"></a>Adagrad</h4><p>计算σ一个常见的类型是算gradient的Root Mean Square</p><p><img src="/img/opti/lr7.png"></p><p>这一招被用在一个叫做Adagrad的方法里面，为什么它可以做到我们刚才讲的，坡度比较大的时候，learning rate就减小，坡度比较小的时候，learning rate就放大呢？</p><p><img src="/img/opti/lr8.png"></p><p>我们可以想像说，现在我们有两个参数：θᵢ¹和θᵢ²，θᵢ¹坡度小，θᵢ²坡度大，θᵢ¹因为它坡度小，所以你在θᵢ¹这个参数上面，算出来的gradient值都比较小，因为gradient算出来的值比较小，然后这个σ是gradient的平方和取平均再开根号，所以算出来的σ就小，所以learning rate就大，同理，θᵢ²会导致learning rate小。</p><p>但这种方法不能解决的是同一参数随时间变化的learning rate需求，他只是对每个参数做了不同的learning rate调整，但同一参数的gradient就会固定是差不多的值。</p><h4 id="RMS-Prop"><a href="#RMS-Prop" class="headerlink" title="RMS Prop"></a>RMS Prop</h4><p><img src="/img/opti/lr8.png"><br>RMS Prop这个方法,它的第一步跟Adagrad的方法,是一模一样的，但是它存有一个α，就像learning rate一样，这个你要自己调它，它是一个hyperparameter。</p><ul><li>α设很小趋近于0，就代表我觉得gᵢ¹相较于之前所算出来的gradient而言，比较重要。</li><li>α设很大趋近于1，那就代表我觉得现在算出来的gᵢ¹比较不重要，之前算出来的gradient比较重要。</li></ul><p>你用α来决定现在刚算出来的gᵢᵗ它有多重要，这个就是RMS Prop。他可以解决动态问题。</p><p>我们现在假设从这个地方开始：</p><p><img src="/img/opti/lr9.png"></p><p>这个黑线是我们的error surface，从这个地方开始你要update参数，好你这个球就从这边走到这边，那因为一路上都很平坦，很平坦就代表说g算出来很小，代表现在update参数的时候，我们会走比较大的步伐。</p><p><img src="/img/opti/lr10.png"></p><p>接下来继续滚，滚到这边以后我们gradient变大了，如果不是RMS Prop，原来的Adagrad的话它反应很慢，但如果你用RMS Prop，把α设小一点，让新的gradient影响比较大的话，那你就可以很快的让σ的值变大，于是可以很快的让你的步伐变小。<br>本来很平滑走到这个地方，突然变得很陡，那RMS Prop可以很快的踩一个刹车，把learning rate变小，如果你没有踩刹车的话，你走到这里这个地方，learning rate太大了，那gradient又很大，两个很大的东西乘起来，你可能就很快就飞出去了，飞到很远的地方。</p><p><img src="/img/opti/lr11.png"></p><p>如果继续走，又走到平滑的地方了，因为这个σᵢᵗ你可以调整α，让它比较看重于最近算出来的gradient，所以你gradient一变小，σ可能就反应很快，它的这个值就变小了，然后你走的步伐就变大了。</p><h4 id="Adam"><a href="#Adam" class="headerlink" title="Adam"></a>Adam</h4><p>实际上如今最常用的optimization的策略是Adam。</p><p>Adam就是RMS Prop加上Momentum，Adam的演算法跟原始的论文在<a href="https://arxiv.org/pdf/1412.6980.pdf">这里</a></p><p>实际上adam在今天的pytorch里面已经集成，所以不用担心这种optimization的问题，optimizer这个deep learning的套件，往往都帮你做好了，然后optimizer里面，也有一些参数需要调，也有一些hyperparameter，需要人工决定，但是你往往用预设的，那一种参数就够好了，你自己调有时候会调到比较差的，往往直接复制pytorch里面Adam这个optimizer，然后预设的参数不要随便调，就可以得到不错的结果了，关于Adam的细节，如有兴趣，大家可以自己研究。</p><h4 id="Learning-Rate-Scheduling"><a href="#Learning-Rate-Scheduling" class="headerlink" title="Learning Rate Scheduling"></a>Learning Rate Scheduling</h4><p>采用原始的adagrad，我们会得到如下：</p><p><img src="/img/opti/lr13.png"></p><p>这个走下来没有问题，然后接下来在左转的时候，这边也是update了十万次，之前update了十万次，只卡在左转这个地方，现在有Adagrad以后，你可以再继续走到非常接近终点的位置，因为当你走到这个地方的时候，你因为这个左右的方向的，这个gradient很小，所以learning rate会自动调整，左右这个方向的，learning rate会自动变大，所以你这个步伐就可以变大，就可以不断的前进。</p><p>接下来的问题就是，为什么快走到终点的时候突然爆炸了呢？</p><p>我们在做这个σ的时候，我们是把过去所有看到的gradient，都拿来作平均</p><p>所以这个纵轴的方向，在这个初始的这个地方，gradient很大<br>可是这边走了很长一段路以后，这个纵轴的方向，gradient算出来都很小，所以纵轴这个方向，这个y轴的方向σ变小，小到一个地步以后，这个step就变很大，然后就爆炸了，爆炸以后没关系，有办法修正回来，因为出去以后，就走到了这个gradient比较大的地方，走到gradient比较大的地方以后，这个σ又慢慢的变大，σ慢慢变大以后，这个参数Update的步伐大小就慢慢的变小。</p><p>有一个方法可以解决这个问题，这个叫做<strong>learning rate scheduling（学习率调度）</strong></p><p>我们刚才这边还有一项η,这个η是一个固定的值，learning rate scheduling的意思就是说，我们不要把η当一个常数，我们把它跟时间有关</p><ul><li>最常见的策略叫做<strong>Learning Rate Decay（学习率衰减）</strong>，也就是说，随着时间的不断地进行，随着参数不断的update，我们这个η让它越来越小。<br>这个就合理了，因为一开始我们距离终点很远，随着参数不断update，我们距离终点越来越近，所以我们把learning rate减小，让我们参数的更新踩了一个刹车，让我们参数的更新能够慢慢地慢下来，所以刚才那个状况，如果加上Learning Rate Decay有办法解决。</li></ul><p><img src="/img/opti/lr14.png"></p><ul><li>另外一个经典，常用的Learning Rate Scheduling的方式，叫做<strong>Warm Up（热身）</strong>，Warm Up的方法是让learning rate，要先变大后变小，你会问说变大要变到多大呢，变大速度要多快呢，变小速度要多快呢，这个也是hyperparameter，要自己用手调的，但是大方向的大策略就是，learning rate要先变大后变小，这个黑科技出现在很多远古时代（2015）的论文里面，总之会让训练效果变得更好。</li></ul><p><img src="/img/opti/lr15.png"></p><p>那为什么需要warm Up呢，这个仍然是今天可以研究的问题。</p><p>一个可能的解释是说，我们在用Adam，RMS Prop或Adagrad的时候，我们会需要计算σ,它是一个统计的结果，σ告诉我们，某一个方向它到底有多陡，或者是多平滑，那这个统计的结果，要看走狗i数据以后，这个统计才精准，所以一开始我们的统计是不精准的。</p><p>一开始我们的σ是不精准的，所以我们一开始不要让我们的参数，离初始的地方太远，先让它在初始的地方呢，做一些探索，所以一开始learning rate比较小，是让它探索收集一些有关error surface的情报，先收集有关σ的统计数据，等σ统计得比较精准以后，再让learning rate慢慢地爬升。</p><p>这是一个解释为什么我们需要warm up的可能性，如果你想要了解更多有关warm up的东西的话，可以看一篇paper，它是Adam的进阶版叫做RAdam，里面对warm up这件事情有更多的理解。</p><p><img src="/img/opti/lr16.png"></p><h2 id="Summary-of-Optimization"><a href="#Summary-of-Optimization" class="headerlink" title="Summary of Optimization"></a>Summary of Optimization</h2><p>从最原始的gradient descent进化到这一个版本：</p><p><img src="/img/opti/lr17.png"></p><p>这个最终的版本里面有：</p><ul><li>Momentum，Update的方向，不是只考虑现在的Gradient，而是考虑过去所有Gradient的总和，<br>针对Saddle Point或Local Minima的技术。</li><li>Root Mean Square，动态控制update的步伐，平原峭壁任我行。</li><li>Learning rate scheduling，宏观把控步伐的调整。</li></ul><p><img src="/img/opti/lr17.png"></p><p>到目前为止，我们说的是当我们的error surface非常的崎岖，就像这个例子一样非常的崎岖的时候<br>我们用一些比较好的方法，来做optimization，前面有一座山挡着，我们希望可以绕过那座山，山不转路转。</p><p>有没有可能，直接把这个error surface移平，我们改Network里面的什么东西，改Network的架构activation function，或者是其它的东西，直接移平error surface，让它变得比较好train，也就是山挡在前面，就把山直接铲平的意思。</p><h2 id="本文尚待回答的问题（后续文章更新）"><a href="#本文尚待回答的问题（后续文章更新）" class="headerlink" title="本文尚待回答的问题（后续文章更新）"></a>本文尚待回答的问题（后续文章更新）</h2><ul><li>如何“移山”</li></ul>]]></content>
    
    
    <categories>
      
      <category>深度学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>Optimization</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>深度学习-2-降低Loss上篇-Model Bias</title>
    <link href="/2021/07/27/DLover/"/>
    <url>/2021/07/27/DLover/</url>
    
    <content type="html"><![CDATA[<blockquote><p>创作声明：主要为李宏毅老师的听课笔记，附视频链接：<a href="https://www.bilibili.com/video/BV1Wv411h7kN?p=10">https://www.bilibili.com/video/BV1Wv411h7kN?p=10</a></p></blockquote><h2 id="从一张图开始"><a href="#从一张图开始" class="headerlink" title="从一张图开始"></a>从一张图开始</h2><p><img src="/img/dlover/1.png"></p><p>这张图十分简单易懂，指出了一个通用的降低 loss 的方法，接下来是一些补充的阐述。</p><h2 id="Model-Bias与Optimization"><a href="#Model-Bias与Optimization" class="headerlink" title="Model Bias与Optimization"></a>Model Bias与Optimization</h2><p><strong>Model Bias</strong><br>我们已经知道，假设我们的 model 过于简单，那即使我们找到了使这个模型效果最好的一组参数，它的 loss 依然很高，甚至不如别的模型相对效果很差的一组参数，这个时候需要重新设计一个 model，给 model 更大的弹性，可以用 Deep Learning增加更多的弹性，你可以增加更多 features，总之使你的 model 更加复杂，更有弹性去描述复杂情况。  </p><p>但并不是 training 时，loss大就代表一定是 <strong>Model Bias</strong>，你可能会遇到另外一个问题，这个问题是 <strong>Optimization</strong> 做得不好。<br><strong>Optimization</strong><br>我们经常用的gradient desecent，之前的<a href="%5B/img/dlover/1.png%5D(https://love2017.asia/2021/07/26/DLFundamental/#3-Optimization)">笔记</a>已经提到了会有 local minima 的问题。</p><p>那么 training data 的 loss 不够低的时候，到底是 model bias，还是 optimization 的问题呢？一个建议是看到一个你从来没有做过的问题，可以先跑一些比较小的，比较浅的network，甚至用一些不是 deep learning 的方法，比如说 linear model，它们比较容易做Optimize的，可以让我们先有个概念说，这些简单的 model，到底可以得到什么样的 loss，如果你发现你深的model，跟浅的model比起来，深的model明明弹性比较大，但loss却没有办法比浅的model压得更低，那就代表说你的optimization有问题，需要有一些其它的方法，来把optimization这件事情做得更好。<br>假设你现在经过一番努力，你已经可以让你的 training data 的loss变小了，接下来你就可以来看 testing data loss，如果 testing data loss 也足够小就结束了。<br>如果觉得还不够小，testing data上面的loss比 training data 上的loss小很多,那可能就是真的遇到 <strong>overfitting</strong> 的问题。</p><h2 id="Overfitting"><a href="#Overfitting" class="headerlink" title="Overfitting"></a>Overfitting</h2><h3 id="为什么会出现Overfitting"><a href="#为什么会出现Overfitting" class="headerlink" title="为什么会出现Overfitting"></a>为什么会出现Overfitting</h3><p><img src="/img/dlover/2.png"></p><p>一个极端的例子：假设根据我们的训练集，machine learning 的方法找出了一个一无是处的function，这个一无是处的function如下工作，如果今天x当做输入的时候，我们就去比对这个x，有没有出现在训练集里面，如果x有出现在训练集里面，就把它对应的ŷ当做输出，如果x没有出现在训练集里面，就输出一个随机的值。<br>在 training data 上面，这个一无是处的function，它的loss是0，可在 testing data 上面，它的loss会变得很大，因为它其实什么都没有预测，这是一个比较极端的例子，在一般的状况下，也有可能发生类似的事情。<br>这张图很好的解释了为什么会有 overfitting，</p><p><img src="/img/dlover/3.png"></p><p>假设我们输入的feature叫做x，我们输出的level叫做y，那x跟y都是一维的，x跟y之间的关系，是这个二次的曲线，这个曲线我们刻意用虚线来表示，因为我们通常没有办法直接观察到这条曲线，们真正可以观察到的是我们的训练集，训练集你可以想像成，就是从这条曲线上面，随机**sample（采样）**出来的几个点。  </p><p>今天的模型它的的弹性很大的话，你只给它这三个点，它会知道说，在这三个点上面我们要让loss低，所以的model的曲线会通过这三个点，但是其它没有训练集做为限制的地方，它就会有<strong>freestyle</strong>，因为它的弹性很大，所以你的model可以变成各式各样的function，产生各式各样奇怪的结果。</p><p>testing data是橙色的这些点，训练data是蓝色的这些点，用蓝色的这些点，找出一个function以后，你测试在橘色的这些点上，不一定会好，如果你的model它的自由度很大的话，它可以产生非常奇怪的曲线，导致训练集上的结果好，但是测试集上的loss很大，至于更详细的背后的数学原理，我们之后的笔记再予以探讨。</p><h3 id="如何处理Overfitting"><a href="#如何处理Overfitting" class="headerlink" title="如何处理Overfitting"></a>如何处理Overfitting</h3><ol><li>第一个方向，也许也是最有效的方向，增加你的训练集</li></ol><p><img src="/img/dlover/4.png"></p><p>蓝色的点变多了，那虽然 model 的弹性可能很大，但是因为你这边的点非常非常的多，它就可以限制住，所以我们可以 <strong>Data Augmentation（数据集扩增）</strong> 等方法扩大训练集。</p><ol start="2"><li>限制模型的弹性</li></ol><p><img src="/img/dlover/5.png"></p><p>假设我们直接限制说，现在我们的 model，我们猜测出x跟y背后的关系，其实就是一条二次曲线，只是我们不明确的知道这个二次曲线里面的每一个参数长什么样，猜测的结果取决于你对这个问题的理解，如果模型就是二次曲线，那么选择function的时候就会有很大的限制，因为二次曲线来来去去就是那几个形状而已。<br><strong>制造限制的方法举例来说</strong>： </p><ul><li>给它比较少的参数，如果是deep learning的话，就给它比较少的神经元的数目，本来每层一千个神经元，改成一百个神经元之类的，或者让model共享参数，你可以让一些参数有一样的数值，一个例子是我们之前笔记的network的架构，叫做<strong>fully-connected network（全连接神经网络）</strong>，fully-connected network 是一个比较有弹性的架构，而今天图像处理常用的**CNN（全连接神经网络）**是一个比较有限制的架构，CNN是一种比较没有弹性的model，它厉害的地方就是，它是针对图像的特性，来限制模型的弹性，关于CNN，之后的笔记会详谈。</li><li>用比较少的features。</li><li><strong>Early stopping（早停）</strong></li><li><strong>Regularization（正则化）</strong></li><li><strong>Dropout</strong><br>后三者，之后的笔记会单独阐述。</li></ul><p>但模型也不能给太多限制，限制太大就有了 model bias 过大的问题。  </p><p>观察下图：</p><p><img src="/img/dlover/6.png"></p><p>随着model越来越复杂，Training的loss可以越来越低，但是testing的时候呢，当model越来越复杂的时候，刚开始testing的loss会跟着下降，但是当复杂的程度，超过某一个程度以后，Testing的loss就会突然暴增了。</p><p>这是因为说当model越来越复杂的时候，复杂到某一个程度，overfitting的状况就会出现，那我们当然期待说，我们可以选一个中庸的模型，不是太复杂的也不是太简单的，给我们最低的testing loss，那如何选出这样的model？</p><p>以下的讨论：<strong>限于不采用精心设计的数据集（那个直接用testing set衡量即可）</strong></p><p>下面的讨论适用于<strong>只有一个数据集（类似比赛）</strong></p><p>可能很直觉的做法就是说：</p><p>假设我们有三个模型，它们的复杂的程度不一样，我不知道要选哪一个模型才会刚刚好，那直接用 testing set 去看哪个效果最好。<br>我们引入一个概念 <strong>Cross Validation（交叉验证）</strong>。</p><h2 id="Cross-Validation"><a href="#Cross-Validation" class="headerlink" title="Cross Validation"></a>Cross Validation</h2><p>交叉验证（Cross validation)，交叉验证用于防止模型过于复杂而引起的过拟合.有时亦称循环估计， 是一种统计学上将数据样本切割成较小子集的实用方法。于是可以先在一个子集上做分析， 而其它子集则用来做后续对此分析的确认及验证。 一开始的子集被称为训练集。而其它的子集则被称为验证集或测试集。交叉验证是一种评估统计分析、机器学习算法对独立于训练数据的数据集的泛化能力（generalize）。（注，主要用来评估 model bias，使人们可以选择适当复杂度的模型）</p><h3 id="留出法（holdout-cross-validation）"><a href="#留出法（holdout-cross-validation）" class="headerlink" title="留出法（holdout cross validation）"></a>留出法（holdout cross validation）</h3><p>最简单的交叉验证：</p><p>在机器学习任务中，拿到数据后，我们首先会将原始数据集分为三部分：训练集、验证集和测试集。</p><p>训练集用于训练模型，验证集用于模型的参数选择配置，测试集对于模型来说是未知数据，用于评估模型的泛化能力。</p><p>这个方法操作简单，只需随机把原始数据分为三组即可。</p><p>不过如果只做一次分割，它对训练集、验证集和测试集的样本数比例，还有分割后数据的分布是否和原始数据集的分布相同等因素比较敏感，</p><p>不同的划分会得到不同的最优模型，</p><p>而且分成三个集合后，用于训练的数据更少了。</p><h3 id="k-折交叉验证（k-fold-cross-validation）"><a href="#k-折交叉验证（k-fold-cross-validation）" class="headerlink" title="k 折交叉验证（k-fold cross validation）"></a>k 折交叉验证（k-fold cross validation）</h3><p><strong>k 折交叉验证</strong>通过对 k 个不同分组训练的结果进行平均来减少方差，因此模型的性能对数据的划分就不那么敏感。</p><p>第一步，不重复抽样将原始数据随机分为 k 份。</p><p>第二步，每一次挑选其中 1 份作为测试集，剩余 k-1 份作为训练集用于模型训练。</p><p>第三步，重复第二步 k 次，这样每个子集都有一次机会作为测试集，其余机会作为训练集。在每个训练集上训练后得到一个模型，用这个模型在相应的测试集上测试，计算并保存模型的评估指标。</p><p>第四步，计算 k 组测试结果的平均值作为模型精度的估计，并作为当前 k 折交叉验证下模型的性能指标。k 一般取 10，数据量小的时候，k 可以设大一点，这样训练集占整体比例就比较大，不过同时训练的模型个数也增多。数据量大的时候，k 可以设小一点。</p><h3 id="留一法（Leave-one-out-cross-validation）-LOOCV"><a href="#留一法（Leave-one-out-cross-validation）-LOOCV" class="headerlink" title="留一法（Leave one out cross validation） LOOCV"></a>留一法（Leave one out cross validation） LOOCV</h3><p>当 k＝m 即样本总数时，叫做 <strong>留一法（Leave one out cross validation）</strong>，每次的测试集都只有一个样本，要进行 m 次训练和预测。</p><p>这个方法用于训练的数据只比整体数据集少了一个样本，因此最接近原始样本的分布。但是训练复杂度增加了，因为模型的数量与原始数据样本数量相同。一般在数据缺乏时使用。样本数很多的话，这种方法开销很大。</p><p>此外：多次 k 折交叉验证再求均值，例如：10 次 10 折交叉验证，以求更精确一点。划分时有多种方法，例如对非平衡数据可以用分层采样，就是在每一份子集中都保持和原始数据集相同的类别比例。模型训练过程的所有步骤，包括模型选择，特征选择等都是在单个折叠 fold 中独立执行的。</p><h2 id="本文尚待回答的问题（后续文章更新）"><a href="#本文尚待回答的问题（后续文章更新）" class="headerlink" title="本文尚待回答的问题（后续文章更新）"></a><strong>本文尚待回答的问题（后续文章更新）</strong></h2><ul><li>Overfitting的数学原理</li><li>CNN的相关知识</li><li>Early stopping，Regularization，Dropout的相关知识</li></ul>]]></content>
    
    
    <categories>
      
      <category>深度学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>Overfitting</tag>
      
      <tag>Model Bias</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>深度学习-1-基础知识</title>
    <link href="/2021/07/25/DLFundamental/"/>
    <url>/2021/07/25/DLFundamental/</url>
    
    <content type="html"><![CDATA[<blockquote><p>创作声明：主要为李宏毅老师的听课笔记，附视频链接：<a href="https://www.bilibili.com/video/BV1Wv411h7kN?p=2">https://www.bilibili.com/video/BV1Wv411h7kN?p=2</a></p></blockquote><h2 id="机器学习"><a href="#机器学习" class="headerlink" title="机器学习"></a><strong>机器学习</strong></h2><p>提到<strong>深度学习（Deep Learning）</strong>，不得不先提到<strong>机器学习（Machine Learning）</strong>。  </p><blockquote><p>机器学习是一门多领域交叉学科，涉及概率论、统计学、逼近论、凸分析、算法复杂度理论等多门学科。专门研究计算机怎样模拟或实现人类的学习行为，以获取新的知识或技能，重新组织已有的知识结构使之不断改善自身的性能。</p></blockquote><p>我个人认为，当下的机器学习可以概述成一句话，</p><blockquote><p><strong>机器学习就是让计算机具备找函数的问题</strong></p></blockquote><p>举例而言：<br> 语音辨识，向计算机输入一段声音，产生这段声音对应的文字，那你需要的就是一个函数，这个函数的输入是声音讯号，输出是这段声音讯号的属性。这个函数非常复杂，人类没有能力把它写出来，所以我们期待凭借着计算机，把这个函数找出来，这件事情，就是机器学习。</p><h2 id="机器学习的任务"><a href="#机器学习的任务" class="headerlink" title="机器学习的任务"></a><strong>机器学习的任务</strong></h2><p>我们要找的函数不同，机器学习有不同的类别，简单举几个常用：</p><ul><li><strong>Regression</strong><br><strong>Regreesion（回归）</strong>，输出是一个 <strong>scalar（标量）</strong>。举个例子：<br>预测未来某一个时间的PM2.5的数值，计算机做的事情是找一个函数，这个函数的输出，是明天中午的PM2.5的数值，的输入可能是种种跟预测PM2.5有关的指数，包括今天的PM2.5的数值，今天的平均温度，今天平均的臭氧浓度等等，这一个函数可以拿这些数值当作输入，输出明天中午的PM2.5的数值，那这一个找这个函数的任务，叫作 Regression。</li><li><strong>Classification</strong><br><strong>Classification（分类）</strong>，输出的是给定选项中的某个选项。举个例子：<br>每个人都有邮箱，那邮箱里面有一个函数，这个函数可以帮我们侦测一封邮件，是不是垃圾邮件，在侦测垃圾邮件这个问题里面，可能的选项就是两个，是垃圾邮件或不是垃圾邮件，Yes或者是No，输出一个选项，寻找这样一个函数的问题叫作 Classification，Classification 不一定只有两个选项，也可以有多个选项。</li><li><strong>Structured Learning</strong><br><strong>Structured Learning（结构化学习）</strong>，输出是一个有结构的事物，举例而言。计算机画一张图，写一篇文章。</li></ul><h2 id="从机器学习案例开始了解"><a href="#从机器学习案例开始了解" class="headerlink" title="从机器学习案例开始了解"></a><strong>从机器学习案例开始了解</strong></h2><h3 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a><strong>问题描述</strong></h3><p>根据一个频道过往所有的信息去预测它明天有可能的观看的次数是多少，就是找一个函数，这个函数的输入是这个频道过去的信息，输出就是第二天这个频道的观看的次数。</p><h3 id="解决步骤"><a href="#解决步骤" class="headerlink" title="解决步骤"></a><strong>解决步骤</strong></h3><h4 id="1-Function-with-Unknown-Parameters"><a href="#1-Function-with-Unknown-Parameters" class="headerlink" title="1.Function with Unknown Parameters"></a><strong>1.Function with Unknown Parameters</strong></h4><p>第一步，我们先做一个最初步的猜测，比如：$y&#x3D;b+w*x_1$  </p><ul><li>$y$是我们要预测的数值</li><li>$x_1$是这个频道前一天总共观看的次数，$x_1$在这里是数值</li><li>$b$和$w$都是未知的参数，我们并不知道</li></ul><p>这个猜测来自于你对这个问题本质上的了解，就是 <strong>Domain knowledge（领域知识）</strong>。<br>关于$y&#x3D;b+w*x_1$，$b$和$w$都是未知的 Parameters（参数）<br>而这个带有 Unknown Parameters 的 Function（函数）我们就叫做 <strong>model（模型）</strong>，<strong>model在机器学习里面，就是一个带有未知的 Parameter 的 Function</strong>。<br>$w$：weight（权重）<br>$x_1$：Feature（特征）<br>$b$：bias（偏差）</p><h4 id="2-Define-loss-from-Training-Data"><a href="#2-Define-loss-from-Training-Data" class="headerlink" title="2.Define loss from Training Data"></a><strong>2.Define loss from Training Data</strong></h4><p>第二步，我们定义一个东西叫做 <strong>loss</strong>，</p><blockquote><p>Loss is a Function of parameters.  $L（b,w）$</p></blockquote><blockquote><p>loss：how good a set of values are. </p></blockquote><p><strong>Loss Function（损失函数）</strong>，它的输入，是我们 model 里面的参数（b和w），输出的值代表我们把这一组未知的参数，设定成某一组数值的时候，这组数值好还是不好。</p><p>计算 loss 要从训练数据出发，在这个问题里面，我们的训练数据是这一个频道过去每天的观看次数，假设此时b设为0.5k，w设为1的时候，我们把2017年1月1号的观看次数代入x1，结果是5.3k，我们是知道1月2号的真实值，所以我们可以比较一下，现在这个函数预估的结果跟真正的结果的差距有多大，这个函数预估的结果是5.3k，真正的结果是4.9k，那这个真实的值叫做 <strong>Label（标签）</strong> ，估测的值用y来表示，真实的值用ŷ来表示，你可以计算y跟ŷ之间的差距，得到一个eₗ，代表估测的值跟真实的值之间的差距，计算差距其实不只一种方式，这边把y跟ŷ相减，直接取绝对值，算出来的值是0.4k。</p><p><img src="/img/dlfun/loss1.png"></p><p>同一个方法，可以算出这三年来每一天的预测的误差，假设我们今天的 Function，是$y&#x3D;0.5k+1*x_1$，这三年来每一天的误差都可以算出来，每一天的误差都可以给我们一个小e，接下来我们就把每一天的误差加起来然后取得平均，N代表我们的训验数据的个数，对于三年来的训练数据，每年365天，所以365乘以3，那我们算出一个L，我们算出一个L，这L是每一笔训练数据的平均误差，在这里这个L就是我们的 loss。</p><p><img src="/img/dlfun/loss2.png"></p><p>L越大，代表我们现在这一组参数越不好，这个L越小，代表现在这一组参数越好。<br>估测的值跟实际的值之间的差距，其实有不同的计算方法，在我们刚才的例子里，我们是算y跟ŷ之间绝对值的差距，这一种计算差距的方法，得到的这个L，得到的 loss 叫 <strong>mean absolute error</strong>，缩写是 <strong>MAE</strong>，如果你今天的e是用y跟ŷ相减的平方算出来的，这个叫<strong>mean square error</strong>，又叫 <strong>MSE</strong>，我们选择 <strong>MAE</strong>，作为我们计算这个误差的方式，把所有的误差加起来，就得到 loss，那你要选择 <strong>MSE</strong> 也是可以的，有一些任务，如果y和ŷ它都是概率，都是概率分布的话，在这个时候，你可能会选择<strong>Cross-entropy</strong>，这个我们之后再说，我们这边就是选择了<strong>MAE</strong>，那这个是机器学习的第二步。<br><strong>补充：</strong><br><strong>Error Surface（误差曲面)</strong><br>我们可以调整不同的w和不同的b，组合起来以后为不同的w跟b的组合计算它的 loss，然后就可以画出以下这一个等高线图，这个等高线图就叫<strong>Error Surface</strong>。</p><p><img src="/img/dlfun/loss3.png"></p><p>在这个等高线图上面，越偏红色系，代表计算出来的 loss 越大，就代表这一组参数越差，如果越偏蓝色系，就代表 loss越小，就代表这一组参数越好。</p><h4 id="3-Optimization"><a href="#3-Optimization" class="headerlink" title="3.Optimization"></a><strong>3.Optimization</strong></h4><p>第三步，要做的事情其实是解一个最优化的问题，英文也叫<strong>Optimization（最优化）</strong>，对于这个案例，我们要做的事情就是，找一组w跟b的数值出来使我们的 loss 的值最小。<br>一种常用的<strong>Optimization</strong>的方法，叫做 <strong>Gradient Descent（梯度下降）</strong>。  简化起见，我们先假设没有b那个未知的参数，只有w这个未知的参数.<br><strong>Gradient Descent的步骤</strong></p><ol><li>随机选取一个初始的点，我们叫做w0，这个初始的点往往真的就是随机的，其实有一些方法可以给我们一个比较好的w0的值，我们先当作都是随机的。</li><li>计算在w&#x3D;w0的时候，w这个参数对 loss 的微分是多少，也就是计算在这一个点 Error Surface 的切线斜率，也就是这一条蓝色的虚线，如果这一条虚线的斜率是负的代表说在这个位置左边比较高，右边比较低。</li><li>左边比较高右边比较低的话，我们就把w的值变大，那我们就可以让 loss 变小，如果算出来的斜率是正的，就代表说左边比较低右边比较高，我们把w变小了，w往左边移，可以让 loss 的值变小，可以想像说有一个人站在这个地方下山，然后他左右环视一下，那这一个算微分这件事啊，就是左右环视，它会知道左边比较高还是右边比较高，看哪边比较低，它就往比较低的地方跨出一步，那这一步的步伐的大小取决于两件事情。</li></ol><ul><li>第一件事情是这个地方的斜率有多大，这个地方的斜率大，这个步伐就跨的大一点，斜率小步伐就跨的小一点。</li><li>除了斜率以外，就是除了微分这一项，还有另外一个东西会影响步伐大小，叫做 <strong>learning rate（学习速率）</strong>，它是你自己设定的，你自己决定这个的大小，如果设大一点，那你每次参数 <strong>Update（更新）</strong> 就会量大，你的学习可能就比较快，如果η设小一点，那你参数的 Update 就很慢，每次只会改变一点点参数的数值，这种在机器学习需要自己设定的东西，叫做<strong>HyperParameters（超参数）</strong>。</li></ul><p><img src="/img/dlfun/loss4.png"></p><p><strong>补充：loss 可以是负的吗?</strong><br>loss 这个函数是自己定义的，如果 loss 的定义，就跟刚才定的一样是绝对值，那它就不可能是负值，但这个 loss，这个 Function 是你自己决定的，所以它有可能是负的。<br>我们说我们要把w⁰往右移一步，那这个新的位置就叫做w¹，这一步的步伐是η乘上微分的结果，如果用数学式来表示它的话，就是把w⁰减掉η乘上微分的结果，得到w¹，如图所示。</p><p><strong>Gradient Descent 什么时候会停下来</strong></p><ul><li>第一种状况是你失去耐心了，你一开始会设定说，我今天在调整我的参数的时候，我在计算我的微分的时候，我最多计算几次，比如设定100万次，那参数更新100万次以后就不再更新了，至于要更新几次，这个也是一个 HyperParameter，由自己决定的。</li><li>理想上的停下来的可能是，我们不断调整参数，调整到一个地方，它的微分的值算出来正好是0的时候，如果这一项正好算出来是0，0乘上 learning rate η 还是0，所以你的参数就不会再移动位置，那假设我们是这个理想的状况，我们把w⁰更新到w¹，再更新到w²，最后更新到wᵗ，wᵗ卡住了，也就是算出来这个微分的值是0了，那参数的位置就不会再更新。</li></ul><p><img src="/img/dlfun/loss5.png"></p><p>Gradient Descent 这个方法，有一个严重的问题，我们很可能没有找到真正最好的解，我们没有找到那个可以让 loss 最小的那个w，在这个例子里面，把w设定在右侧红点附近这个地方，你可以让 loss 最小，但是如果 Gradient Descent，是从w⁰这个地方当作随机初始的位置的话，很有可能走到wᵗ这里，你的训练就停住了，你就没有办法再移动w的位置。</p><p>那右侧红点这一个位置，这个真的可以让 loss 最小的地方，叫做 <strong>global minima（全局最小值）</strong>，而wᵗ这个地方叫做 <strong>local minima（局部最小值）</strong>，它的左右两边，都比这个地方的 loss 还要高一点，但是它不是整个 Error Surface 上面的最低点。</p><p><strong>补充：</strong> 常常可能会听到有人讲到，Gradient Descent 不是个好方法，这个方法会有 local minima 的问题，没有办法真的找到 global minima，事实上，假设你有做过深度学习相关的事情，假设你有自己训练 network ，自己做过 Gradient Descent 经验的话，我们在做 Gradient Descent 的时候，真正面对的难题不是 local minima，到底是什么之后再说。</p><p>实际上我们刚才的模型有两个参数，有w跟b，那有两个参数的情况下，怎么用 Gradient Descent 呢，其实跟刚才一个参数没有什么不同，若一个参数你没有问题的话，你可以很快的推广到两个参数。</p><p><img src="/img/dlfun/loss6.png"></p><p>实际上真的用 <strong>Gradient Descent</strong>，对数据进行一番计算以后我们算出来的最好的w是0.97，最好的b是0.1k，跟我们的猜测蛮接近的，那 loss 多大呢，loss 算一下是0.48k，也就是在2017到2020年的数据上，如果使用这一个函数，b代入0.1k，w代入0.97，那平均的误差是0.48k，也就是它的预测的误差，大概是500次左右。</p><h3 id="Linear-Model"><a href="#Linear-Model" class="headerlink" title="Linear Model"></a><strong>Linear Model</strong></h3><p><strong>Linear Model（线性模型）</strong> 实际上，以上<strong>三个步骤</strong>我们合起来叫做模型的<strong>训练</strong>。</p><p>我们接下来要做的事情就是拿这个函数，来真的预测一下未来的观看次数，那这边，我们只有2017年到2020年的值，我们在2020年的最后一天，跨年夜的时候，找出了这个函数，接下来从2021年开始每一天，我们都拿这个函数，去预测隔天的观看人次，我们就拿2020年的12月31号的，观看人次，去预测2021年元旦的观看人次，用2021年元旦的观看人次，预测一下2021年元旦隔天，1月2号的观看人次，用1月2号的观看人次去预测，1月3号的观看人次，每天都做这件事，一直做到2月14号，就做到情人节，然后得到平均的值，平均的误差值是多少呢，这个是真实的数据的结果，在2021年没有看过的数据上，这个误差值是，我们这边用 L prime 来表示，它是0.58，所以在有看过的数据上，在训练数据上，误差值是比较小的，在没有看过的数据上，在2021年的数据上，看起来误差值是比较大的，那我们每一天的平均误差，有600人左右。</p><p><img src="/img/dlfun/loss7.png"></p><p>分析结果（后期修改）</p><ul><li>横轴代表时间</li><li>纵轴代表观看次数</li><li>红线为真实次数</li><li>蓝线为预测次数</li></ul><p>这蓝色的线没什么神奇的地方，基本就是红色的线往右平移一天而已，计算机就是拿前一天的观看人次来预测第二天的观看人次，但是如果你仔细观察这个图，你就会发现，这个真实的数据有一个很神奇的现象，它是有周期性的，我们看真实的数据，每隔七天一个循环，每个礼拜五礼拜六，看的人就是特别少。所以既然我们已经知道每隔七天，就是一个循环，那这一个model，显然很烂，因为它只能够看前一天。<br>每隔七天它一个循环，如果我们一个模型，它是参考前七天的数据，把七天前的数据，直接复制到拿来当作预测的结果，也许预测的会更准也说不定，所以我们就要修改一下我们的模型，通常一个模型的修改，往往来自于你对这个问题的理解，也就是 Domain Knowledge。<br>一开始，我们对问题完全不理解的时候，我们就胡乱写一个并没有做得特别好，接下来我们观察了真实的数据以后，得到一个结论是每隔七天有一个循环，所以我们应该要把前七天的观看人次都列入考虑，所以我们写了一个新的模型。<br>xⱼ下标j代表是几天前，然后这个j等于1到7，也就是从一天前两天前，一直考虑到七天前，乘上不同的wⱼ，加起来，再加上 bias，得到预测的结果。<br>如果这个是我们的 model，那我们得到的结果是怎么样呢，我们在训练数据上的 loss 是0.38k，那因为这边只考虑一天，这边考虑七天，所以在训练数据上，你会得到比较低的 loss，这边考虑了比较多的信息，在训练数据上理论应该要得到更好的，更低的 loss，这边算出来是0.38k，但它在没有看过的数据上面，做不做得好呢，在没有看到的数据上确实有比较好，是0.49k，只考虑一天是0.58k的误差，考虑七天是0.49k的误差。</p><p><img src="/img/dlfun/loss8.png"></p><p>然后考虑28天会怎么样呢，预测出来结果在训练数据上是0.33k，那在2021年的数据上，在没有看过的数据上是0.46k，看起来又更好一点，那接下来考虑56天会怎么样呢，在训练数据上是稍微再好一点，是0.32k，在没看过的数据上还是0.46k，看起来，考虑更多天没有办法再更进步了，看来考虑天数这件事，也许已经到了一个极限，这些模型，它们都是把输入的这个x，它叫做 Feature，把 Feature 乘上一个 weight，再加上一个bias就得到预测的结果，这样的模型有一个共同的名字，叫做 <strong>Linear model</strong>，那我们接下来会看，怎么把 <strong>Linear model</strong>做得更好。<br>Linear Model 过于简单只能描述简单的线性关系，我们需要更复杂的 model，</p><p><img src="/img/dlfun/model1.png"></p><p>你不管怎么改变w跟b，你永远无法用 Linear Model，制造红色这一条线，显然LinearModel有很大的限制，这一种来自于Model的限制，叫做 <strong>Model Bias（模型偏差）</strong>，跟b的这个Bias不太一样，它指的意思是说，模型先天限制没有办法模拟真实的状况。</p><h2 id="深度学习"><a href="#深度学习" class="headerlink" title="深度学习"></a><strong>深度学习</strong></h2><h3 id="从更好的Function的开始"><a href="#从更好的Function的开始" class="headerlink" title="从更好的Function的开始"></a><strong>从更好的Function的开始</strong></h3><p>我们需要写一个更复杂的，更有弹性的，有未知参数的 Function，<br><img src="/img/dlfun/model2.png"><br>观察一下红色的这一条曲线，它可以看作是一个常数，再加上一群蓝色的这样子的 Function。<br>蓝色的 Function，它的特性是：</p><ul><li>当输入的值，当x轴的值小于某一个数的时候，它是某一个定值。</li><li>大于另外一个数的时候，又是另外一个定值。</li><li>中间有一个斜坡。</li></ul><p>红色的线可以看作是一个常数项加一大堆的蓝线，常数项应该设跟y轴的交点一样。如何怎么加上这个蓝色的 Function 以后，变成红色的这一条线。<br>以本图为例子：</p><p><img src="/img/dlfun/model3.png"></p><p>蓝线“1” Function 斜坡的起点，设在红色 Function 的起始的地方，然后第二个斜坡的终点设在第一个转角处，你刻意让这边这个蓝色 Function 的斜坡，跟这个红色 Function 的斜坡，它们的斜率是一样的，然后接下来，再加第二个蓝色的 Function，你就看红色这个线，第二个转折点出现在哪里，所以第二个蓝色 Function，它的斜坡就在红色 Function 的第一个转折点，到第二个转折点之间，你刻意让这边的斜率跟这边的斜率一样<br>然后接下来第三个部分，第二个转折点之后的部分，你就加第三个蓝色的 Function，第三个蓝色的 Function，它这个坡度的起始点，故意设的跟这个转折点一样，这边的斜率，故意设的跟这边的斜率一样，好接下来你把0，1，2，3全部加起来，你就得到红色的这个线。  </p><p><img src="/img/dlfun/model4.png"></p><p>上图的 <strong>Curves（曲线）</strong> ，由很多线段所组成的，这个叫做 <strong>Piecewise Linear（分段线性）</strong> 的 Curves，这些 Piecewise Linear 的 Curves，有办法用常数项，加一大堆的蓝色 Function 组合出来。只是如果 Piecewise Linear 的 Curves 越复杂，就是转折的点越多，那你需要的这个蓝色的 Function 就越多。</p><p><img src="/img/dlfun/model5.png"></p><p>很容易由以直代曲的思想得出结论，可以用 Piecewise Linear 的 Curves，去逼近任何的连续的曲线，而每一个 Piecewise Linear 的 Curves，又都可以用一大堆蓝色的 Function 组合起来，也就是说，只要有足够的蓝色 Function 把它加起来，就可以变成任何连续的曲线。</p><p>蓝色的 Function，它的式子如何表示呢。</p><p><img src="/img/dlfun/model6.png"></p><p>我们用<strong>Sigmoid Function</strong>去逼近这个蓝色的 Function，那这个蓝色的 Function，其实通常就叫做<strong>Hard Sigmoid</strong>，<strong>Sigmoid Function</strong>是 S 型的 Function，</p><p><img src="/img/dlfun/model7.png"></p><p>调整这里的b跟w跟c，你就可以制造各种不同形状的Sigmoid Function，用各种不同形状的Sigmoid Function，去逼近这个蓝色的Function。</p><ul><li>改w你就会改变斜率你就会改变斜坡的坡度</li><li>改b你就可以把这一个 Sigmoid Function 左右移动</li><li>改c你就可以改变它的高度</li></ul><p>总结：你只要有不同的w不同的b不同的c，你就可以制造出不同的 Sigmoid Function，把不同的 Sigmoid Function 叠起来以后，你就可以去逼近各种不同的Piecewise Linear的 Function，然后Piecewise Linear的 Function，可以拿来近似各种不同的连续的 Function。</p><p><img src="/img/dlfun/model8.png"></p><p>红色这条线就是0加1+2+3，123都是蓝色的 Function，所以它们的函数就是有一个固定的样子，但是它们的w不一样，它们的b不一样，它们的c不一样，如果是第一个蓝色 Function，它就是w1，b1，c1，第二个蓝色 Function，用的是w2，b2，c2，第三个蓝色 Function，我们就说它用的是w3，b3，c3。</p><p>我们刚才其实已经不是只用一个 Feature，我们可以用多个 Feature。用j表示 Feature 编号，很容易写出来式子，如图。</p><p><img src="/img/dlfun/model9.png"></p><h3 id="从线性代数来看-Function"><a href="#从线性代数来看-Function" class="headerlink" title="从线性代数来看 Function"></a><strong>从线性代数来看 Function</strong></h3><p>假设只考虑三个 Feature，等于1 2 3，那所以输入就是x1代表前一天的观看人数，x2两天前观看人数，x3三天前的观看人数<br>每一个i就代表了一个蓝色的 Function，而我们现在每一个蓝色的 Function，都用一个 Sigmoid Function 来逼近它。<br>这个1，2，3就代表我们有三个 Sigmoid Function，那我们先来看一下，这个括号里面做的事情如图：</p><p><img src="/img/dlfun/model10.png"></p><p>简化起见，我们把括号里面的东西用r1，r2，r3表示：</p><p><img src="/img/dlfun/model11.png"></p><p>由线性代数知识简化成矩阵跟向量的相乘，并可以改换表示形式成：x乘上矩阵w再加上向量 <strong>b</strong>，得到一个向量叫做<strong>r</strong>。</p><p><img src="/img/dlfun/model12.png"></p><p>在这个括号里面做的事情就是这么一回事，把x乘上w加上<strong>b</strong>等于r，就是这边的r1，r2，r3，这是r1，r2，r3，然后分别通过 Sigmoid Function 得到a1，a2，a3，所以下图这个蓝色的虚线里面做的事情，就是从x1，x2，x3得到了a1，a2，a3。</p><p><img src="/img/dlfun/model13.png"></p><p>这个 Sigmoid 的输出，还要乘上ci然后还要再加上b，如果你要用向量来表示的话，a1，a2，a3拼起来叫这个向量<strong>a</strong>，c1，c2，c3拼起来叫一个向量<strong>c</strong>，那我们可以把这个c呢，作<strong>Transpose（转置）</strong>，<strong>a</strong>乘上<strong>c</strong>的 Transpose 再加上b，我们就得到了y，于是可以继续简化：</p><p><img src="/img/dlfun/model14.png"></p><p>最终的线性代数式子：</p><p><img src="/img/dlfun/model15.png"></p><p>补充解释：<br>灰色b是数值，绿色是向量，表示他们不同。<br>将b，<strong>b</strong>，w的每一个 row 或者 column 拿出来，还有 <strong>c</strong>拼成一个很长的向量$\theta$，$\theta$统称我们所有的未知的参数。</p><h3 id="loss"><a href="#loss" class="headerlink" title="loss"></a><strong>loss</strong></h3><p>loss 没有什么不同，定义的方法是一样的，只是我们的符号改了一下，之前是$L（w，b）$，因为w跟b是未知的，那我们现在接下来的未知的参数很多了，所以我们直接用$\theta$来统设所有的参数，所以我们现在的 loss Function 就变成$L（\theta）$</p><p><img src="/img/dlfun/nloss1.png"></p><p>计算的方法跟刚才只有两个参数的时候一样，给定一组$\theta$的值，代入一组 Feature，将估测值与真实label比较得到误差，重复带入不同组 Feature，将得到的所有误差处理得到 loss。</p><h3 id="Optimization"><a href="#Optimization" class="headerlink" title="Optimization"></a><strong>Optimization</strong></h3><p>Optimization 也是一样的，Optimization 的步骤本质没有区别：</p><p><img src="/img/dlfun/opt1.png"></p><p><img src="/img/dlfun/opt2.png"></p><h3 id="补充知识"><a href="#补充知识" class="headerlink" title="补充知识"></a><strong>补充知识</strong></h3><h4 id="Batch-Epoch"><a href="#Batch-Epoch" class="headerlink" title="Batch&amp;&amp;Epoch"></a><strong>Batch&amp;&amp;Epoch</strong></h4><p>实际上我们在做Gradient Descent的时候，我们会这么做，比如我们这边有N个数据，我们会把N个数据随机分成一个一个的<strong>Batch（批）</strong>，每个 Batch 里面有B个数据。<br>本来我们是把所有的 Data 拿出来算一个 Loss，现在我们只拿一个 Batch 里面的 Data 出来算一个 Loss，我们这边把它叫L1，那跟L以示区别，因为你把全部的数据拿出来算 Loss，跟只拿一个 Batch 的数据拿出来算 Loss 不一样，所以这边用L1来表示它。<br>实际操作的时候，每次我们会先选一个 Batch，用这个 Batch 来算L，根据这个L1来算 Gradient，用这个 Gradient 来更新参数，接下来再选下一个 Batch 算出L2，根据L2算出Gradient，然后再更新参数，再取下一个 Batch 算出L3，根据L3算出 Gradient，再用L3算出来的 Gradient 来更新参数，把所有的 Batch 都看过一次，叫做一个 <strong>Epoch（时期）</strong>，每一次更新参数叫做一次 Update， Update 跟 Epoch 并不一样。</p><p><img src="/img/dlfun/batch1.png"></p><p>两个例子：</p><p><img src="/img/dlfun/batch2.png"></p><p>假设有1000对数据，<strong>Batch Size（批尺寸）</strong> 设100，Batch Size 的大小是你自己决定的，所以这边我们又多了一个 HyperParameter，1000个 Example，Batch Size 设100，那1个 Epoch 总共更新10次参数，所以做了一个 Epoch 的训练，更新多少次参数取决于它的 Batch Size 有多大。</p><h4 id="ReLU"><a href="#ReLU" class="headerlink" title="ReLU"></a><strong>ReLU</strong></h4><p>Hard Sigmoid 可以看作是两个<strong>Rectified Linear Unit（线性整流函数，简称 ReLU ）</strong> 的加总，所谓 Rectified Linear Unit，如图所示：</p><p><img src="/img/dlfun/relu.png"></p><p>把两个 ReLU 叠起来，就可以变成 Hard 的 Sigmoid，想要用 ReLU 的话，把 Sigmoid 的地方，换成$c*max（0，b+wx_1）$。</p><p><img src="/img/dlfun/relu2.png"></p><p>Sigmoid 或是 ReLU，它们在机器学习里面统称为 <strong>Activation Function（激活函数）</strong>。<br>当然还有其他的 Activation Function，但 Sigmoid 跟 ReLU，应该是今天最常见的 Activation Function。（一般 ReLU 效果比 Sigmoid 好）。</p><h3 id="时髦的命名"><a href="#时髦的命名" class="headerlink" title="时髦的命名"></a><strong>时髦的命名</strong></h3><p>那我们现在还缺了一个东西，缺一个好名字，只鞋半缕的，说他是汉左将军宜城亭侯中山靖王之后也就有排面起来了，所以我们的模型也需要一个好名字，所以它叫做什么名字呢，这些 Sigmoid 或 ReLU ，它们叫做 <strong>Neuron（神经元）</strong>，我们这边有很多的Neuron，很多的Neuron就叫做 <strong>Neural Network（神经网络）</strong>，Neuron就是神经元，人脑中就是有很多神经元，很多神经元串起来就是一个神经网络，跟大脑一样的，接下来你就可以到处骗麻瓜说这个模型就是在模拟人脑，这个就是人工智能，然后麻瓜就会吓得把钱掏出来。（快笑死了）<br><img src="/img/dlfun/ne1.png"><br>这个把戏在上个80-90年代的时候其实已经玩过了，Neural Network 不是什么新的技术，当时已经把这个技术的名字搞到臭掉了，Neural Network 因为之前吹捧得太过浮夸，所以后来大家对 Neural Network 这个名字，都非常地感冒，它就像是个脏话一样，写在 Paper 上面都注定害你的 Paper 被拒绝，所以后来为了要重振 Neural Network，就需要新的名字，什么样的新的名字呢，这边有很多的 Neuron，每一排 Neuron 我们就叫它一个 <strong>Layer（层）</strong>，它们叫 <strong>Hidden Layer（隐藏层）</strong>，有很多的 Hidden Layer 就叫做 <strong>Deep</strong>，这整套技术就叫做 <strong>Deep Learning</strong>。</p><p>所以人们就开始把神经网络越叠越多越叠越深，12年的时候有一个 AlexNet，它有8层它的错误率是16.4%，两年之后VGG 19层，错误率在图像辨识上进步到7.3 %，这个都是在图像辨识基准的数据集上面的结果，后来 GoogleNe t有错误率降到6.7%，有22层，但这些都不算是什么，Residual Net有152层，但其实要训练这么深的 Network 是有诀窍的，后面再写。</p><p><img src="/img/dlfun/ne2.png"></p><p>层数能一直叠下去吗？分析这个图：</p><p><img src="/img/dlfun/ne3.png"></p><p>在训练数据上，3层比4层差，4层比3层好，但是在没看过的数据上，4层比较差，3层比较好，在有看过的数据上，在训练数据上，跟没看过的数据上，它的结果是不一致的，这种训练数据跟测试，这种训练数据跟没看过的数据，它的结果是不一致的状况，这个状况叫做 <strong>Overfitting（过拟合）</strong>，精确的语言表述是<strong>为了得到一致假设而使假设变得过度复杂</strong>，但你目前明白它是说表现在训练好的模型在训练集上效果很好,但是在测试集上效果差即可，一般情况下过度叠加层数会导致过拟合。<br><strong>一个思考：深的意义在哪里？</strong><br>如果你仔细思考一下，实际上只要够多的 ReLU 够多的 Sigmoid ，就可以逼近任何的连续的Function，我们只要有够多的 Sigmoid，就可以知道够复杂的线段，就可以逼近任何的<strong>Continuous（连续）</strong> 的Function，所以我们只要一排 ReLU 或一排 Sigmoid，够多就足够了，那深的意义到底何在呢？后续的文章会给出解答。</p><h3 id="本文尚待回答的问题（后续文章更新）"><a href="#本文尚待回答的问题（后续文章更新）" class="headerlink" title="本文尚待回答的问题（后续文章更新）"></a><strong>本文尚待回答的问题（后续文章更新）</strong></h3><ul><li>怎么样初始化一组较好的参数</li><li><strong>Cross-entropy</strong> 等损失函数是什么</li><li><strong>Gradient Descent</strong> 的真正难题</li><li>为什么ReLU效果更好</li><li>为什么要分成 <strong>Batch</strong> 处理</li><li>深的意义在哪里？</li></ul>]]></content>
    
    
    <categories>
      
      <category>深度学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>机器学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>深度学习-0-环境配置</title>
    <link href="/2021/07/25/dlenv/"/>
    <url>/2021/07/25/dlenv/</url>
    
    <content type="html"><![CDATA[<blockquote><p>创作声明：主要内容为互联网内容整合，其中关于开发环境配置主要参考为知乎稚辉的专栏教程：<a href="https://zhuanlan.zhihu.com/p/336429888">https://zhuanlan.zhihu.com/p/336429888</a></p></blockquote><h2 id="硬件篇"><a href="#硬件篇" class="headerlink" title="硬件篇"></a><strong>硬件篇</strong></h2><p><strong>建议Intel+NVIDIA</strong><br>作为生产力工具考虑到软件库的兼容性问题，决定选择Intel+NVIDIA平台。</p><figure class="highlight subunit"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs subunit">有钱: <br>3090<span class="hljs-string">+10</span>代i9<span class="hljs-string">+64</span>g内存<span class="hljs-string">+2</span>T固态<span class="hljs-string">+12</span>T机械<br>  GPU方面3090，24G的显存跟TITAN RTX持平了（价格却只要一半）...硬盘的设计是这样的：1T的NVME固态做系统盘，12T的机械盘作为数据集仓库，另外一个1T SATA固态作为训练时的数据集缓存，因为IO读写速度也是会影响训练效率的，所以相比于直接从机械盘里面读取数据，加一块SSD做cache效果会好很多。<br>没钱: <br>3060<span class="hljs-string">+11</span>代i5<span class="hljs-string">+32</span>g内存<span class="hljs-string">+512</span>g固态<span class="hljs-string">+1</span>T机械<br></code></pre></td></tr></table></figure><h2 id="系统篇"><a href="#系统篇" class="headerlink" title="系统篇"></a><strong>系统篇</strong></h2><p>工作站配置为<strong>Windows+Ubuntu双系统</strong>  </p><ul><li><strong>安装windows系统</strong><br>建议使用微PE工具进行安装。（网上有许多相关教程）  </li><li><strong>安装Ubuntu系统</strong></li></ul><ol><li>在官网下载Ubuntu镜像：Ubuntu 20.04.1 LTS Destop image版本，得到.iso镜像文件，在<a href="rufus.ie/zh/">rufus</a>下载rufus，选择安装系统镜像，分区选择GPT，目标系统UEFI，之后点击开始，等待进度条走完，然后点击[关闭]，弹出U盘就ok了。</li><li>重启电脑，启动时连续按F2，进入BIOS，关闭安全启动(不同主板设置不同，请自行查找相关资料进行修改)。</li><li>在BIOS选择U盘启动项进入临时的Ubuntu系统，在图形界面中选择Install Ubuntu，所有配置都可以使用默认的，改一下用户名和密码即可。 这里建议使用英文作为默认语言，避免给自己日后开发找麻烦，安装过程中会联网下载一些软件包更新，可以直接点skip掉，在安装好系统之后再换源手动更新更快。</li><li>关于分区（建议系统默认安装自动分区，在Ubuntu分区时选择“向导──使用最大的连续空闲空间”）</li></ol><figure class="highlight coq"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><code class="hljs coq">手动分区：<br>一共分出<span class="hljs-number">5</span>个系统分区。<br>(<span class="hljs-number">1</span>). 设置efi引导<br>因为是u盘的uefi启动，因此设置一个efi引导项。<br>具体参数：<br>大小: <span class="hljs-number">500</span>到<span class="hljs-number">1024</span>mb即可<br>（视自身的存储空间而定）<br>新分区的类型： 逻辑分区<br>新分区的位置： 空间起始位置<br>用于： EFI系统分区<br>(<span class="hljs-number">2</span>).<span class="hljs-built_in">swap</span><br><span class="hljs-built_in">swap</span>用作虚拟内存，根据自身的物理内存决定。<br>如果物理内存在<span class="hljs-number">8</span>G以下，则<span class="hljs-built_in">swap</span>设置为物理内存一样的大小，如果超过<span class="hljs-number">8</span>G，则一般设置为<span class="hljs-number">8</span>G大小的虚拟内存就足够了。根据自身的使用需求，也可以适当增大<span class="hljs-built_in">swap</span>大小。<br>我的电脑用了两个<span class="hljs-number">8</span>g的内存条，因此大小设置为了<span class="hljs-number">16</span>g*<span class="hljs-number">1024</span>=<span class="hljs-number">16384</span>MB。<br>具体参数：<br>大小: （视自身的物理内存而定）<br>新分区的类型： 主分区<br>新分区的位置： 空间起始位置<br>用于： 交换空间<br>(<span class="hljs-number">3</span>). 挂载点/<br>主要用来存放ubuntu系统文件。有固态硬盘的可以安在固态盘中。<br>具体参数：<br>大小: <span class="hljs-number">100</span>G<br>（视自身的存储空间而定）<br>新分区的类型： 逻辑分区<br>新分区的位置： 空间起始位置<br>用于： Ext4日志文件系统<br>挂载点： /<br>(<span class="hljs-number">4</span>).挂载点/home<br>存放用户文件。这个分区尽量设置大一些，因为我安装了机械盘，因此分配了<span class="hljs-number">300</span>g的存储空间给它。<br>具体参数：<br>大小: <span class="hljs-number">300</span>G<br>（视自身的存储空间而定）<br>新分区的类型： 逻辑分区<br>新分区的位置： 空间起始位置<br>用于： Ext4日志文件系统<br>挂载点： /home<br>分区完成之后，设置安装启动器设备，注意，这里选择刚刚分出来的efi分区，具体的设备号最好再三对照表格中的数据。<br>完成后，会弹出一个确认界面，再一次确认是否分区正确。<br></code></pre></td></tr></table></figure><ol start="5"><li>进入系统后设置一下root账户密码：</li></ol><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">sudo</span> passwd root<br></code></pre></td></tr></table></figure><p>同时为了避免每次sudo都要输入密码，这里配置一下visudo：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">sudo</span> visudo<br></code></pre></td></tr></table></figure><p>在文件最后加上一句（改为自己的用户名）：</p><figure class="highlight nsis"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs nsis"><span class="hljs-literal">user</span> <span class="hljs-literal">ALL</span>=(<span class="hljs-literal">ALL</span>) NOPASSWD: <span class="hljs-literal">ALL</span><br></code></pre></td></tr></table></figure><ol start="6"><li>替换软件源<br>提升后续安装软件时的速度<br>备份原来的源：</li></ol><figure class="highlight avrasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs avrasm"><span class="hljs-keyword">cp</span> /etc/apt/sources<span class="hljs-meta">.list</span> /etc/apt/sources<span class="hljs-meta">.list</span>.bak<br></code></pre></td></tr></table></figure><p>将源的内容设置为阿里云镜像：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">sudo</span> vim /etc/apt/sources.list<br></code></pre></td></tr></table></figure><p>内容改为：(自行查找vim基本指令)</p><figure class="highlight 1c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs 1c">deb http<span class="hljs-punctuation">:</span><span class="hljs-comment">//mirrors.aliyun.com/ubuntu/ focal main restricted universe multiverse</span><br>deb<span class="hljs-punctuation">-</span>src http<span class="hljs-punctuation">:</span><span class="hljs-comment">//mirrors.aliyun.com/ubuntu/ focal main restricted universe multiverse</span><br>deb http<span class="hljs-punctuation">:</span><span class="hljs-comment">//mirrors.aliyun.com/ubuntu/ focal-security main restricted universe multiverse</span><br>deb<span class="hljs-punctuation">-</span>src http<span class="hljs-punctuation">:</span><span class="hljs-comment">//mirrors.aliyun.com/ubuntu/ focal-security main restricted universe multiverse</span><br>deb http<span class="hljs-punctuation">:</span><span class="hljs-comment">//mirrors.aliyun.com/ubuntu/ focal-updates main restricted universe multiverse</span><br>deb<span class="hljs-punctuation">-</span>src http<span class="hljs-punctuation">:</span><span class="hljs-comment">//mirrors.aliyun.com/ubuntu/ focal-updates main restricted universe multiverse</span><br>deb http<span class="hljs-punctuation">:</span><span class="hljs-comment">//mirrors.aliyun.com/ubuntu/ focal-proposed main restricted universe multiverse</span><br>deb<span class="hljs-punctuation">-</span>src http<span class="hljs-punctuation">:</span><span class="hljs-comment">//mirrors.aliyun.com/ubuntu/ focal-proposed main restricted universe multiverse</span><br>deb http<span class="hljs-punctuation">:</span><span class="hljs-comment">//mirrors.aliyun.com/ubuntu/ focal-backports main restricted universe multiverse</span><br>deb<span class="hljs-punctuation">-</span>src http<span class="hljs-punctuation">:</span><span class="hljs-comment">//mirrors.aliyun.com/ubuntu/ focal-backports main restricted universe multiverse</span><br></code></pre></td></tr></table></figure><p>更新软件列表：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">sudo</span> apt update<br><span class="hljs-built_in">sudo</span> apt upgrade<br></code></pre></td></tr></table></figure><ol start="7"><li>Ubuntu系统默认自带python，有版本需求的话也可以自己安装一下（不安装也行因为后面会安装conda环境）：</li></ol><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">sudo</span> apt install python3<br><span class="hljs-built_in">sudo</span> apt install python3-pip<br></code></pre></td></tr></table></figure><p>替换python的pip源建议是一定操作一下的，pip安装速度会快很多：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">cd</span> ~<br><span class="hljs-built_in">mkdir</span> .pip<br></code></pre></td></tr></table></figure><p>直接新建并编辑pip.conf：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">sudo</span> vim ~/.pip/pip.conf<br></code></pre></td></tr></table></figure><p>改为以下内容（这里用的清华源，也可以试一下阿里、豆瓣等源）：</p><figure class="highlight pgsql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs pgsql">[<span class="hljs-keyword">global</span>]<br><span class="hljs-keyword">index</span>-url = https://pypi.tuna.tsinghua.edu.cn/simple/ <br>[install]<br><span class="hljs-keyword">trusted</span>-host = pypi.tuna.tsinghua.edu.cn<br></code></pre></td></tr></table></figure><p>更改默认python版本，python目录默认链接的是python2，而现在基本都是用python3开发了，每次都输入python3很麻烦所以这里直接更换默认的python命令链接。<br>把原来的python软链接删掉：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">sudo</span> <span class="hljs-built_in">rm</span> /usr/bin/python<br></code></pre></td></tr></table></figure><p>新建一个软链接：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">sudo</span> <span class="hljs-built_in">ln</span> -s /usr/bin/python3 /usr/bin/python<br><span class="hljs-built_in">sudo</span> <span class="hljs-built_in">ln</span> -s /usr/bin/pip3 /usr/bin/pip<br></code></pre></td></tr></table></figure><p>现在输入python就会进入python3环境了。</p><h2 id="环境搭建"><a href="#环境搭建" class="headerlink" title="环境搭建"></a><strong>环境搭建</strong></h2><h3 id="远程开发"><a href="#远程开发" class="headerlink" title="远程开发"></a><strong>远程开发</strong></h3><p><strong>安装SSH：</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">sudo</span> apt install ssh<br></code></pre></td></tr></table></figure><p>会自动安装好很多依赖包并启动服务，完成之后用XShell等软件就可以SSH登录服务器了。<br><strong>安装xrdp：</strong><br>Xrdp 是一个微软远程桌面协议（RDP）的开源实现，它允许我们通过图形界面控制远程系统。这里使用RDP而不是VNC作为远程桌面，是因为Windows自带的远程桌面连接软件就可以连接很方便，另外RDP在Windows下的体验非常好，包括速度很快（因为压缩方案做得比较好），可以直接在主机和远程桌面之间复制粘贴等等。<br>安装过程如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">sudo</span> apt install xrdp <br></code></pre></td></tr></table></figure><p>安装完成xrdp 服务将会自动启动，可以输入下面的命令验证它：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">sudo</span> systemctl status xrdp<br></code></pre></td></tr></table></figure><p>默认情况下，xrdp 使用&#x2F;etc&#x2F;ssl&#x2F;private&#x2F;ssl-cert-snakeoil.key，它仅仅对ssl-cert用户组成语可读，所以需要运行下面的命令，将xrdp用户添加到这个用户组：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">sudo</span> adduser xrdp ssl-cert  <br><span class="hljs-built_in">sudo</span> systemctl restart xrdp<br></code></pre></td></tr></table></figure><p><strong>安装frp进行内网穿透</strong> </p><p>前面介绍的SSH和远程桌面都是需要在局域网下通过IP地址进行连接的，而我们配置一台服务器最重要的诉求，应该是可以随时随地去访问服务器。<br><br>那在家里面，网络运营商提供的网络服务通过路由器路由到各个设备，此时路由器会同时具备内网地址（路由器之内，局域网，LAN，也就是192.168.x.x）和外网地址（路由器之外，互联网，WAN）。但是其实这个WAN口的IP并不是真正的“公网IP”，而时经过了多层的NAT转换之后的地址，外网的设备是不能通过这个地址访问到路由器的。这个问题的原因是ipv4地址池紧张，如果运营商给每家的路由器都安排一个公网ip的话，那ip地址早就不够用了呀。<br><br>因此为了能让外网访问到我们局域网内的设备，就需要跟中国电信等运营商申请公网ip（现在能申请到的概率也已经不大了，而且即使申请到也不是所有端口都可以使用的），或者我们自己动手做一些操作来达到同样的目的。<br><br>使用frp之类的软件做反向代理来实现内网穿透，这个方案需要你有一台带公网IP的云服务器的，优点就是完全可控，自己想配置多少个端口的穿透都可以，速度跟你的云服务器带宽有关。<br>为什么需要多个端口？ 是因为不同应用占用的端口不同，比如我们的SSH走的是22号端口，而远程桌面的rdp走的是3389号端口，如果需要自建Web服务的话则是走80&#x2F;443端口、想把工作站作为上外网的代理服务器的话会需要1080端口等等…所以用上面第二个方案显然会方便很多，而且云服务器也不贵，我在腾讯云上购买一年只要200左右。</p><p>如何安装配置frp：<br><br>frp分为frps（server）和frpc（client）两个包 </p><p>前者安装到我们的云服务器上，后者安装在需要被外网访问到的各个设备上，这里就是指我们的深度学习工作站。<br>云服务器：<br>在<a href="https://github.com/fatedier/frp/releases">frp</a>下载适合你服务器系统的frp软件，我这里是用的是腾讯云64位Ubuntu18.04所以选择frp_0.34.3_linux_amd64.tar.gz，下好之后解压：</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">tar</span> -zxvf frp_0.<span class="hljs-number">34</span>.<span class="hljs-number">3</span>_linux_amd64.tar.gz<br></code></pre></td></tr></table></figure><p>我们需要编辑的文件是frps.ini :<br><br>内容改为：(#后面去掉)</p><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs ini"><span class="hljs-section">[common]</span><br><span class="hljs-attr">bind_port</span> = <span class="hljs-number">7000</span> <span class="hljs-comment"># frp服务的端口号，可以自己定</span><br><span class="hljs-attr">dashboard_port</span> = <span class="hljs-number">7500</span> <span class="hljs-comment"># frp的web界面的端口号</span><br><span class="hljs-attr">dashboard_user</span> = user <span class="hljs-comment"># web界面的登陆账户，自己修改</span><br><span class="hljs-attr">dashboard_pwd</span> = pass <span class="hljs-comment"># web界面的登陆密码，自己修改</span><br><span class="hljs-attr">authentication_method</span> = token<br><span class="hljs-attr">token</span> = xxxxx <span class="hljs-comment"># frp客户端连接时的密码，自己修改</span><br></code></pre></td></tr></table></figure><p>保存配置后，使用该命令启动：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">./frps -c ./frps.ini<br></code></pre></td></tr></table></figure><p>在浏览器输入 [云服务器的公网ip]:7500 (不用带[])即可访问到 frp的web管理界面。<br><br>注意，可能需要去云服务器控制台配置安全组规则开放以上涉及到的端口，否则无法访问。</p><p>本地的深度学习服务器端：<br>下载相应版本的frpc软件包（跟刚刚一样的）：这里选amd64的，下好之后解压到一个临时文件夹。<br>修改frpc.ini配置文件，内容如下：</p><figure class="highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs makefile">[common]<br>server_addr = xx.xx.xx.xx <span class="hljs-comment"># 你的云服务器的公网ip</span><br>authentication_method = token<br>token = xxxxx <span class="hljs-comment"># 刚刚配置的frp连接密码 </span><br>server_port = 7000 <span class="hljs-comment"># 刚刚配置的frp服务端口</span><br><br>[Fusion-ssh]<br>type = tcp<br>local_ip = 127.0.0.1<br>local_port = 22<br>remote_port = 20022<br>​<br>[Fusion-rdp]<br>type = tcp<br>local_ip = 127.0.0.1<br>local_port = 3389<br>remote_port = 23389<br></code></pre></td></tr></table></figure><p>通过上面的脚本就可以把对于云服务器特定端口的访问给重定向到本地服务器的某个端口了，简单地讲就是：假如我用SSH客户端访问 [云服务器ip]:20022，就可以经过反向代理直接访问到[本地的训练服务器ip]:20022；同理需要连接远程桌面的话，只需要访问[云服务器ip]:23389就可以了。<br>添加开机自动启动的脚本，新建一个文件：<br><code>/etc/systemd/system/frpc.service</code>，注意修改其中的路径<br>内容：  </p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs routeros">[Fusion]<br><span class="hljs-attribute">Description</span>=Frp<span class="hljs-built_in"> Server </span>Daemon<br><span class="hljs-attribute">After</span>=syslog.target network.target<br><span class="hljs-attribute">Wants</span>=network.target<br><br>[Service]<br><span class="hljs-attribute">Type</span>=simple<br><span class="hljs-attribute">ExecStart</span>=/usr/local/bin/frp/frpc -c /usr/local/bin/frp/frpc.ini # 修改为你的frp实际安装目录<br><span class="hljs-attribute">ExecStop</span>=/usr/bin/killall frpc<br><span class="hljs-comment">#启动失败1分钟后再次启动</span><br><span class="hljs-attribute">RestartSec</span>=1min<br><span class="hljs-attribute">KillMode</span>=control-group<br><span class="hljs-comment">#重启控制：总是重启</span><br><span class="hljs-attribute">Restart</span>=always<br>​<br>[Install]<br><span class="hljs-attribute">WantedBy</span>=multi-user.target<br></code></pre></td></tr></table></figure><p>然后执行以下命令启用脚本：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">sudo</span> systemctl <span class="hljs-built_in">enable</span> frpc.service<br><span class="hljs-built_in">sudo</span> systemctl start frpc.service<br></code></pre></td></tr></table></figure><p>通过下面的命令查看服务状态，如果是running的话就说明可以了：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">sudo</span> systemctl status frpc.service<br></code></pre></td></tr></table></figure><p>这里顺便提一下，按照习惯一般把上面的frp软件解压防止在&#x2F;usr&#x2F;local&#x2F;bin目录下。Linux 的软件安装目录是也是有讲究的，理解这一点，在对系统管理是有益的</p><figure class="highlight jboss-cli"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs jboss-cli"><span class="hljs-string">/usr</span>：系统级的目录，可以理解为C:<span class="hljs-string">/Windows/</span><br><span class="hljs-string">/usr/lib</span>：可以理解为C:<span class="hljs-string">/Windows/System32</span><br><span class="hljs-string">/usr/local</span>：用户级的程序目录，可以理解为C:<span class="hljs-string">/Progrem</span> Files/，用户自己编译的软件默认会安装到这个目录下<br><span class="hljs-string">/opt</span>：用户级的程序目录，可以理解为D:<span class="hljs-string">/Software</span>，opt有可选的意思，这里可以用于放置第三方大型软件（或游戏），当你不需要时，直接rm -rf掉即可。在硬盘容量不够时，也可将<span class="hljs-string">/opt</span>单独挂载到其他磁盘上使用<br>源码放哪里？<br><span class="hljs-string">/usr/src</span>：系统级的源码目录<br><span class="hljs-string">/usr/local/src</span>：用户级的源码目录。<br></code></pre></td></tr></table></figure><p><strong>安装SAMBA服务</strong><br>如果能把服务器上的磁盘直接挂载到我们使用的Windows个人PC上是不是很爽？<br>我们可以通过建立局域网SAMBA服务来实现这个效果：<br>安装samba 和samba-common-bin</p><figure class="highlight armasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs armasm"><span class="hljs-symbol">sudo</span> apt-<span class="hljs-meta">get</span> install samba samba-<span class="hljs-meta">common</span>-bin<br></code></pre></td></tr></table></figure><p>配置<code>/etc/samba/smb.conf</code>文件</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">sudo</span> nano /etc/samba/smb.conf<br></code></pre></td></tr></table></figure><p>在最后一行后面加入：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># 共享文件夹显示的名称</span><br>[home]<br><span class="hljs-comment"># 说明信息</span><br>comment = Fusion WorkStation Storage<br><span class="hljs-comment"># 可以访问的用户</span><br>valid <span class="hljs-built_in">users</span> = sage,root<br><span class="hljs-comment"># 共享文件的路径</span><br>path = /home/sage/<br><span class="hljs-comment"># 可被其他人看到资源名称（非内容）</span><br>browseable = <span class="hljs-built_in">yes</span><br><span class="hljs-comment"># 可写</span><br>writable = <span class="hljs-built_in">yes</span><br><span class="hljs-comment"># 新建文件的权限为 664</span><br>create mask = 0664<br><span class="hljs-comment"># 新建目录的权限为 775</span><br>directory mask = 0775<br></code></pre></td></tr></table></figure><p>可以把配置文件中你不需要的分享名称删除，例如 [homes], [printers] 等。<br>运行这个命令测试一下配置文件是否有错误，根据提示做相应修改：</p><figure class="highlight ebnf"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs ebnf"><span class="hljs-attribute">testparm</span><br></code></pre></td></tr></table></figure><p>添加登陆账户并创建密码<br>必须是 linux 已存在的用户：</p><figure class="highlight livecodeserver"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs livecodeserver">sudo smbpasswd -<span class="hljs-keyword">a</span> <span class="hljs-literal">pi</span><br></code></pre></td></tr></table></figure><p>然后重启服务即可：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">sudo</span> /etc/init.d/samba-ad-dc restart<br></code></pre></td></tr></table></figure><p>接下来可以在Windows的网络中发现设备了：<br>但是可能会出现无法点开的情况，这里需要在Windows的凭据管理器中添加账户信息（开始菜单里搜索凭据管理器即可打开），点击添加Windows凭据，输入你的服务器名称和账户密码，接下来就可以点进去看到服务器上的文件了。 为了更加方便地进行文件交互，我们添加对应的磁盘到Windows资源管理器的此电脑中，选择刚刚服务器的网络路径即可添加。</p><h3 id="GPU相关的驱动和环境"><a href="#GPU相关的驱动和环境" class="headerlink" title="GPU相关的驱动和环境"></a><strong>GPU相关的驱动和环境</strong></h3><p><strong>在安装显卡驱动前一定要到BIOS关闭安全启动！</strong><br>系统自带的显卡驱动不是NVIDIA的，先删除这个驱动</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs routeros">sudo apt-<span class="hljs-built_in">get</span> <span class="hljs-built_in">remove</span> --purge nvidia*<br></code></pre></td></tr></table></figure><p><strong>NVIDIA驱动</strong><br>最简单的方式是通过系统的软件与更新来安装：<br>进入系统的图形桌面，打开Software &amp; Updates软件，可以看到标签栏有一个Additional Drivers：<br>选择第一个安装Nvidia官方驱动（第二个是开源驱动）即可，根据网络情况稍等大概十分钟，安装完重启服务器，重启完之后更新一下软件。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">sudo</span> apt update<br><span class="hljs-built_in">sudo</span> apt upgrade<br></code></pre></td></tr></table></figure><p>这里会连带Nvidia的驱动一起升级一遍，更新到最新的驱动；更新完可能会出现nvidia-smi命令报错，再重启一下就解决了。<br>安装Cuda<br>去官网下载cuda安装包：<a href="https://docs.nvidia.com/cuda/cuda-toolkit-release-notes/index.html">cuda</a> （根据实际情况选择）</p><p>运行下面的命令进行安装（换成自己的文件名）：</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">chmod</span> +x cuda_11.<span class="hljs-number">0</span>.<span class="hljs-number">2</span>_450.<span class="hljs-number">51</span>.<span class="hljs-number">05</span>_linux.run<br><span class="hljs-attribute">sudo</span> sh ./cuda_11.<span class="hljs-number">0</span>.<span class="hljs-number">2</span>_450.<span class="hljs-number">51</span>.<span class="hljs-number">05</span>_linux.run<br></code></pre></td></tr></table></figure><p>可能会报一个警告<br><img src="/img/dlenv/1.png"><br>前面已经卸载过旧版本了直接Continue就好。然后根据提示选择安装选项，注意不要勾选第一个安装显卡驱动的，因为之前已经安装过了。 安装完成后提示：<br><img src="/img/dlenv/2.png"><br>根据上图提示需要配置环境变量：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">nano  ~/.bashrc<br></code></pre></td></tr></table></figure><p>在文件最后加入以下语句：</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs routeros"><span class="hljs-built_in">export</span> <span class="hljs-attribute">CUDA_HOME</span>=/usr/local/cuda-11.0<br><span class="hljs-built_in">export</span> <span class="hljs-attribute">LD_LIBRARY_PATH</span>=<span class="hljs-variable">$&#123;CUDA_HOME&#125;</span>/lib64<br><span class="hljs-built_in">export</span> <span class="hljs-attribute">PATH</span>=<span class="hljs-variable">$&#123;CUDA_HOME&#125;</span>/bin:$&#123;PATH&#125;<br></code></pre></td></tr></table></figure><p>然后使其生效：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">source</span> ~/.bashrc<br></code></pre></td></tr></table></figure><p>可以使用命令<code>nvcc -V</code>查看安装的版本信息，也可以编译一个程序测试安装是否成功，执行以下几条命令：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">cd</span> ~/Softwares/cuda/NVIDIA_CUDA-11.0_Samples/1_Utilities/deviceQuery<br>make<br>./deviceQuery<br></code></pre></td></tr></table></figure><p>正常的话会有相应输出，打印显卡的信息。<br><strong>安装CuDNN</strong><br>进入到<a href="https://developer.nvidia.com/rdp/cudnn-download">CuDNN</a>的下载官网，点击Download开始选择下载版本，当然在下载之前还有登录，选择版本界面如下：<br><img src="/img/dlenv/3.png"><br>我们选择和之前cuda版本对应的cudnn版本：<br><img src="/img/dlenv/4.png"><br>下载之后是一个压缩包，对它进行解压，命令如下：</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">tar</span> -xzvf cudnn-<span class="hljs-number">11</span>.<span class="hljs-number">0</span>-linux-x64-v<span class="hljs-number">8.0.5.39</span>.tgz<br></code></pre></td></tr></table></figure><p>使用以下两条命令复制这些文件到CUDA目录下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">sudo</span> <span class="hljs-built_in">cp</span> cuda/lib64/* /usr/local/cuda-11.0/lib64/<br><span class="hljs-built_in">sudo</span> <span class="hljs-built_in">cp</span> cuda/include/* /usr/local/cuda-11.0/include/<br></code></pre></td></tr></table></figure><p>拷贝完成之后，可以使用以下命令查看CUDNN的版本信息：</p><figure class="highlight gradle"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs gradle">cat <span class="hljs-regexp">/usr/</span>local<span class="hljs-regexp">/cuda/i</span>nclude/cudnn_version.h | <span class="hljs-keyword">grep</span> CUDNN_MAJOR -A <span class="hljs-number">2</span><br></code></pre></td></tr></table></figure><p>可以看到版本信息如下，为8.0.5：<br><img src="/img/dlenv/5.png"></p><p><strong>安装Conda环境</strong><br>不同的训练框架和版本可能会需要不同的python版本相对应，而且有的包比如numpy也对版本有要求，所以比较优雅的方法是给每个配置建立一个虚拟的python环境，在需要的时候可以随时切换，而不需要的时候也能删除不浪费磁盘资源，那在这方面conda是做得最好的。</p><ol><li>在Anaconda官网下载Linux版本安装包：<a href="https://www.anaconda.com/products/individual">Anaconda</a></li><li>运行下面的命令安装：</li></ol><figure class="highlight llvm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs llvm">chmod +<span class="hljs-keyword">x</span> Anaconda<span class="hljs-number">3</span><span class="hljs-number">-2020.11</span>-Linux-<span class="hljs-keyword">x</span><span class="hljs-number">86</span>_<span class="hljs-number">64</span>.sh<br>./Anaconda<span class="hljs-number">3</span><span class="hljs-number">-2020.11</span>-Linux-<span class="hljs-keyword">x</span><span class="hljs-number">86</span>_<span class="hljs-number">64</span>.sh<br></code></pre></td></tr></table></figure><p>一路按ENTER确认，然后根据提示输入yes，这里我为了目录整洁不安装在默认路径，设置为下面的路径：&#x2F;home&#x2F;sage&#x2F;Softwares&#x2F;anaconda<br>然后会询问你是否要初始化conda，输入yes确认，重开终端窗口之后，就可以看到conda环境可用了（base代表默认环境）：<br>conda的使用方法网上搜一下有很多，这里就不赘述了。</p><p><strong>安装Nvidia-Docker</strong><br>Docker也是虚拟化环境的神器，前面说的conda虽然可以提供python的虚拟环境并方便地切换，但是有的时候我们的开发环境并不只是用到python，比如有的native库需要对应gcc版本的编译环境，或者进行交叉编译时要安装很多工具链等等。如果这些操作都在服务器本地上进行，那时间久了就会让服务器的文件系统非常杂乱，而且还会遇到各种软件版本冲突问题。<br><br>Docker就可以很好地解决这些问题，它其实可以理解为就是一个非常轻量化的虚拟机，我们可以在宿主服务器上新建很多个这种被称为容器的虚拟机，然后在里面配置我们的开发环境，且这些配置好的环境是可以打包成镜像的，方便随时做分享和重用；不需要的时候，我们直接删除容器就好了，其资源是和我们的服务器宿主机完全隔离的。<br><br>Docker的具体使用可以自己搜索一下很多教程，这里主要介绍如何把GPU暴露给Docker的容器（因为大家都知道像是VMware这种虚拟机里面都是无法共享宿主机的GPU的），是通过nvidia-docker实现的。<br><br>以前为了配置nvidia-docker，需要安装完docker之后再安装单独的nvidia docker2，而现在只需要安装nvidia container toolkit即可，更加方便了。<br>docker安装在官网上有详细的介绍或者运行下面的命令安装：</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs routeros">sudo apt-<span class="hljs-built_in">get</span> update<br>sudo apt-<span class="hljs-built_in">get</span> install docker.io<br>systemctl start docker<br>systemctl <span class="hljs-built_in">enable</span> docker<br></code></pre></td></tr></table></figure><p>可以运行这条命令检查是否安装成功：</p><figure class="highlight applescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs applescript">docker <span class="hljs-built_in">version</span><br></code></pre></td></tr></table></figure><p>安装NVIDIA Container Toolkit</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment">##首先要确保已经安装了nvidia driver</span><br><span class="hljs-comment">#添加源</span><br>distribution=$(. /etc/os-release;<span class="hljs-built_in">echo</span> $ID<span class="hljs-variable">$VERSION_ID</span>)<br>curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | <span class="hljs-built_in">sudo</span> apt-key add -<br>curl -s -L https://nvidia.github.io/nvidia-docker/<span class="hljs-variable">$distribution</span>/nvidia-docker.list | <span class="hljs-built_in">sudo</span> <span class="hljs-built_in">tee</span> /etc/apt/sources.list.d/nvidia-docker.list<br><br>安装并重启<br><span class="hljs-built_in">sudo</span> apt-get update &amp;&amp; <span class="hljs-built_in">sudo</span> apt-get install -y nvidia-container-toolkit<br><span class="hljs-built_in">sudo</span> systemctl restart docker<br></code></pre></td></tr></table></figure><p>安装完成后可以新建一个容器测试一下：</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">sudo</span> docker run -it --name test_nvidia_docker --gpus <span class="hljs-literal">all</span> nvidia/cuda:<span class="hljs-number">11</span>.<span class="hljs-number">1</span>-base <br></code></pre></td></tr></table></figure><p>其中最后的参数nvidia&#x2F;cuda:11.1-base 是Nvidia官方的镜像，需要根据工作站主机中实际安装的cuda版本进行修改，版本可以用nvcc -V查看。<br>进入容器之后可以跑一下<code>nvidia-smi</code>命令看看</p><h3 id="环境测试"><a href="#环境测试" class="headerlink" title="环境测试"></a><strong>环境测试</strong></h3><p>这里通过一个简单的python脚本测试一下GPU训练是否一切正常，跑一个DL里面的Hello World程序，通过两种方法测试：本地conda和docker虚拟机。以后的开发过程中一般还是使用Docker的方式来进行更为优雅。<br><strong>本地Conda环境方式：</strong><br>先用conda新建一个python3.8+pytorch1.7+cuda11.0的虚拟环境：</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">conda</span> create --name python_38-pytorch_1.<span class="hljs-number">7</span>.<span class="hljs-number">0</span> python=<span class="hljs-number">3</span>.<span class="hljs-number">8</span><br></code></pre></td></tr></table></figure><p>创建完成后进入环境：</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">conda</span> activate python_38-pytorch_1.<span class="hljs-number">7</span>.<span class="hljs-number">0</span><br></code></pre></td></tr></table></figure><p>检查一下是否切换到所需环境了：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">which</span> pip<br></code></pre></td></tr></table></figure><p>如果看到使用的确实是我们设置的环境目录中的pip的话说明就ok。<br><br>安装pytorch，可以参考官网的安装命令</p><p>环境配置就完成了，下面新建一个简单的测试脚本验证功能，新建<code>mnist_train.py</code>，内容如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<br><span class="hljs-keyword">import</span> torch.nn.functional <span class="hljs-keyword">as</span> F<br><span class="hljs-keyword">import</span> torch.optim <span class="hljs-keyword">as</span> optim<br><span class="hljs-keyword">import</span> torch.backends.cudnn <span class="hljs-keyword">as</span> cudnn<br><span class="hljs-keyword">from</span> torchvision <span class="hljs-keyword">import</span> datasets, transforms<br><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Net</span>(nn.Module):<br><span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>    <span class="hljs-built_in">super</span>(Net, <span class="hljs-variable language_">self</span>).__init__()<br>    <span class="hljs-variable language_">self</span>.conv1 = nn.Conv2d(<span class="hljs-number">1</span>, <span class="hljs-number">10</span>, kernel_size=<span class="hljs-number">5</span>)<br>    <span class="hljs-variable language_">self</span>.conv2 = nn.Conv2d(<span class="hljs-number">10</span>, <span class="hljs-number">20</span>, kernel_size=<span class="hljs-number">5</span>)<br>    <span class="hljs-variable language_">self</span>.conv2_drop = nn.Dropout2d()<br>    <span class="hljs-variable language_">self</span>.fc1 = nn.Linear(<span class="hljs-number">320</span>, <span class="hljs-number">50</span>)<br>    <span class="hljs-variable language_">self</span>.fc2 = nn.Linear(<span class="hljs-number">50</span>, <span class="hljs-number">10</span>)<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>    x = F.relu(F.max_pool2d(<span class="hljs-variable language_">self</span>.conv1(x), <span class="hljs-number">2</span>))<br>    x = F.relu(F.max_pool2d(<span class="hljs-variable language_">self</span>.conv2_drop(<span class="hljs-variable language_">self</span>.conv2(x)), <span class="hljs-number">2</span>))<br>    x = x.view(-<span class="hljs-number">1</span>, <span class="hljs-number">320</span>)<br>    x = F.relu(<span class="hljs-variable language_">self</span>.fc1(x))<br>    x = F.dropout(x, training=<span class="hljs-variable language_">self</span>.training)<br>    x = <span class="hljs-variable language_">self</span>.fc2(x)<br>    <span class="hljs-keyword">return</span> F.log_softmax(x, dim=<span class="hljs-number">1</span>)<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">train</span>(<span class="hljs-params">model, device, train_loader, optimizer, epoch</span>):<br>    model.train()<br>    <span class="hljs-keyword">for</span> batch_idx, (data, target) <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(train_loader):<br>        data, target = data.to(device), target.to(device)<br>        optimizer.zero_grad()<br>        output = model(data)<br>        loss = F.nll_loss(output, target)<br>        loss.backward()<br>        optimizer.step()<br>        <span class="hljs-keyword">if</span> batch_idx % <span class="hljs-number">10</span> == <span class="hljs-number">0</span>:<br>            <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Train Epoch: &#123;&#125; [&#123;&#125;/&#123;&#125; (&#123;:.0f&#125;%)]\tLoss: &#123;:.6f&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(<br>                epoch, batch_idx * <span class="hljs-built_in">len</span>(data), <span class="hljs-built_in">len</span>(train_loader.dataset),<br>                       <span class="hljs-number">1.</span>   * batch_idx / <span class="hljs-built_in">len</span>(train_loader), loss.item()))<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">main</span>():<br>cudnn.benchmark = <span class="hljs-literal">True</span><br>torch.manual_seed(<span class="hljs-number">1</span>)<br>device = torch.device(<span class="hljs-string">&quot;cuda&quot;</span>) <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> torch.device(<span class="hljs-string">&quot;cpu&quot;</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Using device: &#123;&#125;&quot;</span>.<span class="hljs-built_in">format</span>(device))<br>kwargs = &#123;<span class="hljs-string">&#x27;num_workers&#x27;</span>: <span class="hljs-number">1</span>, <span class="hljs-string">&#x27;pin_memory&#x27;</span>: <span class="hljs-literal">True</span>&#125;<br>train_loader = torch.utils.data.DataLoader(<br>    datasets.MNIST(<span class="hljs-string">&#x27;./data&#x27;</span>, train=<span class="hljs-literal">True</span>, download=<span class="hljs-literal">True</span>,<br>                    transform=transforms.Compose([<br>                        transforms.ToTensor(),<br>                        transforms.Normalize((<span class="hljs-number">0.1307</span>,), (<span class="hljs-number">0.3081</span>,))<br>                    ])),<br>    batch_size=<span class="hljs-number">64</span>, shuffle=<span class="hljs-literal">True</span>, **kwargs)<br><br>model = Net().to(device)<br>optimizer = optim.SGD(model.parameters(), lr=<span class="hljs-number">0.01</span>, momentum=<span class="hljs-number">0.5</span>)<br><br><span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, <span class="hljs-number">11</span>):<br>    train(model, device, train_loader, optimizer, epoch)<br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&#x27;__main__&#x27;</span>:<br>main()<br></code></pre></td></tr></table></figure><p>运行脚本，正常的话就可以看到训练输出了：<br><br><strong>Docker环境方式：</strong><br>首先还是新建一个Docker镜像，运行下面的命令：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">sudo</span> docker run  -it  --name train_mnist \<br>-v /etc/timezone:/etc/timezone \<br>-v /etc/localtime:/etc/localtime \<br>-v /home/user/WorkSpace/_share:/home/workspace/_share  \<br>--gpus all nvidia/cuda:11.1-base<br></code></pre></td></tr></table></figure><p>就进入到了带gpu的ubuntu20.04容器中，效果可以参考文章开头的视频。按照前面的配置方法同样配置好pytorch和其他软件包，然后运行同样的脚本，也可以得到上述输出，说明gpu在docker中正常工作。</p><h3 id="clash的配置"><a href="#clash的配置" class="headerlink" title="clash的配置"></a>clash的配置</h3><p>Clash 项目地址：<a href="https://github.com/Dreamacro/clash%EF%BC%9B">https://github.com/Dreamacro/clash；</a><br>下载 clash-linux.gz<br>直接双击下好的包解压缩，提取 clash-linux 至你想存放的文件夹下；<br>右键点击 clash-linux，选择属性，权限处勾选允许以程序执行：<br>然后双击 clash-linux，点击运行，此时 clash 已经在后台运行了，并且生成<del>&#x2F;.config&#x2F;clash文件夹；<br>编辑</del>&#x2F;.config&#x2F;clash下的 config.yml文件，即自己的服务器及规则等信息；<br>重新启动 clash-linux 以加载配置文件，可通过 <a href="http://clash.razord.top/">http://clash.razord.top</a>  进行策略组节点的切换，连接的 ip 和端口根据自己的 config.yml 中的external-controller填写；<br>使用系统代理（推荐），系统代理可在控制中心-&gt;网络-&gt;系统代理-&gt;手动处填写（根据自己配置文件里的端口填写） 配置系统proxy，以ubuntu为例，在Settings -&gt; Network -&gt; Network proxy ，选择手动配置，按config.yaml内容设置相应IP地址和端口值，如果 clash运行在本机那地址写127.0.0.1就行了。</p><p>操作顺序的问题需要注意一下，先运行clash，再打开浏览器进入clash.razord.top，按实际测速结果选择合适的代理服务器，然后打开系统proxy。</p><p>这样做的原因是，如果先打开系统proxy，万一clash默认使用的那个proxy不能工作，那么不但无法正常代理上网，也无法进入clash.razord.top进行proxy选择。</p><p>20.04找个简单办法，搜索应用：启动应用程序，打开添加，然后只需要找到你clash放置的位置，保存即可，再次开机就直接clash在后台了，同样可以平时ps -x<br>找到进程kill掉。</p><h2 id="工作站维护篇"><a href="#工作站维护篇" class="headerlink" title="工作站维护篇"></a><strong>工作站维护篇</strong></h2><ul><li>备份<br>由于Linux本身万物皆文件的设计理念，加上root用户对几乎全部的系统文件都有访问和更改的权限，因此Linux系统的备份和还原其实非常简单，我们直接打包整个根文件系统就可以了。</li></ul><p>我们可以使用tar命令来打包并压缩文件系统，不过这里在打包的过程中需要排除一些不需要文件，或者与新系统文件冲突的文件，包括&#x2F;tmp、&#x2F;proc、&#x2F;lost+found 等目录。<br><br>找一个你想保存备份文件的目录，运行下面的命令：</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs routeros">tar -cvpzf ubuntu_backup@`date +%Y-%m+%d`.tar.gz <span class="hljs-attribute">--exclude</span>=/proc <span class="hljs-attribute">--exclude</span>=/tmp <span class="hljs-attribute">--exclude</span>=/boot  <span class="hljs-attribute">--exclude</span>=/lost+found <span class="hljs-attribute">--exclude</span>=/media <span class="hljs-attribute">--exclude</span>=/mnt <span class="hljs-attribute">--exclude</span>=/run /<br></code></pre></td></tr></table></figure><p>我们会得到一个名为backup.tgz的压缩文件，这个文件包含我们需要备份的系统的全部内容。</p><ul><li>还原</li></ul><p>如果系统没有出问题可以正常启动的话，那直接在刚刚的压缩包找找到想还原的文件替换就好了。而如果系统无法启动了，或者说想换一块硬盘克隆一样的系统，那么可以按一下步骤操作：</p><ol><li>重装干净的Ubuntu系统。<br>跟上面介绍的一样，使用U盘给目标磁盘重装一个干净的系统，这一步是为了省去自己分配存储空间和挂载的麻烦，如果你会自己配置的话那也可以不做这一步。<br>再次使用U盘进入系统，这次选择<code>try ubuntu without installing</code>，然后可以看到挂载好的刚刚安装了干净系统的另一个盘，我们在这里对盘里的根文件系统进行一些文件的提取：</li></ol><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">sudo</span> su<br><br><span class="hljs-comment"># 在tryUbuntu根目录下有media文件夹，里面是U盘文件夹和新安装的系统文件夹，在在里分别用（U盘）和（UBUNTU）表示</span><br><span class="hljs-built_in">cd</span> /media/（U盘）<br>mount -o remount rw ./<br> ​<br><span class="hljs-comment"># 将新系统根目录下/boot/grub/grub.cfg文件备份到U盘中</span><br><span class="hljs-built_in">sudo</span> <span class="hljs-built_in">cp</span> /media/(Ubuntu)/boot/grub/grub.cfg ./    <br> ​<br><span class="hljs-comment"># 将新系统根目录下/etc/fstab文件备份到U盘中，fstab是与系统开机挂载有关的文件，grub.cfg是与开机引导有关的文件，所以这一步至关重要</span><br><span class="hljs-built_in">sudo</span> <span class="hljs-built_in">cp</span> /media/(UBUNTU)/etc/fstab ./<br> ​<br><span class="hljs-comment"># 这一步删除新装ubuntu全部的系统文件，有用的fstab及grub.cfg已经备份</span><br><span class="hljs-built_in">cd</span> /media/(UBUNTU)<br><span class="hljs-built_in">sudo</span> <span class="hljs-built_in">rm</span> -rf ./*<br> ​<br><span class="hljs-comment"># 将U盘中backup.tgz复制到该目录下并解压缩</span><br><span class="hljs-built_in">cp</span> /media/(U盘)/backup.tgz ./<br><span class="hljs-built_in">sudo</span> tar xvpfz backup.tgz ./<br> ​<br><span class="hljs-comment"># 创建打包系统时排除的文件</span><br><span class="hljs-built_in">sudo</span> <span class="hljs-built_in">mkdir</span> proc lost+found mnt sys media<br></code></pre></td></tr></table></figure><p>这一步完成后，在用我们在新系统中备份的fatab及grub.cfg 文件去替换压缩包中解压出来的同名文件，<code>sudo reboot</code>重启后就发现系统已经恢复到备份时的状态，包括各种框架，环境，系统设置。</p>]]></content>
    
    
    <categories>
      
      <category>深度学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>环境配置</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Markdown-可靠的标记语言</title>
    <link href="/2021/07/24/markdown/"/>
    <url>/2021/07/24/markdown/</url>
    
    <content type="html"><![CDATA[<blockquote><p>创作声明：主要内容为互联网内容整合。</p></blockquote><h2 id="你为什么需要Markdown"><a href="#你为什么需要Markdown" class="headerlink" title="你为什么需要Markdown"></a><strong>你为什么需要Markdown</strong></h2><p>Markdown 是一种轻量级标记语言，创始人为 John Gruber。它允许人们「使用易读易写的纯文本格式编写文档，然后转换成有效的 XHTML（或者 HTML）文档」。——维基百科。简单地说，Markdown 与 HTML 语言一样，使用一些符号就代替样式，但是它比 HTML 语言更加简单，通过简单的标记语法，它可以使普通文本内容具有一定的格式。<br>Markdown 的本质是让我们回归到内容本身，注重文章本身的结构，而不是样式。</p><h2 id="Markdown环境搭建"><a href="#Markdown环境搭建" class="headerlink" title="Markdown环境搭建"></a><strong>Markdown环境搭建</strong></h2><p>市面上有许多Markdown解决方案，我选择使用VS Code+插件解决（一体化解决方案），VS Code的安装与使用不再赘述，以下必备插件：</p><h3 id="Markdown-All-in-One"><a href="#Markdown-All-in-One" class="headerlink" title="Markdown All in One"></a><strong>Markdown All in One</strong></h3><p>这是个大一统型的扩展，集成了撰写 Markdown 时所需要的大部分功能，是 Markdown 类插件中下载榜榜首。可以认为是 VS Code 中的 Markdown 必备扩展。其功能涵盖：  </p><ul><li><strong>快捷键</strong><br>Ctrl + B 加粗<br>Ctrl + I 斜体<br>Alt + S 删除线<br>Alt + C 勾选&#x2F;取消勾选任务清单项目<br>Ctrl + M开启LaTeX 数学公式编写</li><li><strong>自动生成并更新目录</strong><br>Markdown All in One 插件可以自动根据你正在编辑的文档，生成相应的目录。我们利用快捷键 Ctrl + Shift + P 调出「命令面板」，输入「目录」，即可直接调用命令「Markdown: 创建目录」。之后如果你更新了文章内容，也可以直接通过命令「Markdown：更新目录」更新。</li><li><strong>路径补全</strong><br>Markdown All in One 可以直接帮助我们补全本地图片的路径，这个功能非常方便，因此我也推荐将 Markdown 文档所需的图片素材保存在一个和源文件同一个路径的文件夹下，比如这一的安排方式就比较合理：<br>.<br>├── 1 文章一<br>│   ├── image&#x2F;<br>│   └── Post1.md<br>├── 2 文章二<br>│   ├── image&#x2F;<br>│   └── Post2.md<br>…  </li><li><strong>LaTeX数学公式支持</strong><br>Markdown All in One 直接提供了基于 LaTeX 的数学公式渲染，支持行内数学公式，以及整段的数学公式。</li></ul><h3 id="Markdown-Preview-Mermaid-Support"><a href="#Markdown-Preview-Mermaid-Support" class="headerlink" title="Markdown Preview Mermaid Support"></a><strong>Markdown Preview Mermaid Support</strong></h3><p>由于我偶尔会使用 Markdown 绘制流程图、时序图或甘特图，因此我也会使用 Mermaid 渲染引擎来辅助我的绘制。这里 Markdown Preview Mermaid Support 这个插件就让 VS Code 的 Markdown 预览能够正确渲染 Mermaid 图，非常方便。</p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs css">```mermaid<br>graph <span class="hljs-selector-tag">TD</span>;<br>    <span class="hljs-selector-tag">A</span>--&gt;<span class="hljs-selector-tag">B</span>;<br>    <span class="hljs-selector-tag">A</span>--&gt;C;<br>    <span class="hljs-selector-tag">B</span>--&gt;D;<br>    C--&gt;D;<br></code></pre></td></tr></table></figure><pre><code class=" mermaid">graph TD;    A--&gt;B;    A--&gt;C;    B--&gt;D;    C--&gt;D;</code></pre><h3 id="格式转换Pandoc"><a href="#格式转换Pandoc" class="headerlink" title="格式转换Pandoc"></a><strong>格式转换Pandoc</strong></h3><p>Pandoc 的下载可以去官网手动下载<a href="https://pandoc.org/">pandoc</a><br>也可以使用包管理器自动下载。<br>对 Windows 用户，有 <code>Scoop</code> 和 <code>Chocolatey</code></p><figure class="highlight mipsasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs mipsasm"><span class="hljs-keyword">scoop </span><span class="hljs-keyword">install </span>pandoc<br>choco <span class="hljs-keyword">install </span>pandoc<br></code></pre></td></tr></table></figure><p>对 MacOS 用户，有 Homebrew</p><figure class="highlight mipsasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs mipsasm"><span class="hljs-keyword">brew </span><span class="hljs-keyword">install </span>pandoc<br></code></pre></td></tr></table></figure><p>安装完毕后，在 VS Code 中安装相关扩展，这里首推 VS Code-pandoc（选 Chris 的那个），可以实现 .md 到 .pdf，.docx 和 .html 的转换。</p><h3 id="进阶操作"><a href="#进阶操作" class="headerlink" title="进阶操作"></a><strong>进阶操作</strong></h3><p>VS Code 最强大的地方在于其无穷无尽的自定义功能。可能你觉得默认的 Markdown 预览风格并不符合你的口味，那么你可以直接在 VS Code 的设置中（快捷键 Ctrl + , 打开）自定义 Markdown 渲染使用的 CSS 样式文件：  </p><figure class="highlight prolog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs prolog">&#123;  <br>    <span class="hljs-string">&quot;markdown.styles&quot;</span>: [<br>        <span class="hljs-string">&quot;Style.css&quot;</span><br>    ]<br>&#125;<br></code></pre></td></tr></table></figure><p>其中的 Style.css 就是更换默认 Markdown 渲染使用样式的 CSS 文件，这里需要填写你所使用文件的绝对路径。<br>VS Code 强大的插件系统，以及开源的方便性，让我们可以从几乎任何角度、利用各种手段去自定义我们的 Markdown 编写环境（比如自定义 VS Code 的 CSS 修改样式、自定义编译自动化将 Markdown 编译为 HTML、PDF 等）。更多进阶的操作功能，请直接参考微软官方的开发文档：<a href="https://code.visualstudio.com/docs/languages/markdown">Markdown and Visual Studio Code</a>  </p><h2 id="Markdown常用操作"><a href="#Markdown常用操作" class="headerlink" title="Markdown常用操作"></a><strong>Markdown常用操作</strong></h2><h3 id="字体设置"><a href="#字体设置" class="headerlink" title="字体设置"></a><strong>字体设置</strong></h3><p>这里显示正文  </p><figure class="highlight asciidoc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs asciidoc"><span class="hljs-strong">*这里显示的是斜体*</span>  <br><span class="hljs-emphasis">_这里显示的倾斜体_</span>  <br><span class="hljs-strong">**这里显示的文字是加粗了**</span>  <br><span class="hljs-strong">***这里的文字是倾斜加粗的**</span>*  <br>~~这里的文字是加下划线的~~  <br></code></pre></td></tr></table></figure><p><em>这里显示的是斜体</em><br><em>这里显示的倾斜体</em><br><strong>这里显示的文字是加粗了</strong><br><em><strong>这里的文字是倾斜加粗的</strong></em><br><del>这里的文字是加下划线的</del>  </p><p><strong>分级标题</strong><br>类 Atx 形式则是在行首插入 1 到 6 个 # ，对应到标题 1 到 6 阶，例如：</p><figure class="highlight clean"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs clean"># 一级标题<br>## 二级标题<br>### 三级标题<br>#### 四级标题<br>##### 五级标题<br>###### 六级标题  <br></code></pre></td></tr></table></figure><h1 id="一级标题"><a href="#一级标题" class="headerlink" title="一级标题"></a>一级标题</h1><h2 id="二级标题"><a href="#二级标题" class="headerlink" title="二级标题"></a>二级标题</h2><h3 id="三级标题"><a href="#三级标题" class="headerlink" title="三级标题"></a>三级标题</h3><h4 id="四级标题"><a href="#四级标题" class="headerlink" title="四级标题"></a>四级标题</h4><h5 id="五级标题"><a href="#五级标题" class="headerlink" title="五级标题"></a>五级标题</h5><h6 id="六级标题"><a href="#六级标题" class="headerlink" title="六级标题"></a>六级标题</h6><p>类 Setext 形式是用底线的形式，利用 &#x3D; （最高阶标题）和 - （第二阶标题），任何数量的 &#x3D; 和 - 都可以有效果。</p><p>例如：</p><figure class="highlight abnf"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs abnf">这是一个一级标题<br><span class="hljs-operator">=</span><span class="hljs-operator">=</span><span class="hljs-operator">=</span><span class="hljs-operator">=</span><span class="hljs-operator">=</span><span class="hljs-operator">=</span><span class="hljs-operator">=</span><span class="hljs-operator">=</span><span class="hljs-operator">=</span><span class="hljs-operator">=</span><span class="hljs-operator">=</span><span class="hljs-operator">=</span><span class="hljs-operator">=</span><span class="hljs-operator">=</span><span class="hljs-operator">=</span><span class="hljs-operator">=</span><span class="hljs-operator">=</span><span class="hljs-operator">=</span><span class="hljs-operator">=</span><span class="hljs-operator">=</span><span class="hljs-operator">=</span><span class="hljs-operator">=</span><span class="hljs-operator">=</span><span class="hljs-operator">=</span><span class="hljs-operator">=</span><span class="hljs-operator">=</span><span class="hljs-operator">=</span><span class="hljs-operator">=</span><br>二级标题<br>-----------<br></code></pre></td></tr></table></figure><hr><h3 id="链接"><a href="#链接" class="headerlink" title="链接"></a><strong>链接</strong></h3><ul><li><strong>插入本地图片链接</strong><br>![图片描述]（图片路径） 注：图片描述可以不写。</li></ul><figure class="highlight scss"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs scss">如本地插入图片示范：<br>!<span class="hljs-selector-attr">[插入下一层级目录下的图片]</span>(/短视频封面/<span class="hljs-number">083</span>.jpg)<br>!<span class="hljs-selector-attr">[插入同级目录下的图片]</span>(/原创扁平方盒-逐浪字体.jpg)<br></code></pre></td></tr></table></figure><ul><li><strong>插入互联网上图片</strong></li></ul><p>![图片描述]（图片网络路径） 注：图片描述与插入本地图片一样，描述可以不写。</p><figure class="highlight less"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs less">!<span class="hljs-selector-attr">[图片描述]</span>(<span class="hljs-attribute">https</span>:<span class="hljs-comment">//www.baidu.com/images/1.jpg)</span><br></code></pre></td></tr></table></figure><ul><li><strong>自动连接</strong><br>Markdown 支持以比较简短的自动链接形式来处理网址和电子邮件信箱，只要是用&lt;&gt;包起来， Markdown 就会自动把它转成链接。也可以直接写，也是可以显示成链接形式的。 链接内容定义的形式为：</li></ul><figure class="highlight prolog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs prolog">[这是百度链接](http://www.baidu.com/ <span class="hljs-string">&quot;欢迎访问百度&quot;</span>).<br>[这是百度链接](http://www.baidu.com/ <span class="hljs-string">&#x27;欢迎访问百度&#x27;</span>).<br>[这是百度链接](http://www.baidu.com/ (欢迎访问百度)).<br></code></pre></td></tr></table></figure><p><a href="http://www.baidu.com/" title="欢迎访问百度">这是百度链接</a>.</p><ul><li><strong>代码块</strong><br>对于程序员来说代码功能是必不可少的，插入程序代码的方式有两种，一种是利用缩进(tab), 另一种是利用英文“&#96;&#96;&#96;”符号（一般在ESC键下方，和~同一个键）包裹代码。</li></ul><figure class="highlight autohotkey"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs autohotkey">如果要标记一小段行内代码，你可以用反引号把它包起来（`），例如：<br>     Use the `printf()` function.<br></code></pre></td></tr></table></figure><h3 id="分割线"><a href="#分割线" class="headerlink" title="分割线"></a><strong>分割线</strong></h3><p>你可以在一行中用三个以上的星号、减号、底线来建立一个分隔线，行内不能有其他东西。你也可以在星号或是减号中间插入空格。下面每种写法都可以建立分隔线：</p><figure class="highlight gams"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs gams"><span class="hljs-comment">* * *</span><br>​<br><span class="hljs-comment">***</span><br>​<br><span class="hljs-comment">*****</span><br>​<br>- - -<br>​<br>---------------------------------------<br></code></pre></td></tr></table></figure><h3 id="引用"><a href="#引用" class="headerlink" title="引用"></a><strong>引用</strong></h3><p>在被引用的文本前加上&gt;符号，以及一个空格就可以了，如果只输入了一个&gt;符号会产生一个空白的引用。</p><figure class="highlight node-repl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs node-repl"><span class="hljs-meta prompt_">&gt;</span> <span class="language-javascript">文字引用 </span><br><span class="hljs-meta prompt_">&gt;</span><br><span class="hljs-meta prompt_">&gt;</span> <span class="language-javascript">文字引用 </span><br><span class="hljs-meta prompt_">&gt;</span> <span class="language-javascript">文字引用 </span><br><span class="hljs-meta prompt_">&gt;</span> <span class="language-javascript">文字引用 </span><br></code></pre></td></tr></table></figure><blockquote><p>文字引用 </p><p>文字引用<br>文字引用<br>文字引用 </p></blockquote><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_">&gt;</span><span class="language-bash">&gt;&gt;第一层嵌套引用</span><br><span class="hljs-meta prompt_">&gt;</span><span class="language-bash">&gt;第二层嵌套引用</span><br><span class="hljs-meta prompt_">&gt;</span><span class="language-bash">第三层嵌套引用</span><br></code></pre></td></tr></table></figure><blockquote><blockquote><blockquote><p>第一层嵌套引用<br>第二层嵌套引用<br>第三层嵌套引用</p></blockquote></blockquote></blockquote><h3 id="列表"><a href="#列表" class="headerlink" title="列表"></a><strong>列表</strong></h3><p><strong>无序列表</strong><br>使用 星号*，加号+，减号- 表示无序列表。</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs markdown"><span class="hljs-bullet">*</span> 无序列表文字<br><span class="hljs-bullet">*</span> 无序列表文字<br><span class="hljs-bullet">*</span> 无序列表文字<br>等同于：<br><br><span class="hljs-bullet">+</span> 无序列表文字<br><span class="hljs-bullet">+</span> 无序列表文字<br><span class="hljs-bullet">+</span> 无序列表文字<br>也等同于：<br><br><span class="hljs-bullet">-</span> 无序列表文字<br><span class="hljs-bullet">-</span> 无序列表文字<br><span class="hljs-bullet">-</span> 无序列表文字<br></code></pre></td></tr></table></figure><ul><li>无序列表文字</li><li>无序列表文字<br>…………</li></ul><p><strong>有序列表</strong><br>有序列表则使用数字接着一个英文句点。<br>注意：英文句点后面一定要有一个空格，起到缩进的作用。</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs markdown"><span class="hljs-bullet">1.</span> 有序列表<br><span class="hljs-bullet">2.</span> 有序列表<br><span class="hljs-bullet">3.</span> 有序列表<br></code></pre></td></tr></table></figure><ol><li>有序列表</li><li>有序列表</li><li>有序列表</li></ol><p><strong>无序列表和有序列表同时使用</strong>  </p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs markdown"><span class="hljs-bullet">*</span> 这是无序列表1<br><span class="hljs-bullet">-</span> 这是无序列表2<br><span class="hljs-bullet">+</span> 这是无序列表3<br><span class="hljs-bullet">1.</span> 这是有序列表1 <br><span class="hljs-bullet">2.</span> 这是有序列表2<br><span class="hljs-bullet">*</span> 1. 有序无序混合使用1<br><span class="hljs-bullet">+</span> 2. 有序无序混合使用2<br></code></pre></td></tr></table></figure><ul><li>这是无序列表1</li></ul><ul><li>这是无序列表2</li></ul><ul><li>这是无序列表3</li></ul><ol><li>这是有序列表1 </li><li>这是有序列表2</li></ol><ul><li><ol><li>有序无序混合使用1</li></ol></li></ul><ul><li><ol start="2"><li>有序无序混合使用2</li></ol></li></ul><p><strong>注意事项</strong><br>在使用列表时，只要是数字后面加上英文的点，就会无意间产生列表，比如2020.5.25,这时候想表达的是日期，有些软件把它被误认为是列表。解决方式：在每个点前面加上\就可以了。如下图所示：2020. 05. 25. 今天是2020年5月25日显示如下： 2020. 05. 25. 今天是2020年5月25日。</p><h3 id="表格"><a href="#表格" class="headerlink" title="表格"></a><strong>表格</strong></h3><figure class="highlight gherkin"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs gherkin">|<span class="hljs-string"> name  </span>|<span class="hljs-string"> 111  </span>|<span class="hljs-string">  222  </span>|<span class="hljs-string">  333  </span>|<span class="hljs-string">  444  </span>|<br>|<span class="hljs-string"> :-: </span>|<span class="hljs-string"> :--- </span>|<span class="hljs-string">  ---: </span>|<span class="hljs-string"> :---: </span>|<span class="hljs-string"> :---: </span>|<br>|<span class="hljs-string">  aaa  </span>|<span class="hljs-string"> bbb  </span>|<span class="hljs-string">  ccc  </span>|<span class="hljs-string">  ddd  </span>|<span class="hljs-string">  eee  </span>|<br>|<span class="hljs-string">  fff  </span>|<span class="hljs-string"> ggg  </span>|<span class="hljs-string">  hhh  </span>|<span class="hljs-string">  iii  </span>|<span class="hljs-string">  000  </span>|<br>:- 左对齐<br>:-: 居中<br>-: 右对齐<br></code></pre></td></tr></table></figure><table><thead><tr><th align="center">name</th><th align="left">111</th><th align="center">222</th><th align="center">333</th><th align="center">444</th></tr></thead><tbody><tr><td align="center">aa</td><td align="left">bbb</td><td align="center">ccc</td><td align="center">ddd</td><td align="center">eee</td></tr><tr><td align="center">fff</td><td align="left">ggg</td><td align="center">hhh</td><td align="center">iii</td><td align="center">000</td></tr></tbody></table>]]></content>
    
    
    <categories>
      
      <category>生产力工具</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Markdown</tag>
      
      <tag>VS Code</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
